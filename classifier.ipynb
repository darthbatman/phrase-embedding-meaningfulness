{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "conscious-cardiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Classifier implementation (model architecture, training, testing, etc.) derived from\n",
    "#     https://towardsdatascience.com/pytorch-tabular-binary-classification-a0368da5bb89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "paperback-pencil",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "actual-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PHRASE_LEN = 6\n",
    "VECTOR_SIZE = 200\n",
    "WINDOW_SIZE = 5\n",
    "NUM_LAYERS = 5\n",
    "\n",
    "MIN_FREQUENCY = 5\n",
    "\n",
    "SHOULD_EXTRACT_NOUN_PHRASES = False\n",
    "SHOULD_GENERATE_UNDERSCORED_CORPUS = True\n",
    "SHOULD_TRAIN_WORD2VEC_MODEL = True\n",
    "\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "appropriate-psychiatry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 arxiv abstracts.\n"
     ]
    }
   ],
   "source": [
    "with open('data/arxiv_abstracts_10000.txt', 'r') as f:\n",
    "    arxiv_abstracts = f.read().split('\\n')[:-1]\n",
    "    arxiv_abstracts_raw = '\\n'.join(arxiv_abstracts)\n",
    "    f.close()\n",
    "print(f'Loaded {len(arxiv_abstracts)} arxiv abstracts.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "advanced-language",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1900 negative samples.\n"
     ]
    }
   ],
   "source": [
    "negative_samples = pickle.load(open('data/negative_samples.pkl', 'rb'))\n",
    "print(f'Loaded {len(negative_samples)} negative samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pleased-camera",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1900 positive samples.\n"
     ]
    }
   ],
   "source": [
    "positive_samples = pickle.load(open('data/positive_samples.pkl', 'rb'))\n",
    "print(f'Loaded {len(positive_samples)} positive samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "handed-disposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_phrase(tree_str, label):\n",
    "    phrases = []\n",
    "    trees = Tree.fromstring(tree_str)\n",
    "    for tree in trees:\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == label:\n",
    "                t = subtree\n",
    "                t = ' '.join(t.leaves())\n",
    "                phrases.append(t)\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "crude-karma",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOULD_EXTRACT_NOUN_PHRASES:\n",
    "    nlp = StanfordCoreNLP('data/stanford-corenlp-4.1.0')\n",
    "    noun_phrases = []\n",
    "    for i, abstract in enumerate(arxiv_abstracts):\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f'Extracting noun phrases from abstract {i + 1} of {len(arxiv_abstracts)}')\n",
    "            pickle.dump(noun_phrases, open('data/noun_phrases.pkl', 'wb'))\n",
    "        try:\n",
    "            tree_str = nlp.parse(abstract)\n",
    "            noun_phrases.extend(extract_phrase(tree_str, 'NP'))\n",
    "        except Exception:\n",
    "            pass\n",
    "    noun_phrases = [np for np in list(set(noun_phrases)) if len(np.split()) <= MAX_PHRASE_LEN]\n",
    "    pickle.dump(noun_phrases, open('data/noun_phrases.pkl', 'wb'))\n",
    "noun_phrases = pickle.load(open('data/noun_phrases.pkl', 'rb'))\n",
    "noun_phrases = [np for np in list(set(noun_phrases)) if len(np.split()) <= MAX_PHRASE_LEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "functional-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_phrase_in_corpus(corpus, phrase):\n",
    "    s_idx = corpus.find(phrase)\n",
    "    e_idx = s_idx + len(phrase)\n",
    "    if s_idx != -1 and \\\n",
    "       (s_idx == 0 or corpus[s_idx - 1] in (string.punctuation + ' ')) and \\\n",
    "       (e_idx == len(corpus) or corpus[e_idx] in (string.punctuation + ' ')):\n",
    "        return (s_idx, e_idx)\n",
    "    return (-1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "electoral-knife",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOULD_GENERATE_UNDERSCORED_CORPUS:\n",
    "    corpus = arxiv_abstracts_raw[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "environmental-blame",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing positive_sample 100 of 1900\n",
      "Replacing positive_sample 200 of 1900\n",
      "Replacing positive_sample 300 of 1900\n",
      "Replacing positive_sample 400 of 1900\n",
      "Replacing positive_sample 500 of 1900\n",
      "Replacing positive_sample 600 of 1900\n",
      "Replacing positive_sample 700 of 1900\n",
      "Replacing positive_sample 800 of 1900\n",
      "Replacing positive_sample 900 of 1900\n",
      "Replacing positive_sample 1000 of 1900\n",
      "Replacing positive_sample 1100 of 1900\n",
      "Replacing positive_sample 1200 of 1900\n",
      "Replacing positive_sample 1300 of 1900\n",
      "Replacing positive_sample 1400 of 1900\n",
      "Replacing positive_sample 1500 of 1900\n",
      "Replacing positive_sample 1600 of 1900\n",
      "Replacing positive_sample 1700 of 1900\n",
      "Replacing positive_sample 1800 of 1900\n",
      "Replacing positive_sample 1900 of 1900\n"
     ]
    }
   ],
   "source": [
    "if SHOULD_GENERATE_UNDERSCORED_CORPUS:\n",
    "    for i, positive_sample in enumerate(positive_samples):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Replacing positive_sample {i + 1} of {len(positive_samples)}')\n",
    "        found_indices = set()\n",
    "        while find_phrase_in_corpus(corpus, positive_sample) != (-1, -1) and find_phrase_in_corpus(corpus, positive_sample)[0] not in found_indices:\n",
    "            s_idx, e_idx = find_phrase_in_corpus(corpus, positive_sample)\n",
    "            found_indices.add(s_idx)\n",
    "            corpus = corpus[:s_idx] + positive_sample.replace(' ', '_') + corpus[e_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "suited-forge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing negative_sample 100 of 1900\n",
      "Replacing negative_sample 200 of 1900\n",
      "Replacing negative_sample 300 of 1900\n",
      "Replacing negative_sample 400 of 1900\n",
      "Replacing negative_sample 500 of 1900\n",
      "Replacing negative_sample 600 of 1900\n",
      "Replacing negative_sample 700 of 1900\n",
      "Replacing negative_sample 800 of 1900\n",
      "Replacing negative_sample 900 of 1900\n",
      "Replacing negative_sample 1000 of 1900\n",
      "Replacing negative_sample 1100 of 1900\n",
      "Replacing negative_sample 1200 of 1900\n",
      "Replacing negative_sample 1300 of 1900\n",
      "Replacing negative_sample 1400 of 1900\n",
      "Replacing negative_sample 1500 of 1900\n",
      "Replacing negative_sample 1600 of 1900\n",
      "Replacing negative_sample 1700 of 1900\n",
      "Replacing negative_sample 1800 of 1900\n",
      "Replacing negative_sample 1900 of 1900\n"
     ]
    }
   ],
   "source": [
    "if SHOULD_GENERATE_UNDERSCORED_CORPUS:\n",
    "    for i, negative_sample in enumerate(negative_samples):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Replacing negative_sample {i + 1} of {len(negative_samples)}')\n",
    "        found_indices = set()\n",
    "        while find_phrase_in_corpus(corpus, negative_sample) != (-1, -1) and find_phrase_in_corpus(corpus, negative_sample)[0] not in found_indices:\n",
    "            s_idx, e_idx = find_phrase_in_corpus(corpus, negative_sample)\n",
    "            found_indices.add(s_idx)\n",
    "            corpus = corpus[:s_idx] + negative_sample.replace(' ', '_') + corpus[e_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "under-topic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing noun_phrase 100 of 38952\n",
      "Replacing noun_phrase 200 of 38952\n",
      "Replacing noun_phrase 300 of 38952\n",
      "Replacing noun_phrase 400 of 38952\n",
      "Replacing noun_phrase 500 of 38952\n",
      "Replacing noun_phrase 600 of 38952\n",
      "Replacing noun_phrase 700 of 38952\n",
      "Replacing noun_phrase 800 of 38952\n",
      "Replacing noun_phrase 900 of 38952\n",
      "Replacing noun_phrase 1000 of 38952\n",
      "Replacing noun_phrase 1100 of 38952\n",
      "Replacing noun_phrase 1200 of 38952\n",
      "Replacing noun_phrase 1300 of 38952\n",
      "Replacing noun_phrase 1400 of 38952\n",
      "Replacing noun_phrase 1500 of 38952\n",
      "Replacing noun_phrase 1600 of 38952\n",
      "Replacing noun_phrase 1700 of 38952\n",
      "Replacing noun_phrase 1800 of 38952\n",
      "Replacing noun_phrase 1900 of 38952\n",
      "Replacing noun_phrase 2000 of 38952\n",
      "Replacing noun_phrase 2100 of 38952\n",
      "Replacing noun_phrase 2200 of 38952\n",
      "Replacing noun_phrase 2300 of 38952\n",
      "Replacing noun_phrase 2400 of 38952\n",
      "Replacing noun_phrase 2500 of 38952\n",
      "Replacing noun_phrase 2600 of 38952\n",
      "Replacing noun_phrase 2700 of 38952\n",
      "Replacing noun_phrase 2800 of 38952\n",
      "Replacing noun_phrase 2900 of 38952\n",
      "Replacing noun_phrase 3000 of 38952\n",
      "Replacing noun_phrase 3100 of 38952\n",
      "Replacing noun_phrase 3200 of 38952\n",
      "Replacing noun_phrase 3300 of 38952\n",
      "Replacing noun_phrase 3400 of 38952\n",
      "Replacing noun_phrase 3500 of 38952\n",
      "Replacing noun_phrase 3600 of 38952\n",
      "Replacing noun_phrase 3700 of 38952\n",
      "Replacing noun_phrase 3800 of 38952\n",
      "Replacing noun_phrase 3900 of 38952\n",
      "Replacing noun_phrase 4000 of 38952\n",
      "Replacing noun_phrase 4100 of 38952\n",
      "Replacing noun_phrase 4200 of 38952\n",
      "Replacing noun_phrase 4300 of 38952\n",
      "Replacing noun_phrase 4400 of 38952\n",
      "Replacing noun_phrase 4500 of 38952\n",
      "Replacing noun_phrase 4600 of 38952\n",
      "Replacing noun_phrase 4700 of 38952\n",
      "Replacing noun_phrase 4800 of 38952\n",
      "Replacing noun_phrase 4900 of 38952\n",
      "Replacing noun_phrase 5000 of 38952\n",
      "Replacing noun_phrase 5100 of 38952\n",
      "Replacing noun_phrase 5200 of 38952\n",
      "Replacing noun_phrase 5300 of 38952\n",
      "Replacing noun_phrase 5400 of 38952\n",
      "Replacing noun_phrase 5500 of 38952\n",
      "Replacing noun_phrase 5600 of 38952\n",
      "Replacing noun_phrase 5700 of 38952\n",
      "Replacing noun_phrase 5800 of 38952\n",
      "Replacing noun_phrase 5900 of 38952\n",
      "Replacing noun_phrase 6000 of 38952\n",
      "Replacing noun_phrase 6100 of 38952\n",
      "Replacing noun_phrase 6200 of 38952\n",
      "Replacing noun_phrase 6300 of 38952\n",
      "Replacing noun_phrase 6400 of 38952\n",
      "Replacing noun_phrase 6500 of 38952\n",
      "Replacing noun_phrase 6600 of 38952\n",
      "Replacing noun_phrase 6700 of 38952\n",
      "Replacing noun_phrase 6800 of 38952\n",
      "Replacing noun_phrase 6900 of 38952\n",
      "Replacing noun_phrase 7000 of 38952\n",
      "Replacing noun_phrase 7100 of 38952\n",
      "Replacing noun_phrase 7200 of 38952\n",
      "Replacing noun_phrase 7300 of 38952\n",
      "Replacing noun_phrase 7400 of 38952\n",
      "Replacing noun_phrase 7500 of 38952\n",
      "Replacing noun_phrase 7600 of 38952\n",
      "Replacing noun_phrase 7700 of 38952\n",
      "Replacing noun_phrase 7800 of 38952\n",
      "Replacing noun_phrase 7900 of 38952\n",
      "Replacing noun_phrase 8000 of 38952\n",
      "Replacing noun_phrase 8100 of 38952\n",
      "Replacing noun_phrase 8200 of 38952\n",
      "Replacing noun_phrase 8300 of 38952\n",
      "Replacing noun_phrase 8400 of 38952\n",
      "Replacing noun_phrase 8500 of 38952\n",
      "Replacing noun_phrase 8600 of 38952\n",
      "Replacing noun_phrase 8700 of 38952\n",
      "Replacing noun_phrase 8800 of 38952\n",
      "Replacing noun_phrase 8900 of 38952\n",
      "Replacing noun_phrase 9000 of 38952\n",
      "Replacing noun_phrase 9100 of 38952\n",
      "Replacing noun_phrase 9200 of 38952\n",
      "Replacing noun_phrase 9300 of 38952\n",
      "Replacing noun_phrase 9400 of 38952\n",
      "Replacing noun_phrase 9500 of 38952\n",
      "Replacing noun_phrase 9600 of 38952\n",
      "Replacing noun_phrase 9700 of 38952\n",
      "Replacing noun_phrase 9800 of 38952\n",
      "Replacing noun_phrase 9900 of 38952\n",
      "Replacing noun_phrase 10000 of 38952\n",
      "Replacing noun_phrase 10100 of 38952\n",
      "Replacing noun_phrase 10200 of 38952\n",
      "Replacing noun_phrase 10300 of 38952\n",
      "Replacing noun_phrase 10400 of 38952\n",
      "Replacing noun_phrase 10500 of 38952\n",
      "Replacing noun_phrase 10600 of 38952\n",
      "Replacing noun_phrase 10700 of 38952\n",
      "Replacing noun_phrase 10800 of 38952\n",
      "Replacing noun_phrase 10900 of 38952\n",
      "Replacing noun_phrase 11000 of 38952\n",
      "Replacing noun_phrase 11100 of 38952\n",
      "Replacing noun_phrase 11200 of 38952\n",
      "Replacing noun_phrase 11300 of 38952\n",
      "Replacing noun_phrase 11400 of 38952\n",
      "Replacing noun_phrase 11500 of 38952\n",
      "Replacing noun_phrase 11600 of 38952\n",
      "Replacing noun_phrase 11700 of 38952\n",
      "Replacing noun_phrase 11800 of 38952\n",
      "Replacing noun_phrase 11900 of 38952\n",
      "Replacing noun_phrase 12000 of 38952\n",
      "Replacing noun_phrase 12100 of 38952\n",
      "Replacing noun_phrase 12200 of 38952\n",
      "Replacing noun_phrase 12300 of 38952\n",
      "Replacing noun_phrase 12400 of 38952\n",
      "Replacing noun_phrase 12500 of 38952\n",
      "Replacing noun_phrase 12600 of 38952\n",
      "Replacing noun_phrase 12700 of 38952\n",
      "Replacing noun_phrase 12800 of 38952\n",
      "Replacing noun_phrase 12900 of 38952\n",
      "Replacing noun_phrase 13000 of 38952\n",
      "Replacing noun_phrase 13100 of 38952\n",
      "Replacing noun_phrase 13200 of 38952\n",
      "Replacing noun_phrase 13300 of 38952\n",
      "Replacing noun_phrase 13400 of 38952\n",
      "Replacing noun_phrase 13500 of 38952\n",
      "Replacing noun_phrase 13600 of 38952\n",
      "Replacing noun_phrase 13700 of 38952\n",
      "Replacing noun_phrase 13800 of 38952\n",
      "Replacing noun_phrase 13900 of 38952\n",
      "Replacing noun_phrase 14000 of 38952\n",
      "Replacing noun_phrase 14100 of 38952\n",
      "Replacing noun_phrase 14200 of 38952\n",
      "Replacing noun_phrase 14300 of 38952\n",
      "Replacing noun_phrase 14400 of 38952\n",
      "Replacing noun_phrase 14500 of 38952\n",
      "Replacing noun_phrase 14600 of 38952\n",
      "Replacing noun_phrase 14700 of 38952\n",
      "Replacing noun_phrase 14800 of 38952\n",
      "Replacing noun_phrase 14900 of 38952\n",
      "Replacing noun_phrase 15000 of 38952\n",
      "Replacing noun_phrase 15100 of 38952\n",
      "Replacing noun_phrase 15200 of 38952\n",
      "Replacing noun_phrase 15300 of 38952\n",
      "Replacing noun_phrase 15400 of 38952\n",
      "Replacing noun_phrase 15500 of 38952\n",
      "Replacing noun_phrase 15600 of 38952\n",
      "Replacing noun_phrase 15700 of 38952\n",
      "Replacing noun_phrase 15800 of 38952\n",
      "Replacing noun_phrase 15900 of 38952\n",
      "Replacing noun_phrase 16000 of 38952\n",
      "Replacing noun_phrase 16100 of 38952\n",
      "Replacing noun_phrase 16200 of 38952\n",
      "Replacing noun_phrase 16300 of 38952\n",
      "Replacing noun_phrase 16400 of 38952\n",
      "Replacing noun_phrase 16500 of 38952\n",
      "Replacing noun_phrase 16600 of 38952\n",
      "Replacing noun_phrase 16700 of 38952\n",
      "Replacing noun_phrase 16800 of 38952\n",
      "Replacing noun_phrase 16900 of 38952\n",
      "Replacing noun_phrase 17000 of 38952\n",
      "Replacing noun_phrase 17100 of 38952\n",
      "Replacing noun_phrase 17200 of 38952\n",
      "Replacing noun_phrase 17300 of 38952\n",
      "Replacing noun_phrase 17400 of 38952\n",
      "Replacing noun_phrase 17500 of 38952\n",
      "Replacing noun_phrase 17600 of 38952\n",
      "Replacing noun_phrase 17700 of 38952\n",
      "Replacing noun_phrase 17800 of 38952\n",
      "Replacing noun_phrase 17900 of 38952\n",
      "Replacing noun_phrase 18000 of 38952\n",
      "Replacing noun_phrase 18100 of 38952\n",
      "Replacing noun_phrase 18200 of 38952\n",
      "Replacing noun_phrase 18300 of 38952\n",
      "Replacing noun_phrase 18400 of 38952\n",
      "Replacing noun_phrase 18500 of 38952\n",
      "Replacing noun_phrase 18600 of 38952\n",
      "Replacing noun_phrase 18700 of 38952\n",
      "Replacing noun_phrase 18800 of 38952\n",
      "Replacing noun_phrase 18900 of 38952\n",
      "Replacing noun_phrase 19000 of 38952\n",
      "Replacing noun_phrase 19100 of 38952\n",
      "Replacing noun_phrase 19200 of 38952\n",
      "Replacing noun_phrase 19300 of 38952\n",
      "Replacing noun_phrase 19400 of 38952\n",
      "Replacing noun_phrase 19500 of 38952\n",
      "Replacing noun_phrase 19600 of 38952\n",
      "Replacing noun_phrase 19700 of 38952\n",
      "Replacing noun_phrase 19800 of 38952\n",
      "Replacing noun_phrase 19900 of 38952\n",
      "Replacing noun_phrase 20000 of 38952\n",
      "Replacing noun_phrase 20100 of 38952\n",
      "Replacing noun_phrase 20200 of 38952\n",
      "Replacing noun_phrase 20300 of 38952\n",
      "Replacing noun_phrase 20400 of 38952\n",
      "Replacing noun_phrase 20500 of 38952\n",
      "Replacing noun_phrase 20600 of 38952\n",
      "Replacing noun_phrase 20700 of 38952\n",
      "Replacing noun_phrase 20800 of 38952\n",
      "Replacing noun_phrase 20900 of 38952\n",
      "Replacing noun_phrase 21000 of 38952\n",
      "Replacing noun_phrase 21100 of 38952\n",
      "Replacing noun_phrase 21200 of 38952\n",
      "Replacing noun_phrase 21300 of 38952\n",
      "Replacing noun_phrase 21400 of 38952\n",
      "Replacing noun_phrase 21500 of 38952\n",
      "Replacing noun_phrase 21600 of 38952\n",
      "Replacing noun_phrase 21700 of 38952\n",
      "Replacing noun_phrase 21800 of 38952\n",
      "Replacing noun_phrase 21900 of 38952\n",
      "Replacing noun_phrase 22000 of 38952\n",
      "Replacing noun_phrase 22100 of 38952\n",
      "Replacing noun_phrase 22200 of 38952\n",
      "Replacing noun_phrase 22300 of 38952\n",
      "Replacing noun_phrase 22400 of 38952\n",
      "Replacing noun_phrase 22500 of 38952\n",
      "Replacing noun_phrase 22600 of 38952\n",
      "Replacing noun_phrase 22700 of 38952\n",
      "Replacing noun_phrase 22800 of 38952\n",
      "Replacing noun_phrase 22900 of 38952\n",
      "Replacing noun_phrase 23000 of 38952\n",
      "Replacing noun_phrase 23100 of 38952\n",
      "Replacing noun_phrase 23200 of 38952\n",
      "Replacing noun_phrase 23300 of 38952\n",
      "Replacing noun_phrase 23400 of 38952\n",
      "Replacing noun_phrase 23500 of 38952\n",
      "Replacing noun_phrase 23600 of 38952\n",
      "Replacing noun_phrase 23700 of 38952\n",
      "Replacing noun_phrase 23800 of 38952\n",
      "Replacing noun_phrase 23900 of 38952\n",
      "Replacing noun_phrase 24000 of 38952\n",
      "Replacing noun_phrase 24100 of 38952\n",
      "Replacing noun_phrase 24200 of 38952\n",
      "Replacing noun_phrase 24300 of 38952\n",
      "Replacing noun_phrase 24400 of 38952\n",
      "Replacing noun_phrase 24500 of 38952\n",
      "Replacing noun_phrase 24600 of 38952\n",
      "Replacing noun_phrase 24700 of 38952\n",
      "Replacing noun_phrase 24800 of 38952\n",
      "Replacing noun_phrase 24900 of 38952\n",
      "Replacing noun_phrase 25000 of 38952\n",
      "Replacing noun_phrase 25100 of 38952\n",
      "Replacing noun_phrase 25200 of 38952\n",
      "Replacing noun_phrase 25300 of 38952\n",
      "Replacing noun_phrase 25400 of 38952\n",
      "Replacing noun_phrase 25500 of 38952\n",
      "Replacing noun_phrase 25600 of 38952\n",
      "Replacing noun_phrase 25700 of 38952\n",
      "Replacing noun_phrase 25800 of 38952\n",
      "Replacing noun_phrase 25900 of 38952\n",
      "Replacing noun_phrase 26000 of 38952\n",
      "Replacing noun_phrase 26100 of 38952\n",
      "Replacing noun_phrase 26200 of 38952\n",
      "Replacing noun_phrase 26300 of 38952\n",
      "Replacing noun_phrase 26400 of 38952\n",
      "Replacing noun_phrase 26500 of 38952\n",
      "Replacing noun_phrase 26600 of 38952\n",
      "Replacing noun_phrase 26700 of 38952\n",
      "Replacing noun_phrase 26800 of 38952\n",
      "Replacing noun_phrase 26900 of 38952\n",
      "Replacing noun_phrase 27000 of 38952\n",
      "Replacing noun_phrase 27100 of 38952\n",
      "Replacing noun_phrase 27200 of 38952\n",
      "Replacing noun_phrase 27300 of 38952\n",
      "Replacing noun_phrase 27400 of 38952\n",
      "Replacing noun_phrase 27500 of 38952\n",
      "Replacing noun_phrase 27600 of 38952\n",
      "Replacing noun_phrase 27700 of 38952\n",
      "Replacing noun_phrase 27800 of 38952\n",
      "Replacing noun_phrase 27900 of 38952\n",
      "Replacing noun_phrase 28000 of 38952\n",
      "Replacing noun_phrase 28100 of 38952\n",
      "Replacing noun_phrase 28200 of 38952\n",
      "Replacing noun_phrase 28300 of 38952\n",
      "Replacing noun_phrase 28400 of 38952\n",
      "Replacing noun_phrase 28500 of 38952\n",
      "Replacing noun_phrase 28600 of 38952\n",
      "Replacing noun_phrase 28700 of 38952\n",
      "Replacing noun_phrase 28800 of 38952\n",
      "Replacing noun_phrase 28900 of 38952\n",
      "Replacing noun_phrase 29000 of 38952\n",
      "Replacing noun_phrase 29100 of 38952\n",
      "Replacing noun_phrase 29200 of 38952\n",
      "Replacing noun_phrase 29300 of 38952\n",
      "Replacing noun_phrase 29400 of 38952\n",
      "Replacing noun_phrase 29500 of 38952\n",
      "Replacing noun_phrase 29600 of 38952\n",
      "Replacing noun_phrase 29700 of 38952\n",
      "Replacing noun_phrase 29800 of 38952\n",
      "Replacing noun_phrase 29900 of 38952\n",
      "Replacing noun_phrase 30000 of 38952\n",
      "Replacing noun_phrase 30100 of 38952\n",
      "Replacing noun_phrase 30200 of 38952\n",
      "Replacing noun_phrase 30300 of 38952\n",
      "Replacing noun_phrase 30400 of 38952\n",
      "Replacing noun_phrase 30500 of 38952\n",
      "Replacing noun_phrase 30600 of 38952\n",
      "Replacing noun_phrase 30700 of 38952\n",
      "Replacing noun_phrase 30800 of 38952\n",
      "Replacing noun_phrase 30900 of 38952\n",
      "Replacing noun_phrase 31000 of 38952\n",
      "Replacing noun_phrase 31100 of 38952\n",
      "Replacing noun_phrase 31200 of 38952\n",
      "Replacing noun_phrase 31300 of 38952\n",
      "Replacing noun_phrase 31400 of 38952\n",
      "Replacing noun_phrase 31500 of 38952\n",
      "Replacing noun_phrase 31600 of 38952\n",
      "Replacing noun_phrase 31700 of 38952\n",
      "Replacing noun_phrase 31800 of 38952\n",
      "Replacing noun_phrase 31900 of 38952\n",
      "Replacing noun_phrase 32000 of 38952\n",
      "Replacing noun_phrase 32100 of 38952\n",
      "Replacing noun_phrase 32200 of 38952\n",
      "Replacing noun_phrase 32300 of 38952\n",
      "Replacing noun_phrase 32400 of 38952\n",
      "Replacing noun_phrase 32500 of 38952\n",
      "Replacing noun_phrase 32600 of 38952\n",
      "Replacing noun_phrase 32700 of 38952\n",
      "Replacing noun_phrase 32800 of 38952\n",
      "Replacing noun_phrase 32900 of 38952\n",
      "Replacing noun_phrase 33000 of 38952\n",
      "Replacing noun_phrase 33100 of 38952\n",
      "Replacing noun_phrase 33200 of 38952\n",
      "Replacing noun_phrase 33300 of 38952\n",
      "Replacing noun_phrase 33400 of 38952\n",
      "Replacing noun_phrase 33500 of 38952\n",
      "Replacing noun_phrase 33600 of 38952\n",
      "Replacing noun_phrase 33700 of 38952\n",
      "Replacing noun_phrase 33800 of 38952\n",
      "Replacing noun_phrase 33900 of 38952\n",
      "Replacing noun_phrase 34000 of 38952\n",
      "Replacing noun_phrase 34100 of 38952\n",
      "Replacing noun_phrase 34200 of 38952\n",
      "Replacing noun_phrase 34300 of 38952\n",
      "Replacing noun_phrase 34400 of 38952\n",
      "Replacing noun_phrase 34500 of 38952\n",
      "Replacing noun_phrase 34600 of 38952\n",
      "Replacing noun_phrase 34700 of 38952\n",
      "Replacing noun_phrase 34800 of 38952\n",
      "Replacing noun_phrase 34900 of 38952\n",
      "Replacing noun_phrase 35000 of 38952\n",
      "Replacing noun_phrase 35100 of 38952\n",
      "Replacing noun_phrase 35200 of 38952\n",
      "Replacing noun_phrase 35300 of 38952\n",
      "Replacing noun_phrase 35400 of 38952\n",
      "Replacing noun_phrase 35500 of 38952\n",
      "Replacing noun_phrase 35600 of 38952\n",
      "Replacing noun_phrase 35700 of 38952\n",
      "Replacing noun_phrase 35800 of 38952\n",
      "Replacing noun_phrase 35900 of 38952\n",
      "Replacing noun_phrase 36000 of 38952\n",
      "Replacing noun_phrase 36100 of 38952\n",
      "Replacing noun_phrase 36200 of 38952\n",
      "Replacing noun_phrase 36300 of 38952\n",
      "Replacing noun_phrase 36400 of 38952\n",
      "Replacing noun_phrase 36500 of 38952\n",
      "Replacing noun_phrase 36600 of 38952\n",
      "Replacing noun_phrase 36700 of 38952\n",
      "Replacing noun_phrase 36800 of 38952\n",
      "Replacing noun_phrase 36900 of 38952\n",
      "Replacing noun_phrase 37000 of 38952\n",
      "Replacing noun_phrase 37100 of 38952\n",
      "Replacing noun_phrase 37200 of 38952\n",
      "Replacing noun_phrase 37300 of 38952\n",
      "Replacing noun_phrase 37400 of 38952\n",
      "Replacing noun_phrase 37500 of 38952\n",
      "Replacing noun_phrase 37600 of 38952\n",
      "Replacing noun_phrase 37700 of 38952\n",
      "Replacing noun_phrase 37800 of 38952\n",
      "Replacing noun_phrase 37900 of 38952\n",
      "Replacing noun_phrase 38000 of 38952\n",
      "Replacing noun_phrase 38100 of 38952\n",
      "Replacing noun_phrase 38200 of 38952\n",
      "Replacing noun_phrase 38300 of 38952\n",
      "Replacing noun_phrase 38400 of 38952\n",
      "Replacing noun_phrase 38500 of 38952\n",
      "Replacing noun_phrase 38600 of 38952\n",
      "Replacing noun_phrase 38700 of 38952\n",
      "Replacing noun_phrase 38800 of 38952\n",
      "Replacing noun_phrase 38900 of 38952\n"
     ]
    }
   ],
   "source": [
    "if SHOULD_GENERATE_UNDERSCORED_CORPUS:\n",
    "    for i, noun_phrase in enumerate(noun_phrases):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Replacing noun_phrase {i + 1} of {len(noun_phrases)}')\n",
    "        found_indices = set()\n",
    "        while find_phrase_in_corpus(corpus, noun_phrase) != (-1, -1) and find_phrase_in_corpus(corpus, noun_phrase)[0] not in found_indices:\n",
    "            s_idx, e_idx = find_phrase_in_corpus(corpus, noun_phrase)\n",
    "            found_indices.add(s_idx)\n",
    "            corpus = corpus[:s_idx] + noun_phrase.replace(' ', '_') + corpus[e_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "disciplinary-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOULD_GENERATE_UNDERSCORED_CORPUS:\n",
    "    pickle.dump(corpus, open('data/underscored_corpus.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "greater-island",
   "metadata": {},
   "outputs": [],
   "source": [
    "underscored_corpus = pickle.load(open('data/underscored_corpus.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afraid-graphic",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOULD_TRAIN_WORD2VEC_MODEL:\n",
    "    underscored_corpus_data = []\n",
    "    for i in sent_tokenize(underscored_corpus):\n",
    "        temp = []\n",
    "        for j in word_tokenize(i):\n",
    "            temp.append(j.lower())\n",
    "        underscored_corpus_data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "loved-carrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOULD_TRAIN_WORD2VEC_MODEL:\n",
    "    word2vec_model = Word2Vec(underscored_corpus_data, min_count=1, window=WINDOW_SIZE, size=VECTOR_SIZE)\n",
    "    word2vec_model.save(f'data/word2vec_model_vs_{VECTOR_SIZE}_ws_{WINDOW_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "expired-weekly",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec.load(f'data/word2vec_model_vs_{VECTOR_SIZE}_ws_{WINDOW_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "economic-activation",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token for token in list(word2vec_model.wv.vocab.keys())]\n",
    "embeddings = {token: word2vec_model.wv[token] for token in tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "signed-blowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_samples = [ps for ps in positive_samples if ps.replace(' ', '_') in embeddings and word2vec_model.wv.vocab[ps.replace(' ', '_')].count >= MIN_FREQUENCY]\n",
    "negative_samples = [ns for ns in negative_samples if ns.replace(' ', '_') in embeddings and word2vec_model.wv.vocab[ns.replace(' ', '_')].count >= MIN_FREQUENCY]\n",
    "\n",
    "positive_samples = positive_samples[:min(len(positive_samples), len(negative_samples))]\n",
    "negative_samples = negative_samples[:min(len(positive_samples), len(negative_samples))]\n",
    "\n",
    "ps_set = set(positive_samples)\n",
    "ns_set = set(negative_samples)\n",
    "\n",
    "noun_phrases = [np for np in noun_phrases if np not in ps_set and np not in ns_set and np.replace(' ', '_') in embeddings and word2vec_model.wv.vocab[np.replace(' ', '_')].count >= MIN_FREQUENCY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "pointed-following",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for phrase in positive_samples:\n",
    "    X.append(embeddings[phrase.replace(' ', '_')])\n",
    "    y.append(1)\n",
    "for phrase in negative_samples:\n",
    "    X.append(embeddings[phrase.replace(' ', '_')])\n",
    "    y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "delayed-freedom",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = list(zip(X, y))\n",
    "random.shuffle(c)\n",
    "X, y = zip(*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dimensional-cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "rolled-millennium",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "\n",
    "        self.layer_1 = nn.Linear(VECTOR_SIZE, 128)\n",
    "        \n",
    "        self.layers = []\n",
    "        for _ in range(NUM_LAYERS - 1):\n",
    "            self.layers.append(nn.Linear(128, 128))\n",
    "        \n",
    "        self.layer_out = nn.Linear(128, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = self.relu(layer(x))\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "conceptual-omaha",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryClassifier(\n",
       "  (layer_1): Linear(in_features=200, out_features=128, bias=True)\n",
       "  (layer_out): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BinaryClassifier()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "unavailable-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "greater-brain",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "train_data = TrainDataset(torch.FloatTensor(np.array(X_train, dtype=np.float64)), \n",
    "                          torch.FloatTensor(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "institutional-conspiracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "\n",
    "test_data = TestDataset(torch.FloatTensor(np.array(X_test, dtype=np.float64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "speaking-actor",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "changed-think",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "liberal-jacob",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010: | Loss: 0.61554 | Acc: 68.560\n",
      "Epoch 020: | Loss: 0.56229 | Acc: 70.800\n",
      "Epoch 030: | Loss: 0.53755 | Acc: 72.200\n",
      "Epoch 040: | Loss: 0.51566 | Acc: 73.760\n",
      "Epoch 050: | Loss: 0.50253 | Acc: 75.080\n",
      "Epoch 060: | Loss: 0.48779 | Acc: 75.920\n",
      "Epoch 070: | Loss: 0.47238 | Acc: 76.800\n",
      "Epoch 080: | Loss: 0.46857 | Acc: 77.160\n",
      "Epoch 090: | Loss: 0.45862 | Acc: 77.280\n",
      "Epoch 100: | Loss: 0.44780 | Acc: 77.360\n",
      "Epoch 110: | Loss: 0.43806 | Acc: 78.280\n",
      "Epoch 120: | Loss: 0.42915 | Acc: 78.120\n",
      "Epoch 130: | Loss: 0.42767 | Acc: 78.880\n",
      "Epoch 140: | Loss: 0.42287 | Acc: 77.920\n",
      "Epoch 150: | Loss: 0.41563 | Acc: 78.600\n",
      "Epoch 160: | Loss: 0.41494 | Acc: 78.440\n",
      "Epoch 170: | Loss: 0.41445 | Acc: 78.760\n",
      "Epoch 180: | Loss: 0.40144 | Acc: 80.360\n",
      "Epoch 190: | Loss: 0.39415 | Acc: 79.880\n",
      "Epoch 200: | Loss: 0.38854 | Acc: 81.080\n"
     ]
    }
   ],
   "source": [
    "epoch_losses = []\n",
    "for e in range(1, EPOCHS + 1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "\n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    epoch_losses.append(epoch_loss / len(train_loader)) \n",
    "\n",
    "    if e % 10 == 0:\n",
    "        print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "impressed-college",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc77144b640>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "quantitative-demonstration",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "respective-person",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[121  57]\n",
      " [ 58 153]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.68      0.68       178\n",
      "           1       0.73      0.73      0.73       211\n",
      "\n",
      "    accuracy                           0.70       389\n",
      "   macro avg       0.70      0.70      0.70       389\n",
      "weighted avg       0.70      0.70      0.70       389\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred_list))\n",
    "print(classification_report(y_test, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "assisted-accordance",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = random.sample(noun_phrases, 1000)\n",
    "X_samples = []\n",
    "for sample in test_samples:\n",
    "    X_samples.append(embeddings[sample.replace(' ', '_')])\n",
    "sample_data = TestDataset(torch.FloatTensor(np.array(X_samples, dtype=np.float64)))\n",
    "sample_loader = DataLoader(dataset=sample_data, batch_size=1)\n",
    "\n",
    "extracted = {}\n",
    "not_extracted = {}\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, X_batch in enumerate(sample_loader):\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        if y_pred_tag.cpu().numpy()[0][0] == 1:\n",
    "            extracted[test_samples[i]] = y_test_pred.item()\n",
    "        else:\n",
    "            not_extracted[test_samples[i]] = y_test_pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bored-sympathy",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = set(['a', 'the', 'an'])\n",
    "extracted = {k: v for k, v in sorted(extracted.items(), key=lambda x: x[1], reverse=True) if k.split()[0] not in articles}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fluid-description",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 0.9999998807907104,\n",
       " 'method': 0.9999995231628418,\n",
       " 'patient': 0.9994611144065857,\n",
       " 'short term memory': 0.9993560910224915,\n",
       " 'part': 0.9979509711265564,\n",
       " 'focus': 0.997767448425293,\n",
       " 'random field': 0.9943311214447021,\n",
       " 'area': 0.9942725896835327,\n",
       " 'limits': 0.9936187267303467,\n",
       " 'general': 0.991628885269165,\n",
       " 'de': 0.9913837909698486,\n",
       " 'us': 0.9913699626922607,\n",
       " 'fisher': 0.9892249703407288,\n",
       " 'mobile': 0.9887565970420837,\n",
       " 'basis': 0.9868499040603638,\n",
       " 'scale': 0.9850148558616638,\n",
       " 'nonlinear': 0.9844915270805359,\n",
       " 'emotion': 0.9839656352996826,\n",
       " 'integer': 0.9772638082504272,\n",
       " 'mri': 0.9770360589027405,\n",
       " 'mixture': 0.9743661880493164,\n",
       " 'decision': 0.9731950163841248,\n",
       " 'w': 0.9696369767189026,\n",
       " 'maximum': 0.9694655537605286,\n",
       " 'extracts': 0.9650328755378723,\n",
       " '2013': 0.9628392457962036,\n",
       " 'unsupervised': 0.9626750349998474,\n",
       " 'fmri': 0.9589870572090149,\n",
       " 'item': 0.9582691788673401,\n",
       " 'method which': 0.9574536085128784,\n",
       " 'news': 0.9567950963973999,\n",
       " '2012': 0.9540184736251831,\n",
       " 'variational': 0.9536139369010925,\n",
       " 'qa': 0.9503633379936218,\n",
       " 'fit': 0.9498841166496277,\n",
       " 'nature': 0.9473811388015747,\n",
       " 'kernel learning': 0.93581622838974,\n",
       " 'sketch': 0.9350128173828125,\n",
       " 'majority': 0.9291422963142395,\n",
       " 'service': 0.9283223152160645,\n",
       " 'dependency': 0.9265044331550598,\n",
       " 'direction': 0.9249566793441772,\n",
       " 'play': 0.9238008856773376,\n",
       " 'compositional': 0.9204526543617249,\n",
       " 'generation': 0.9174357652664185,\n",
       " 'theta': 0.9108605980873108,\n",
       " 'fourier': 0.9086447358131409,\n",
       " 'line': 0.9074322581291199,\n",
       " 'go': 0.9057429432868958,\n",
       " 'unitary': 0.9049962162971497,\n",
       " 'technology': 0.9041593670845032,\n",
       " 'discrimination': 0.9031509757041931,\n",
       " 'factorization': 0.8950120806694031,\n",
       " 'answer': 0.8943172097206116,\n",
       " 'design': 0.8831883072853088,\n",
       " 'high probability': 0.8805208802223206,\n",
       " 'k means': 0.8760547637939453,\n",
       " 'neighbors': 0.8744484782218933,\n",
       " 'adaptive': 0.8716769218444824,\n",
       " 'attracting': 0.8652558922767639,\n",
       " 'registration': 0.8622146844863892,\n",
       " 'management': 0.853165328502655,\n",
       " 'music': 0.852361261844635,\n",
       " 'new learning': 0.8472855091094971,\n",
       " 'coefficients': 0.843779444694519,\n",
       " 'lack': 0.8403087258338928,\n",
       " 'news articles': 0.8352920413017273,\n",
       " 'matlab': 0.8301951289176941,\n",
       " 'reasoning': 0.8299708962440491,\n",
       " 'bayesian approach': 0.8299199342727661,\n",
       " 'operators': 0.8260147571563721,\n",
       " 'dr': 0.8252779841423035,\n",
       " 'non stationary': 0.8186236619949341,\n",
       " 'point processes': 0.8178609013557434,\n",
       " 'daily': 0.8158584237098694,\n",
       " 'smile': 0.8135711550712585,\n",
       " 'mine': 0.8113799095153809,\n",
       " 'chatbot': 0.8045154213905334,\n",
       " 'brain computer': 0.8017062544822693,\n",
       " 'face verification': 0.8015995621681213,\n",
       " 'grade': 0.7969911694526672,\n",
       " 'credit': 0.7885535359382629,\n",
       " 'best': 0.7879485487937927,\n",
       " 'semantic': 0.786170244216919,\n",
       " 'identifiability': 0.7859587073326111,\n",
       " 'gibbs': 0.781859278678894,\n",
       " 'reference': 0.780108630657196,\n",
       " 'multiple instances': 0.7793635725975037,\n",
       " 'semantic segmentation': 0.7741290330886841,\n",
       " 'online convex optimization': 0.7711557149887085,\n",
       " 'cost': 0.7704012393951416,\n",
       " 'tomography ct': 0.767936646938324,\n",
       " 'minibatch': 0.7658138871192932,\n",
       " 'fi': 0.7651628851890564,\n",
       " 'optimal decision': 0.7634952664375305,\n",
       " 'particular': 0.7634599804878235,\n",
       " 'nearest': 0.7623050808906555,\n",
       " 'self organizing': 0.7606631517410278,\n",
       " 'recent applications': 0.7574985027313232,\n",
       " 'lasso': 0.7564274668693542,\n",
       " 'winner': 0.755757212638855,\n",
       " 'diversity': 0.7537391781806946,\n",
       " 'beam': 0.753573477268219,\n",
       " 'approximate inference': 0.7475582361221313,\n",
       " 'voice': 0.742924153804779,\n",
       " 'sub sampling': 0.7422422170639038,\n",
       " 'em': 0.7390334606170654,\n",
       " 'important class': 0.7358119487762451,\n",
       " 'square loss': 0.7349491715431213,\n",
       " 'numeric': 0.7263067960739136,\n",
       " 'additive noise': 0.7258305549621582,\n",
       " 'word vectors': 0.7244808077812195,\n",
       " 'separation': 0.7242253422737122,\n",
       " 'potential outcomes': 0.7233920693397522,\n",
       " 'dirt': 0.7229902744293213,\n",
       " 'cameras': 0.7221015691757202,\n",
       " 'movements': 0.71833336353302,\n",
       " 'optimal transport': 0.7179822325706482,\n",
       " 'bayesian learning': 0.7172282338142395,\n",
       " 'hboa': 0.7158942222595215,\n",
       " 'low power': 0.7147818803787231,\n",
       " 'one step': 0.7125294804573059,\n",
       " 'metric learning problem': 0.7072250843048096,\n",
       " 'inter': 0.7059838175773621,\n",
       " 'semantic representation': 0.7056769728660583,\n",
       " 'sentiment classification': 0.7042703032493591,\n",
       " 'ambiguity': 0.7041237354278564,\n",
       " 'simple gradient': 0.7026593089103699,\n",
       " 'potts': 0.7018962502479553,\n",
       " 'mikolov': 0.7018905282020569,\n",
       " 'intervention': 0.6965631246566772,\n",
       " 'text recognition': 0.6946998238563538,\n",
       " 'huge amount': 0.6944459676742554,\n",
       " 'daily activities': 0.6939939260482788,\n",
       " 'metric learning': 0.6928306818008423,\n",
       " 'grading': 0.6927687525749207,\n",
       " 'appearance': 0.6880561709403992,\n",
       " 'sigma': 0.6834385395050049,\n",
       " 'terrain': 0.6831393837928772,\n",
       " 'ce': 0.6821833252906799,\n",
       " 'non linear': 0.6771082878112793,\n",
       " 'enhancement': 0.6751119494438171,\n",
       " 'gesture recognition': 0.6733800768852234,\n",
       " 'meetings': 0.6727458834648132,\n",
       " 'loss of accuracy': 0.6686060428619385,\n",
       " 'image reconstruction': 0.6627328991889954,\n",
       " 'spatial information': 0.6625937819480896,\n",
       " 'arrays': 0.6558825373649597,\n",
       " 'hyperplanes': 0.6546406149864197,\n",
       " 'euclidean space': 0.6485949754714966,\n",
       " 'target classes': 0.6481941342353821,\n",
       " 'doesn t': 0.6473740339279175,\n",
       " 'natural language descriptions': 0.6453171968460083,\n",
       " 'meaningful features': 0.6451095342636108,\n",
       " 'i': 0.6435583829879761,\n",
       " 'sample points': 0.6418794989585876,\n",
       " 'k medoids': 0.640227198600769,\n",
       " 'flats': 0.6402095556259155,\n",
       " 'aide': 0.6397504806518555,\n",
       " 'q sigma': 0.6380296945571899,\n",
       " 'robust optimization': 0.6346117854118347,\n",
       " 'financial data': 0.6330209970474243,\n",
       " 'document collections': 0.6291562914848328,\n",
       " 'image and text': 0.6284135580062866,\n",
       " 'painting': 0.6264709830284119,\n",
       " 'rs': 0.6210165023803711,\n",
       " 'diagrams': 0.6202172040939331,\n",
       " 'factors of variation': 0.6190899610519409,\n",
       " 'great attention': 0.6172486543655396,\n",
       " 'binary features': 0.615828812122345,\n",
       " 'sensor noise': 0.6147379875183105,\n",
       " 'network dnn': 0.6112543940544128,\n",
       " 'natural language understanding': 0.611021876335144,\n",
       " 'data space': 0.6100270748138428,\n",
       " 'discriminative learning': 0.6098064184188843,\n",
       " 'moving objects': 0.6087846159934998,\n",
       " 'faces': 0.6086115837097168,\n",
       " 'you': 0.6048757433891296,\n",
       " 'weak learners': 0.6032553315162659,\n",
       " 'speaker verification': 0.6028883457183838,\n",
       " 'its number': 0.6002192497253418,\n",
       " 'carpenter': 0.6001092791557312,\n",
       " 'various constraints': 0.5983569025993347,\n",
       " 'object categorization': 0.597222626209259,\n",
       " 'compressive sensing': 0.5917360782623291,\n",
       " 'error bounds': 0.5908681750297546,\n",
       " 'gravitational waves': 0.5905643701553345,\n",
       " 'posts': 0.5887959599494934,\n",
       " 'data privacy': 0.5852588415145874,\n",
       " 'cancer diagnosis': 0.5832090377807617,\n",
       " 'densenets': 0.5831606984138489,\n",
       " 'depth information': 0.5808396935462952,\n",
       " 'problems': 0.5761120915412903,\n",
       " 'ridge regression': 0.5752540826797485,\n",
       " 'relational learning': 0.5734512805938721,\n",
       " 'non linear functions': 0.5724916458129883,\n",
       " 'neural turing machines': 0.5717982053756714,\n",
       " 'statistical inference': 0.567272961139679,\n",
       " 'human body': 0.5631901621818542,\n",
       " 'partitions': 0.560964047908783,\n",
       " 'radiomics': 0.5608695149421692,\n",
       " 'linguistic knowledge': 0.557620108127594,\n",
       " 'pedestrian detection': 0.5575447082519531,\n",
       " 'taxis': 0.554854154586792,\n",
       " 'care': 0.5510620474815369,\n",
       " 'experiences': 0.5507796406745911,\n",
       " 'human intelligence': 0.5493069291114807,\n",
       " 'dpps': 0.5488768815994263,\n",
       " 'cho': 0.5487460494041443,\n",
       " 'vlad': 0.5438080430030823,\n",
       " 'gibbs sampling': 0.5421066880226135,\n",
       " 'individual words': 0.5403285026550293,\n",
       " 'video understanding': 0.538684070110321,\n",
       " 'field theory': 0.538349986076355,\n",
       " 'classical approaches': 0.5378835201263428,\n",
       " 'saddle points': 0.5325909852981567,\n",
       " 'text to speech': 0.5316956639289856,\n",
       " 'underfitting': 0.5307314991950989,\n",
       " 'classification systems': 0.5275892019271851,\n",
       " 'task clustering': 0.5264892578125,\n",
       " 'different contexts': 0.5247994661331177,\n",
       " 'their applicability': 0.520602822303772,\n",
       " 'dynet': 0.5191729664802551,\n",
       " 'neural network model': 0.5163926482200623,\n",
       " 'item ratings': 0.5158485770225525,\n",
       " 'importance sampling': 0.5151367783546448,\n",
       " 'blood cells': 0.5149933099746704,\n",
       " 'gradient descent sgd': 0.513969898223877,\n",
       " 'natural language': 0.5138161778450012,\n",
       " 'different representations': 0.5134863257408142,\n",
       " 'realistic samples': 0.513103187084198,\n",
       " 'sparse approximations': 0.5125440359115601,\n",
       " 'knowledge': 0.5123599767684937,\n",
       " 'expressive power': 0.5110965371131897,\n",
       " 'feature selection method': 0.508030354976654,\n",
       " 'probabilistic formulation': 0.5071516633033752,\n",
       " 'attention mechanism': 0.504187285900116}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "quarterly-parade",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_extracted = {k: v for k, v in sorted(not_extracted.items(), key=lambda x: x[1], reverse=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "finnish-lightweight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the effectiveness': 5.73126953895553e-07,\n",
       " 'samples': 5.327800863597076e-06,\n",
       " 'works': 8.802426236798055e-06,\n",
       " 'instances': 3.7580819480353966e-05,\n",
       " 'size': 8.721094491193071e-05,\n",
       " 'values': 0.00010886265226872638,\n",
       " 'distributions': 0.00012562084884848446,\n",
       " 'less': 0.00013326818589121103,\n",
       " 'results': 0.00014533418288920075,\n",
       " 'networks': 0.0001558802032377571,\n",
       " 'words': 0.00021382964041549712,\n",
       " 'the current state': 0.0005454190541058779,\n",
       " 'cases': 0.0006047742208465934,\n",
       " 'improvements': 0.0006287351716309786,\n",
       " 'neural networks cnns': 0.0006426469190046191,\n",
       " 'elements': 0.0009186253882944584,\n",
       " 'the features': 0.0010169079760089517,\n",
       " 'advances': 0.0016336722765117884,\n",
       " 'paths': 0.0019249666947871447,\n",
       " 's': 0.002392793772742152,\n",
       " 'features': 0.003200931241735816,\n",
       " 'orders': 0.003708937205374241,\n",
       " 'the number': 0.004026979673653841,\n",
       " 'concepts': 0.004267136100679636,\n",
       " 'skills': 0.00452770758420229,\n",
       " 'a mixture': 0.004674387164413929,\n",
       " 'the classes': 0.005775765050202608,\n",
       " 'the efficiency': 0.006043335888534784,\n",
       " 'the components': 0.008222472853958607,\n",
       " 'nodes': 0.00829674769192934,\n",
       " 'the output': 0.008466312661767006,\n",
       " 'existing methods': 0.010127357207238674,\n",
       " 'different domains': 0.010674741119146347,\n",
       " 'long term dependencies': 0.011054969392716885,\n",
       " 'the difficulty': 0.012013071216642857,\n",
       " 'explored': 0.01214362308382988,\n",
       " 'desirable properties': 0.012514548376202583,\n",
       " 'the nodes': 0.012641514651477337,\n",
       " 'programs': 0.01311806682497263,\n",
       " 'training samples': 0.013144019059836864,\n",
       " 'many cases': 0.013527178205549717,\n",
       " 'products': 0.013707800768315792,\n",
       " 'performance measures': 0.01388006005436182,\n",
       " 'practical applications': 0.014371268451213837,\n",
       " 'the flexibility': 0.014776526018977165,\n",
       " 'exemplars': 0.015952054411172867,\n",
       " 'classification accuracy': 0.016054896637797356,\n",
       " 'the clusters': 0.01617877185344696,\n",
       " 'the statistics': 0.01654084585607052,\n",
       " 'classification tasks': 0.017081569880247116,\n",
       " 'short texts': 0.01726963184773922,\n",
       " 'machines': 0.017528804019093513,\n",
       " 'the predictions': 0.018254883587360382,\n",
       " 'neural models': 0.018406664952635765,\n",
       " 'multiple tasks': 0.018878163769841194,\n",
       " 'the users': 0.019225431606173515,\n",
       " 'the existing methods': 0.01930880919098854,\n",
       " 'encouraging results': 0.0202412661164999,\n",
       " 'the differences': 0.020382164046168327,\n",
       " 'the images': 0.020391082391142845,\n",
       " 'a new task': 0.02044522948563099,\n",
       " 'agents': 0.020519332960247993,\n",
       " 'unlabeled examples': 0.02094496786594391,\n",
       " 'such methods': 0.0214158296585083,\n",
       " 'an unsupervised way': 0.026330415159463882,\n",
       " 'the capabilities': 0.026531966403126717,\n",
       " 'network structures': 0.02796156518161297,\n",
       " 'invariant features': 0.028030019253492355,\n",
       " 'error rates': 0.02835109457373619,\n",
       " 'certain conditions': 0.02853410504758358,\n",
       " 'relationships': 0.02891630493104458,\n",
       " 'prediction models': 0.02902085892856121,\n",
       " 'network models': 0.02907038852572441,\n",
       " 'reinforcement learning algorithms': 0.031134385615587234,\n",
       " 'practitioners': 0.03185337036848068,\n",
       " 'very large datasets': 0.034121762961149216,\n",
       " 'the classification accuracy': 0.0345815047621727,\n",
       " 'variations': 0.034890320152044296,\n",
       " 'uncertainty estimates': 0.03490784764289856,\n",
       " 'the end': 0.03497045859694481,\n",
       " 'changes': 0.035100776702165604,\n",
       " 'useful information': 0.03566272184252739,\n",
       " 'the problems': 0.03573785722255707,\n",
       " 'prior methods': 0.03585919365286827,\n",
       " 'local features': 0.03617832437157631,\n",
       " 'consideration': 0.03622680529952049,\n",
       " 'the inputs': 0.036842044442892075,\n",
       " 'the relevance': 0.03708212450146675,\n",
       " 'a given input': 0.03729197382926941,\n",
       " 'thinking': 0.037403106689453125,\n",
       " 'two distributions': 0.03861064836382866,\n",
       " 'a significant amount': 0.03890417143702507,\n",
       " 'machine learning models': 0.039127059280872345,\n",
       " 'training instances': 0.04001941159367561,\n",
       " 'other types': 0.04006967321038246,\n",
       " 'input features': 0.04087771102786064,\n",
       " 'a separate': 0.041455600410699844,\n",
       " 'the variance': 0.04260785132646561,\n",
       " 'the test data': 0.04296351969242096,\n",
       " 'new tasks': 0.04304908588528633,\n",
       " 'trade offs': 0.04383165389299393,\n",
       " 'interventions': 0.04440087452530861,\n",
       " 'a given task': 0.04478854313492775,\n",
       " 'a wide variety': 0.04492361471056938,\n",
       " 'learnability': 0.04579770937561989,\n",
       " 'prototypes': 0.04704158753156662,\n",
       " 'curiosity': 0.0472458079457283,\n",
       " 'training and testing': 0.048349034041166306,\n",
       " 'the field': 0.04871134087443352,\n",
       " 'this': 0.048952605575323105,\n",
       " 'the attention mechanism': 0.04933128505945206,\n",
       " 'the learning performance': 0.04994325712323189,\n",
       " 'various fields': 0.0500766821205616,\n",
       " 'small datasets': 0.05038832500576973,\n",
       " 'alignments': 0.05128241702914238,\n",
       " 'the entities': 0.05196740850806236,\n",
       " 'various domains': 0.05446754768490791,\n",
       " 'platforms': 0.05453580617904663,\n",
       " 'the domain': 0.05488213524222374,\n",
       " 'references': 0.05497129261493683,\n",
       " 'the next': 0.05574773997068405,\n",
       " 'functions': 0.055858638137578964,\n",
       " 'a portfolio': 0.05631469935178757,\n",
       " 'applications': 0.05718882009387016,\n",
       " 'the experts': 0.0583794079720974,\n",
       " 'some conditions': 0.05866909772157669,\n",
       " 'important information': 0.05873424559831619,\n",
       " 'specific tasks': 0.05926714465022087,\n",
       " 'matrices': 0.0593411959707737,\n",
       " 'glitches': 0.059439148753881454,\n",
       " 'the state space': 0.05962003022432327,\n",
       " 'generalizations': 0.05976181477308273,\n",
       " 'captchas': 0.06021749973297119,\n",
       " 'the number of clusters': 0.061083193868398666,\n",
       " 'training and inference': 0.06189526617527008,\n",
       " 'each stage': 0.06206821650266647,\n",
       " 'limited applicability': 0.0626092180609703,\n",
       " 'model learning': 0.06289657205343246,\n",
       " 'genes': 0.06292998045682907,\n",
       " 'network embedding': 0.06377700716257095,\n",
       " 'no prior knowledge': 0.06468139588832855,\n",
       " 'the knowledge': 0.0656086653470993,\n",
       " 'several variants': 0.06573544442653656,\n",
       " 'a fixed set': 0.06620766222476959,\n",
       " 'the internet': 0.06627831608057022,\n",
       " 'many models': 0.06629841774702072,\n",
       " 'the progression': 0.06762535870075226,\n",
       " 'rewards': 0.06909078359603882,\n",
       " 'users': 0.06997329741716385,\n",
       " 'vaes': 0.06997755169868469,\n",
       " 'objectives': 0.07056886702775955,\n",
       " 'interpretable representations': 0.07107368111610413,\n",
       " 'the difference': 0.07114621996879578,\n",
       " 'the influence': 0.07115168869495392,\n",
       " 'powerful tools': 0.07122370600700378,\n",
       " 'last few years': 0.07156987488269806,\n",
       " 'intermediate results': 0.07251378148794174,\n",
       " 'species': 0.07317937165498734,\n",
       " 'improvement': 0.0733950212597847,\n",
       " 'the motion': 0.07405997812747955,\n",
       " 'the majority': 0.0741719976067543,\n",
       " 'the number of nodes': 0.07422199845314026,\n",
       " 'a given set': 0.07479232549667358,\n",
       " 'bags': 0.07519052177667618,\n",
       " 'special attention': 0.07573478668928146,\n",
       " 'computational efficiency': 0.07651761919260025,\n",
       " 'a particular task': 0.07835932821035385,\n",
       " 'predictive accuracy': 0.07878773659467697,\n",
       " 'many years': 0.07896069437265396,\n",
       " 'human activities': 0.07952696830034256,\n",
       " 'two stages': 0.07977224141359329,\n",
       " 'its robustness': 0.08052373677492142,\n",
       " 'the sources': 0.0807298794388771,\n",
       " 'significant performance gains': 0.08185810595750809,\n",
       " 'suggestions': 0.08259347826242447,\n",
       " 'a limited amount': 0.08288704603910446,\n",
       " 'many fields': 0.08291517943143845,\n",
       " 'common sense knowledge': 0.08296448737382889,\n",
       " 'salient regions': 0.08344241231679916,\n",
       " 'tokens': 0.08353468775749207,\n",
       " 'hidden layers': 0.08461444079875946,\n",
       " 'progress': 0.08501943200826645,\n",
       " 'multiple modalities': 0.08578134328126907,\n",
       " 'the heterogeneity': 0.08629241585731506,\n",
       " 'morphologically rich languages': 0.08688724040985107,\n",
       " 'artists': 0.08698220551013947,\n",
       " 'the impact': 0.08731454610824585,\n",
       " 'ii': 0.08758843690156937,\n",
       " 'a linear model': 0.08763472735881805,\n",
       " 'three': 0.0879664495587349,\n",
       " 'characteristics': 0.08840504288673401,\n",
       " 'better performance': 0.08878479152917862,\n",
       " 'a data point': 0.08940461277961731,\n",
       " 'the training process': 0.08947132527828217,\n",
       " 'renewed interest': 0.08957737684249878,\n",
       " 'their work': 0.08998352289199829,\n",
       " 'higher quality': 0.09017767757177353,\n",
       " 'the presence': 0.09111392498016357,\n",
       " 'controllers': 0.09126226603984833,\n",
       " 'useful representations': 0.09135011583566666,\n",
       " 'structural similarity': 0.09150461852550507,\n",
       " 'kernel functions': 0.0917556881904602,\n",
       " 'i vectors': 0.09187643975019455,\n",
       " 'most methods': 0.09275980293750763,\n",
       " 'the context': 0.09278836101293564,\n",
       " 'every step': 0.09357567131519318,\n",
       " 'labeling': 0.09427079558372498,\n",
       " 'prior works': 0.09519516676664352,\n",
       " 'deeper understanding': 0.09536425024271011,\n",
       " 'the target distribution': 0.09568586945533752,\n",
       " 'sparse models': 0.09775658696889877,\n",
       " 'the medical domain': 0.09815915673971176,\n",
       " 'few years': 0.09832505136728287,\n",
       " 'linear predictors': 0.09882668405771255,\n",
       " 'a test sample': 0.09902815520763397,\n",
       " 'model uncertainty': 0.09912649542093277,\n",
       " 'the functionality': 0.09924759715795517,\n",
       " 'an overview': 0.09967183321714401,\n",
       " 'this phenomenon': 0.09978272765874863,\n",
       " 'predictors': 0.1001470535993576,\n",
       " 'hybrid models': 0.100365050137043,\n",
       " 'difficult tasks': 0.10037488490343094,\n",
       " 'the challenges': 0.10045085847377777,\n",
       " 'prior distributions': 0.10069923847913742,\n",
       " 'manifold learning algorithms': 0.10078346729278564,\n",
       " 'causal directions': 0.10136883705854416,\n",
       " 'a local optimum': 0.10139773041009903,\n",
       " 'the series': 0.10192286223173141,\n",
       " 'widespread use': 0.10310326516628265,\n",
       " 'narratives': 0.10327937453985214,\n",
       " 'two properties': 0.10328231751918793,\n",
       " 'the same domain': 0.10363798588514328,\n",
       " 'mixtures': 0.10365630686283112,\n",
       " 'question generation': 0.10412312299013138,\n",
       " 'historical data': 0.10439816117286682,\n",
       " 'a curriculum': 0.10445385426282883,\n",
       " 'predictive power': 0.10562973469495773,\n",
       " 'biases': 0.1057201400399208,\n",
       " 'a general method': 0.10639968514442444,\n",
       " 'the individual level': 0.10740744322538376,\n",
       " 'the functions': 0.10753198713064194,\n",
       " 'the hidden layers': 0.1077180728316307,\n",
       " 'the representational power': 0.10790054500102997,\n",
       " 'bayesian active learning': 0.10933903604745865,\n",
       " 'a multilayer perceptron': 0.10994279384613037,\n",
       " 'smoothness assumptions': 0.11040964722633362,\n",
       " 'non decomposable': 0.11140050739049911,\n",
       " 'natural language sentences': 0.11153487116098404,\n",
       " 'the disagreement': 0.11380136013031006,\n",
       " 'assessments': 0.11400444805622101,\n",
       " 'the basis': 0.11444302648305893,\n",
       " 'non deterministic': 0.11475858092308044,\n",
       " 'knowledge bases': 0.11476507037878036,\n",
       " 'an important part': 0.1150197684764862,\n",
       " 'recent advancements': 0.11583737283945084,\n",
       " 'a limited number': 0.11853809654712677,\n",
       " 'problem sizes': 0.11864366382360458,\n",
       " 'the objectives': 0.11899525672197342,\n",
       " 'distributional semantics': 0.11970217525959015,\n",
       " 'a novel technique': 0.12000294029712677,\n",
       " 'two questions': 0.12011146545410156,\n",
       " 'posterior sampling': 0.12018280476331711,\n",
       " 'phrases': 0.12039549648761749,\n",
       " 'many areas': 0.12142283469438553,\n",
       " 'random noise': 0.1216389611363411,\n",
       " 'parseval networks': 0.1218319907784462,\n",
       " 'a new': 0.12191740423440933,\n",
       " 'a promising approach': 0.12203403562307358,\n",
       " 'new environments': 0.12380919605493546,\n",
       " 'graph convolutional networks': 0.12389683723449707,\n",
       " 'real world tasks': 0.12394940108060837,\n",
       " 'recent times': 0.12397516518831253,\n",
       " 'human learning': 0.1240549087524414,\n",
       " 'episodes': 0.12406264990568161,\n",
       " 'large': 0.12419698387384415,\n",
       " 'few shot learning': 0.12431415915489197,\n",
       " 'a new model': 0.12434504181146622,\n",
       " 'each topic': 0.12453985214233398,\n",
       " 'the shift': 0.12462609261274338,\n",
       " 'redundant features': 0.12469153106212616,\n",
       " 'purposes': 0.12475176155567169,\n",
       " 'a shared representation': 0.12484373152256012,\n",
       " 'a novel algorithm': 0.12562192976474762,\n",
       " 'different modalities': 0.12600670754909515,\n",
       " 'recent papers': 0.12628719210624695,\n",
       " 'the computational time': 0.1263578087091446,\n",
       " 'the criteria': 0.12650394439697266,\n",
       " 'pairs of examples': 0.12666252255439758,\n",
       " 'a good': 0.12720218300819397,\n",
       " 'image synthesis': 0.12877210974693298,\n",
       " 'local neighborhoods': 0.12901458144187927,\n",
       " 'labellings': 0.12914444506168365,\n",
       " 'little attention': 0.12977269291877747,\n",
       " 'a variety of tasks': 0.12994495034217834,\n",
       " 'a simple': 0.13062483072280884,\n",
       " 'the dimensionality': 0.13114237785339355,\n",
       " 'adversarial perturbations': 0.13257046043872833,\n",
       " 'a classification task': 0.13363038003444672,\n",
       " 'hand': 0.13613435626029968,\n",
       " 'new ones': 0.13700741529464722,\n",
       " 'statistics and machine learning': 0.13729871809482574,\n",
       " 'a study': 0.13757435977458954,\n",
       " 'conflict': 0.13776841759681702,\n",
       " 'good accuracy': 0.13874252140522003,\n",
       " 'small amounts': 0.13950371742248535,\n",
       " 'strong guarantees': 0.13989880681037903,\n",
       " 'deep learning approaches': 0.1406383514404297,\n",
       " 'a change': 0.14066867530345917,\n",
       " 'the computer vision': 0.140854150056839,\n",
       " 'the messages': 0.1415112465620041,\n",
       " 'an input sequence': 0.14249460399150848,\n",
       " 'a simple method': 0.14292597770690918,\n",
       " 'feature vectors': 0.14450086653232574,\n",
       " 'a novel model': 0.14532272517681122,\n",
       " 'each set': 0.14734423160552979,\n",
       " 'the data': 0.14843928813934326,\n",
       " 'a general framework': 0.14880667626857758,\n",
       " 'what': 0.14948225021362305,\n",
       " 'the target language': 0.1495206356048584,\n",
       " 'multivariate time series': 0.149727463722229,\n",
       " 'weeks': 0.15003740787506104,\n",
       " 'latent vectors': 0.150626078248024,\n",
       " 'the separation': 0.1507827788591385,\n",
       " 'images and videos': 0.15095031261444092,\n",
       " 'data instances': 0.15151475369930267,\n",
       " 'repeated': 0.15152302384376526,\n",
       " 'effect': 0.15220806002616882,\n",
       " 'object detection and segmentation': 0.15357600152492523,\n",
       " 'density models': 0.15361765027046204,\n",
       " 'new results': 0.154267355799675,\n",
       " 'successes': 0.15545547008514404,\n",
       " 'natural languages': 0.15585637092590332,\n",
       " 'small training sets': 0.15632687509059906,\n",
       " 'directed graphs': 0.15722249448299408,\n",
       " 'physicians': 0.157342329621315,\n",
       " 'solutions': 0.15808767080307007,\n",
       " 'weak labels': 0.15867431461811066,\n",
       " 'fpgas': 0.15928567945957184,\n",
       " 'autoregressive models': 0.1601656824350357,\n",
       " 'multiple images': 0.16051647067070007,\n",
       " 'a large collection': 0.16094030439853668,\n",
       " 'chatbots': 0.16239190101623535,\n",
       " 'a new formulation': 0.162461519241333,\n",
       " 'an active area of research': 0.16265207529067993,\n",
       " 'eeg signals': 0.16276858747005463,\n",
       " 'a new one': 0.1629026234149933,\n",
       " 'several approaches': 0.16311952471733093,\n",
       " 'every iteration': 0.16321231424808502,\n",
       " 'each distribution': 0.16406941413879395,\n",
       " 'cilia': 0.16609442234039307,\n",
       " 'video prediction': 0.16675592958927155,\n",
       " 'proxtone': 0.16735894978046417,\n",
       " 'mild conditions': 0.16953697800636292,\n",
       " 'times': 0.1696290671825409,\n",
       " 'a practical algorithm': 0.1696694791316986,\n",
       " 'two representations': 0.17003270983695984,\n",
       " 'the supervised learning': 0.17206712067127228,\n",
       " 'expert advice': 0.17286142706871033,\n",
       " 'science and engineering': 0.17319247126579285,\n",
       " 'classification and regression tasks': 0.17451812326908112,\n",
       " 'phenomena': 0.1754220426082611,\n",
       " 'an effective strategy': 0.17613132297992706,\n",
       " 'the decision process': 0.17614178359508514,\n",
       " 'a class of algorithms': 0.17619115114212036,\n",
       " 'machine learning problems': 0.17797599732875824,\n",
       " 'the means': 0.17870168387889862,\n",
       " 'a large amount of interest': 0.1792195588350296,\n",
       " 'low quality': 0.17924150824546814,\n",
       " 'a recurrent neural network rnn': 0.1809585839509964,\n",
       " 'the wasserstein distance': 0.1810137927532196,\n",
       " 'classical cnns': 0.18128693103790283,\n",
       " 'minimal supervision': 0.18285022675991058,\n",
       " 'more flexibility': 0.18355916440486908,\n",
       " 'visual categories': 0.1849338859319687,\n",
       " 'a deep neural network dnn': 0.185215026140213,\n",
       " 'the range': 0.18671974539756775,\n",
       " 'cnn architectures': 0.1870063692331314,\n",
       " 'replacement': 0.18772412836551666,\n",
       " 'neural network learning': 0.18824464082717896,\n",
       " 'the video': 0.18833030760288239,\n",
       " 'two techniques': 0.1884603053331375,\n",
       " 'the variability': 0.18919073045253754,\n",
       " 'sentiment analysis': 0.18921631574630737,\n",
       " 'the guidance': 0.18926334381103516,\n",
       " 'the understanding': 0.19073684513568878,\n",
       " 'melodies': 0.19210490584373474,\n",
       " 'regions': 0.19230526685714722,\n",
       " 'a joint distribution': 0.1925191879272461,\n",
       " 'load': 0.19268810749053955,\n",
       " 'a kernel function': 0.19530150294303894,\n",
       " 'rumours': 0.1956227719783783,\n",
       " 'mathematical models': 0.19671981036663055,\n",
       " 'human computer interaction': 0.19672012329101562,\n",
       " 'students': 0.19753889739513397,\n",
       " 'a matrix': 0.19796369969844818,\n",
       " 'gradient estimation': 0.20079243183135986,\n",
       " 'a new variant': 0.2010035663843155,\n",
       " 'generalisation': 0.20112423598766327,\n",
       " 'a correspondence': 0.20139609277248383,\n",
       " 'a major limitation': 0.20367896556854248,\n",
       " 'the precision': 0.20402772724628448,\n",
       " 'stochasticity': 0.2068914771080017,\n",
       " 'a markov decision process': 0.2079092115163803,\n",
       " 'the cold start problem': 0.2083449512720108,\n",
       " 'trees': 0.2083941251039505,\n",
       " 'the last decade': 0.2085060328245163,\n",
       " 'the web': 0.20977796614170074,\n",
       " 'posteriors': 0.21493665874004364,\n",
       " 'high computational complexity': 0.2159685641527176,\n",
       " 'counts': 0.21647387742996216,\n",
       " 'causal structures': 0.21806150674819946,\n",
       " 'true labels': 0.21863070130348206,\n",
       " 'a lot of attention': 0.22069230675697327,\n",
       " 'future observations': 0.22112053632736206,\n",
       " 'a sparse linear combination': 0.22164230048656464,\n",
       " 'a supervised machine': 0.22207307815551758,\n",
       " 'an online': 0.2221212238073349,\n",
       " 'domain adaptation': 0.22213459014892578,\n",
       " 'visual understanding': 0.22223800420761108,\n",
       " 'each data point': 0.22295427322387695,\n",
       " 'energy functions': 0.223001167178154,\n",
       " 'a neural network architecture': 0.22545212507247925,\n",
       " 'hidden causes': 0.22710929811000824,\n",
       " 'different channels': 0.22766481339931488,\n",
       " 'the biggest challenges': 0.22796830534934998,\n",
       " 'high dimensions': 0.2301240712404251,\n",
       " 'the trajectories': 0.23059101402759552,\n",
       " 'some assumptions': 0.23187334835529327,\n",
       " 'a predictive model': 0.2333691269159317,\n",
       " 'inverse problems': 0.23350399732589722,\n",
       " 'the distance': 0.23351237177848816,\n",
       " 'performance criteria': 0.23394909501075745,\n",
       " 'the recent past': 0.23459598422050476,\n",
       " 'the training dataset': 0.23475070297718048,\n",
       " 'significant progress': 0.23560963571071625,\n",
       " 'recurrent layers': 0.23686419427394867,\n",
       " 'tensors': 0.23691131174564362,\n",
       " 'computational performance': 0.23792356252670288,\n",
       " 'sift features': 0.23813894391059875,\n",
       " 'negative examples': 0.23911528289318085,\n",
       " 'mean field': 0.23943255841732025,\n",
       " 'bayesian methods': 0.23970845341682434,\n",
       " 'the chosen action': 0.24041126668453217,\n",
       " 'natural scenes': 0.24159899353981018,\n",
       " 'flows': 0.24182568490505219,\n",
       " 'words bow': 0.24250665307044983,\n",
       " 'a new concept': 0.24535301327705383,\n",
       " 'competitive performance': 0.24602043628692627,\n",
       " 'random forests': 0.24637101590633392,\n",
       " 'a generic architecture': 0.2467113435268402,\n",
       " 'a novel data': 0.2469480186700821,\n",
       " 'dictionary atoms': 0.2474742829799652,\n",
       " 'linear projections': 0.24976681172847748,\n",
       " 'a gaussian mixture model': 0.25007155537605286,\n",
       " 'sensor data': 0.2503691613674164,\n",
       " 'motion segmentation': 0.25077947974205017,\n",
       " 'this bias': 0.25204306840896606,\n",
       " 'the graphical structure': 0.2531081736087799,\n",
       " 'the scale': 0.25369951128959656,\n",
       " 'supervised domain adaptation': 0.2543167471885681,\n",
       " 'a max margin': 0.25448235869407654,\n",
       " 'two dimensions': 0.25676047801971436,\n",
       " 'growth': 0.2575112581253052,\n",
       " 'convexity': 0.25821545720100403,\n",
       " 'a low dimensional manifold': 0.2598767876625061,\n",
       " 'china': 0.2600458562374115,\n",
       " 'intuitive physics': 0.26065152883529663,\n",
       " 'monolingual': 0.2606515884399414,\n",
       " 'a support vector machine': 0.26121655106544495,\n",
       " 'information fusion': 0.2616210877895355,\n",
       " 'spikes': 0.2660920023918152,\n",
       " 'its variants': 0.2662113904953003,\n",
       " 'vector spaces': 0.2662911117076874,\n",
       " 'the original features': 0.26780009269714355,\n",
       " 'authors': 0.26839929819107056,\n",
       " 'the present work': 0.2693099081516266,\n",
       " 'a signal': 0.27051639556884766,\n",
       " 'fine tuning': 0.2715452313423157,\n",
       " 'expectations': 0.2715893089771271,\n",
       " 'the face': 0.2732011377811432,\n",
       " 'dis': 0.2738295793533325,\n",
       " 'selection algorithms': 0.274793416261673,\n",
       " 'continuous variables': 0.2752305269241333,\n",
       " 'full advantage': 0.27678000926971436,\n",
       " 'popularity': 0.27940815687179565,\n",
       " 'a novel class': 0.28180721402168274,\n",
       " 'computations': 0.2831974923610687,\n",
       " 'training sequences': 0.285060852766037,\n",
       " 'a popular algorithm': 0.28673896193504333,\n",
       " 'a document': 0.2879844605922699,\n",
       " 'our study': 0.2884474992752075,\n",
       " 'the observation': 0.2894759476184845,\n",
       " 'a total': 0.2898193597793579,\n",
       " 'group activities': 0.29095399379730225,\n",
       " 'an introduction': 0.2923334836959839,\n",
       " 'vanilla': 0.2933191657066345,\n",
       " 'some extent': 0.2933969497680664,\n",
       " 'four types': 0.2938529849052429,\n",
       " 'extreme weather events': 0.29468879103660583,\n",
       " 'extrapolate': 0.2948977053165436,\n",
       " 'summarization tasks': 0.2953352630138397,\n",
       " 'an empirical comparison': 0.29713645577430725,\n",
       " 'gradient descent': 0.2987808883190155,\n",
       " 'data handling': 0.30034515261650085,\n",
       " 'a natural extension': 0.3015369176864624,\n",
       " 'set functions': 0.3016374111175537,\n",
       " 'a source': 0.30308786034584045,\n",
       " 'a training dataset': 0.304329514503479,\n",
       " 'subtasks': 0.3044351041316986,\n",
       " 'this study': 0.30502569675445557,\n",
       " 'the ontology': 0.305510550737381,\n",
       " 'histograms': 0.30724090337753296,\n",
       " 'binary classification': 0.3103662133216858,\n",
       " 'a general approach': 0.3106464445590973,\n",
       " 'the one hand': 0.31240522861480713,\n",
       " 'attention weights': 0.3128344714641571,\n",
       " 'annotators': 0.31388577818870544,\n",
       " 'target task': 0.31505805253982544,\n",
       " 'modern deep neural networks': 0.3151511549949646,\n",
       " 'extractors': 0.3156253397464752,\n",
       " 'the performance of the system': 0.31614434719085693,\n",
       " 'synonyms': 0.3166349232196808,\n",
       " 'human attention': 0.3168134093284607,\n",
       " 'handwritten digits': 0.3176731467247009,\n",
       " 'a bayesian approach': 0.31781086325645447,\n",
       " 'important tasks': 0.31809794902801514,\n",
       " 'a set of assumptions': 0.3197530508041382,\n",
       " 'their relationship': 0.3208051025867462,\n",
       " 'face': 0.3211517930030823,\n",
       " 'wda': 0.32465654611587524,\n",
       " 'states and actions': 0.32505857944488525,\n",
       " 'the role': 0.3255175054073334,\n",
       " 'wearables': 0.32561174035072327,\n",
       " 'dynamic environments': 0.3256274163722992,\n",
       " 'a general model': 0.32614395022392273,\n",
       " 'dnn training': 0.32641029357910156,\n",
       " 'increasing complexity': 0.3277988135814667,\n",
       " 'unstructured text': 0.33019179105758667,\n",
       " 'imagenet classification': 0.33115577697753906,\n",
       " 'an important task': 0.3317757546901703,\n",
       " 'electronic medical records': 0.33200371265411377,\n",
       " 'accounts': 0.33239269256591797,\n",
       " 'the roles': 0.33286792039871216,\n",
       " 'structural information': 0.3330906927585602,\n",
       " 'similarity learning': 0.33367353677749634,\n",
       " 'devices': 0.33501797914505005,\n",
       " 'transfer learning': 0.3350616693496704,\n",
       " 'graph embeddings': 0.33517464995384216,\n",
       " 'the search': 0.33769509196281433,\n",
       " 'the aixi model': 0.3380521833896637,\n",
       " 'many of them': 0.33918479084968567,\n",
       " 'deep learning algorithms': 0.3393080234527588,\n",
       " 'the documents': 0.3398491144180298,\n",
       " 'the promise': 0.34111297130584717,\n",
       " 'a bottleneck': 0.3414086401462555,\n",
       " 'dl': 0.34236353635787964,\n",
       " 'just one': 0.34296196699142456,\n",
       " 'a single decision': 0.3430349826812744,\n",
       " 'deep rl': 0.34374818205833435,\n",
       " 'the density': 0.3446046710014343,\n",
       " 'large scale datasets': 0.3449002802371979,\n",
       " 'mab': 0.3454733192920685,\n",
       " 'prevention': 0.3464532792568207,\n",
       " 'complex domains': 0.34720808267593384,\n",
       " 'neuroscience': 0.34751904010772705,\n",
       " 'the extent': 0.3476726710796356,\n",
       " 'human interactions': 0.34865570068359375,\n",
       " 'hierarchy': 0.3488547205924988,\n",
       " 'its accuracy': 0.3493884801864624,\n",
       " 'each individual': 0.3504912257194519,\n",
       " 'the temporal evolution': 0.3507373034954071,\n",
       " 'brain': 0.3514043986797333,\n",
       " 'algorithm selection': 0.35150212049484253,\n",
       " 'favor': 0.3516146242618561,\n",
       " 'labeling tasks': 0.3544856011867523,\n",
       " 'operator valued kernels': 0.35505765676498413,\n",
       " 'web search': 0.35861343145370483,\n",
       " 'the practice': 0.35895249247550964,\n",
       " 'delayed rewards': 0.3618786334991455,\n",
       " 'aims': 0.3629664480686188,\n",
       " 'the production': 0.3655562400817871,\n",
       " 'a metric': 0.36579322814941406,\n",
       " 'current practice': 0.3663935661315918,\n",
       " 'em algorithms': 0.36698850989341736,\n",
       " 'phase retrieval': 0.3676646947860718,\n",
       " 'paraphrase detection': 0.36904725432395935,\n",
       " 'our method': 0.37301817536354065,\n",
       " 'local image patches': 0.3762654662132263,\n",
       " 'the ambient': 0.37636905908584595,\n",
       " 'any learning algorithm': 0.3772556781768799,\n",
       " 'some others': 0.3776889145374298,\n",
       " 'motion capture data': 0.37805113196372986,\n",
       " 'the success of deep learning': 0.37960800528526306,\n",
       " 'social networks': 0.3797338604927063,\n",
       " 'a real time': 0.3797609806060791,\n",
       " 'probability measures': 0.3805989921092987,\n",
       " 'high dimensionality': 0.38076072931289673,\n",
       " 'its extension': 0.3815767765045166,\n",
       " 'a crucial role': 0.3828703463077545,\n",
       " 'the sparsity': 0.3854229152202606,\n",
       " 'the vanishing': 0.38547369837760925,\n",
       " 'the coefficient': 0.38973045349121094,\n",
       " 'the document': 0.39032045006752014,\n",
       " 'other areas': 0.3924453854560852,\n",
       " 'a metaconflict function': 0.3928421437740326,\n",
       " 'rapid development': 0.39287105202674866,\n",
       " 'image signals': 0.3938818573951721,\n",
       " 'the exponential loss': 0.39516615867614746,\n",
       " 'thompson sampling': 0.395221084356308,\n",
       " 'the energy landscape': 0.3976847529411316,\n",
       " 'its': 0.3983789086341858,\n",
       " 'the proofs': 0.39953333139419556,\n",
       " 'object recognition tasks': 0.39993229508399963,\n",
       " 'gradient information': 0.40179651975631714,\n",
       " 'the hyper parameters': 0.4043567478656769,\n",
       " 'option': 0.4052824079990387,\n",
       " 'generating samples': 0.4055035412311554,\n",
       " 'the cumulative regret': 0.4085370898246765,\n",
       " 'someone': 0.4086219370365143,\n",
       " 'klsh': 0.4097422957420349,\n",
       " 'a flexible framework': 0.40991857647895813,\n",
       " 'non linearities': 0.4110787510871887,\n",
       " 'batch algorithms': 0.41134315729141235,\n",
       " 'supervised machine learning': 0.41496455669403076,\n",
       " 'rr': 0.4157989025115967,\n",
       " 'static images': 0.4159277081489563,\n",
       " 'a certain class': 0.4165433347225189,\n",
       " 'riemannian manifolds': 0.41654789447784424,\n",
       " 'four': 0.417847603559494,\n",
       " 'strings': 0.4183637499809265,\n",
       " 'a critical bottleneck': 0.4188258647918701,\n",
       " 'the extraction': 0.42028021812438965,\n",
       " 'shallow networks': 0.421797513961792,\n",
       " 'density estimation tasks': 0.42358526587486267,\n",
       " 'backpropagation': 0.4241616129875183,\n",
       " 'precision recall': 0.4254700243473053,\n",
       " 'pairwise interactions': 0.4282855987548828,\n",
       " 'gradient optimization': 0.4315732419490814,\n",
       " 'its weights': 0.4321504831314087,\n",
       " 'lstd': 0.4324802756309509,\n",
       " 'a classifier': 0.43449193239212036,\n",
       " 'news recommendation': 0.43675076961517334,\n",
       " 'cost sensitive classification': 0.4370780289173126,\n",
       " 'organism': 0.43728846311569214,\n",
       " 'content recommendation': 0.43911218643188477,\n",
       " 'strongly convex problems': 0.44138413667678833,\n",
       " 'classifications': 0.44169601798057556,\n",
       " 'flaws': 0.44363516569137573,\n",
       " 'the decision boundary': 0.4444647431373596,\n",
       " 'bangla alphabet': 0.4464447796344757,\n",
       " 'the lasso': 0.4474402368068695,\n",
       " 'language learning': 0.4479283094406128,\n",
       " 'this score': 0.44867879152297974,\n",
       " 'the expectation': 0.4491604268550873,\n",
       " 'scene recognition': 0.449502557516098,\n",
       " 'booking': 0.45002296566963196,\n",
       " 'clinical practice': 0.4515933692455292,\n",
       " 'factor matrices': 0.4524191915988922,\n",
       " 'a compact representation': 0.45467409491539,\n",
       " 'extrinsic': 0.4554509222507477,\n",
       " 'their labels': 0.45551609992980957,\n",
       " 'the nature': 0.45767271518707275,\n",
       " 'performance close': 0.45850440859794617,\n",
       " 'soft constraints': 0.4585706293582916,\n",
       " 'indefinite': 0.4592844545841217,\n",
       " 'ucf 101': 0.45934057235717773,\n",
       " 'a critical role': 0.46020135283470154,\n",
       " 'supervised semi': 0.46056610345840454,\n",
       " 'the available data': 0.46338096261024475,\n",
       " 'distinct': 0.4637288451194763,\n",
       " 'a regression model': 0.465207576751709,\n",
       " 'matrix completion': 0.4652637243270874,\n",
       " 'a movie': 0.4655558466911316,\n",
       " 'each source': 0.4681798219680786,\n",
       " 'the point': 0.4698878526687622,\n",
       " 'system identification': 0.47180575132369995,\n",
       " 'vast amounts': 0.47292983531951904,\n",
       " 'fine grained classification': 0.4734187424182892,\n",
       " 'dialogue systems': 0.47424817085266113,\n",
       " 'the player s': 0.4757280647754669,\n",
       " 'the eigenvectors': 0.47665688395500183,\n",
       " 'computational intelligence': 0.47690802812576294,\n",
       " 'feature extraction methods': 0.4783848822116852,\n",
       " 'sample': 0.47862568497657776,\n",
       " 'batch normalization': 0.4793893098831177,\n",
       " 'direction method': 0.479907363653183,\n",
       " 'a large set': 0.48049965500831604,\n",
       " 'the shelf': 0.4805324971675873,\n",
       " 'use': 0.4816059172153473,\n",
       " 'highway networks': 0.48166629672050476,\n",
       " 'lfms': 0.48284754157066345,\n",
       " 'much progress': 0.4839739501476288,\n",
       " 'classification and regression problems': 0.48425763845443726,\n",
       " 'an infinite number': 0.4848042130470276,\n",
       " 'successful approaches': 0.4863739311695099,\n",
       " 'length mdl principle': 0.4865986108779907,\n",
       " 'parameter tuning': 0.48932135105133057,\n",
       " 'the bayesian framework': 0.49099084734916687,\n",
       " 'neural activity': 0.4910338521003723,\n",
       " 'the idea of': 0.49132782220840454,\n",
       " 'the expected error': 0.4913323223590851,\n",
       " 'the relative merits': 0.4918024241924286,\n",
       " 'a challenging problem': 0.492319256067276,\n",
       " 'gestures': 0.49297797679901123,\n",
       " 'weight parameters': 0.49327608942985535,\n",
       " 'incorrect labels': 0.49372610449790955,\n",
       " 'spectra': 0.4949840307235718,\n",
       " 'neural network approaches': 0.49505624175071716,\n",
       " 'brl': 0.49532628059387207,\n",
       " 'our ability': 0.49593186378479004,\n",
       " 'mathematics': 0.4968867003917694,\n",
       " 'each patient': 0.49736812710762024,\n",
       " 'multilayer neural networks': 0.4988992512226105}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "premium-hands",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(extracted, open(f'data/pem_extracted_vs_{VECTOR_SIZE}_ws_{WINDOW_SIZE}_nl_{NUM_LAYERS}_mf_{MIN_FREQUENCY}_e_{EPOCHS}.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pem",
   "language": "python",
   "name": "pem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
