{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Classifier implementation (model architecture, training, testing, etc.) derived from\n",
    "#     https://towardsdatascience.com/pytorch-tabular-binary-classification-a0368da5bb89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PHRASE_LEN = 6\n",
    "VECTOR_SIZE = 200\n",
    "WINDOW_SIZE = 5\n",
    "NUM_LAYERS = 5\n",
    "\n",
    "MIN_FREQUENCY = 5\n",
    "\n",
    "SHOULD_EXTRACT_NOUN_PHRASES = False\n",
    "SHOULD_GENERATE_UNDERSCORED_CORPUS = False\n",
    "SHOULD_TRAIN_WORD2VEC_MODEL = False\n",
    "\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 arxiv abstracts.\n"
     ]
    }
   ],
   "source": [
    "with open('data/arxiv_abstracts_10000.txt', 'r') as f:\n",
    "    arxiv_abstracts = f.read().split('\\n')[:-1]\n",
    "    arxiv_abstracts_raw = '\\n'.join(arxiv_abstracts)\n",
    "    f.close()\n",
    "print(f'Loaded {len(arxiv_abstracts)} arxiv abstracts.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1900 negative samples.\n"
     ]
    }
   ],
   "source": [
    "negative_samples = pickle.load(open('data/negative_samples.pkl', 'rb'))\n",
    "print(f'Loaded {len(negative_samples)} negative samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1900 positive samples.\n"
     ]
    }
   ],
   "source": [
    "positive_samples = pickle.load(open('data/positive_samples.pkl', 'rb'))\n",
    "print(f'Loaded {len(positive_samples)} positive samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_phrase(tree_str, label):\n",
    "    phrases = []\n",
    "    trees = Tree.fromstring(tree_str)\n",
    "    for tree in trees:\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == label:\n",
    "                t = subtree\n",
    "                t = ' '.join(t.leaves())\n",
    "                phrases.append(t)\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOULD_EXTRACT_NOUN_PHRASES:\n",
    "    nlp = StanfordCoreNLP('data/stanford-corenlp-4.1.0')\n",
    "    noun_phrases = []\n",
    "    for i, abstract in enumerate(arxiv_abstracts):\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f'Extracting noun phrases from abstract {i + 1} of {len(arxiv_abstracts)}')\n",
    "            pickle.dump(noun_phrases, open('data/noun_phrases.pkl', 'wb'))\n",
    "        try:\n",
    "            tree_str = nlp.parse(abstract)\n",
    "            noun_phrases.extend(extract_phrase(tree_str, 'NP'))\n",
    "        except Exception:\n",
    "            pass\n",
    "    noun_phrases = [np for np in list(set(noun_phrases)) if len(np.split()) <= MAX_PHRASE_LEN]\n",
    "    pickle.dump(noun_phrases, open('data/noun_phrases.pkl', 'wb'))\n",
    "noun_phrases = pickle.load(open('data/noun_phrases.pkl', 'rb'))\n",
    "noun_phrases = [np for np in list(set(noun_phrases)) if len(np.split()) <= MAX_PHRASE_LEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_phrase_in_corpus(corpus, phrase):\n",
    "    s_idx = corpus.find(phrase)\n",
    "    e_idx = s_idx + len(phrase)\n",
    "    if s_idx != -1 and \\\n",
    "       (s_idx == 0 or corpus[s_idx - 1] in (string.punctuation + ' ')) and \\\n",
    "       (e_idx == len(corpus) or corpus[e_idx] in (string.punctuation + ' ')):\n",
    "        return (s_idx, e_idx)\n",
    "    return (-1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOULD_GENERATE_UNDERSCORED_CORPUS:\n",
    "    corpus = arxiv_abstracts_raw[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOULD_GENERATE_UNDERSCORED_CORPUS:\n",
    "    for i, positive_sample in enumerate(positive_samples):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Replacing positive_sample {i + 1} of {len(positive_samples)}')\n",
    "        found_indices = set()\n",
    "        while find_phrase_in_corpus(corpus, positive_sample) != (-1, -1) and find_phrase_in_corpus(corpus, positive_sample)[0] not in found_indices:\n",
    "            s_idx, e_idx = find_phrase_in_corpus(corpus, positive_sample)\n",
    "            found_indices.add(s_idx)\n",
    "            corpus = corpus[:s_idx] + positive_sample.replace(' ', '_') + corpus[e_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOULD_GENERATE_UNDERSCORED_CORPUS:\n",
    "    for i, negative_sample in enumerate(negative_samples):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Replacing negative_sample {i + 1} of {len(negative_samples)}')\n",
    "        found_indices = set()\n",
    "        while find_phrase_in_corpus(corpus, negative_sample) != (-1, -1) and find_phrase_in_corpus(corpus, negative_sample)[0] not in found_indices:\n",
    "            s_idx, e_idx = find_phrase_in_corpus(corpus, negative_sample)\n",
    "            found_indices.add(s_idx)\n",
    "            corpus = corpus[:s_idx] + negative_sample.replace(' ', '_') + corpus[e_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOULD_GENERATE_UNDERSCORED_CORPUS:\n",
    "    for i, noun_phrase in enumerate(noun_phrases):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Replacing noun_phrase {i + 1} of {len(noun_phrases)}')\n",
    "        found_indices = set()\n",
    "        while find_phrase_in_corpus(corpus, noun_phrase) != (-1, -1) and find_phrase_in_corpus(corpus, noun_phrase)[0] not in found_indices:\n",
    "            s_idx, e_idx = find_phrase_in_corpus(corpus, noun_phrase)\n",
    "            found_indices.add(s_idx)\n",
    "            corpus = corpus[:s_idx] + noun_phrase.replace(' ', '_') + corpus[e_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOULD_GENERATE_UNDERSCORED_CORPUS:\n",
    "    pickle.dump(corpus, open('data/underscored_corpus.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "underscored_corpus = pickle.load(open('data/underscored_corpus.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOULD_TRAIN_WORD2VEC_MODEL:\n",
    "    underscored_corpus_data = []\n",
    "    for i in sent_tokenize(underscored_corpus):\n",
    "        temp = []\n",
    "        for j in word_tokenize(i):\n",
    "            temp.append(j.lower())\n",
    "        underscored_corpus_data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOULD_TRAIN_WORD2VEC_MODEL:\n",
    "    word2vec_model = Word2Vec(underscored_corpus_data, min_count=1, window=WINDOW_SIZE, size=VECTOR_SIZE)\n",
    "    word2vec_model.save(f'data/word2vec_model_vs_{VECTOR_SIZE}_ws_{WINDOW_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec.load(f'data/word2vec_model_vs_{VECTOR_SIZE}_ws_{WINDOW_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token for token in list(word2vec_model.wv.vocab.keys())]\n",
    "embeddings = {token: word2vec_model.wv[token] for token in tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_samples = [ps for ps in positive_samples if ps.replace(' ', '_') in embeddings and word2vec_model.wv.vocab[ps.replace(' ', '_')].count >= MIN_FREQUENCY]\n",
    "negative_samples = [ns for ns in negative_samples if ns.replace(' ', '_') in embeddings and word2vec_model.wv.vocab[ns.replace(' ', '_')].count >= MIN_FREQUENCY]\n",
    "noun_phrases = [np for np in noun_phrases if np.replace(' ', '_') in embeddings and word2vec_model.wv.vocab[np.replace(' ', '_')].count >= MIN_FREQUENCY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for phrase in positive_samples:\n",
    "    X.append(embeddings[phrase.replace(' ', '_')])\n",
    "    y.append(1)\n",
    "for phrase in negative_samples:\n",
    "    X.append(embeddings[phrase.replace(' ', '_')])\n",
    "    y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = list(zip(X, y))\n",
    "random.shuffle(c)\n",
    "X, y = zip(*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "\n",
    "        self.layer_1 = nn.Linear(VECTOR_SIZE, 128)\n",
    "        \n",
    "        self.layers = []\n",
    "        for _ in range(NUM_LAYERS - 1):\n",
    "            self.layers.append(nn.Linear(128, 128))\n",
    "        \n",
    "        self.layer_out = nn.Linear(128, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = self.relu(layer(x))\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryClassifier(\n",
       "  (layer_1): Linear(in_features=200, out_features=128, bias=True)\n",
       "  (layer_out): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BinaryClassifier()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "train_data = TrainDataset(torch.FloatTensor(np.array(X_train, dtype=np.float64)), \n",
    "                          torch.FloatTensor(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "\n",
    "test_data = TestDataset(torch.FloatTensor(np.array(X_test, dtype=np.float64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010: | Loss: 0.58202 | Acc: 74.053\n",
      "Epoch 020: | Loss: 0.46532 | Acc: 79.474\n",
      "Epoch 030: | Loss: 0.42481 | Acc: 80.947\n",
      "Epoch 040: | Loss: 0.39687 | Acc: 82.368\n",
      "Epoch 050: | Loss: 0.37750 | Acc: 83.789\n",
      "Epoch 060: | Loss: 0.36029 | Acc: 84.211\n",
      "Epoch 070: | Loss: 0.34644 | Acc: 85.105\n",
      "Epoch 080: | Loss: 0.33124 | Acc: 85.474\n",
      "Epoch 090: | Loss: 0.32281 | Acc: 86.211\n",
      "Epoch 100: | Loss: 0.31367 | Acc: 86.684\n",
      "Epoch 110: | Loss: 0.30278 | Acc: 87.263\n",
      "Epoch 120: | Loss: 0.29129 | Acc: 87.632\n",
      "Epoch 130: | Loss: 0.28347 | Acc: 88.053\n",
      "Epoch 140: | Loss: 0.28049 | Acc: 88.263\n",
      "Epoch 150: | Loss: 0.26624 | Acc: 88.842\n",
      "Epoch 160: | Loss: 0.26278 | Acc: 89.632\n",
      "Epoch 170: | Loss: 0.25539 | Acc: 89.842\n",
      "Epoch 180: | Loss: 0.24704 | Acc: 90.000\n",
      "Epoch 190: | Loss: 0.24246 | Acc: 89.895\n",
      "Epoch 200: | Loss: 0.24054 | Acc: 90.632\n"
     ]
    }
   ],
   "source": [
    "epoch_losses = []\n",
    "for e in range(1, EPOCHS + 1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "\n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    epoch_losses.append(epoch_loss / len(train_loader)) \n",
    "\n",
    "    if e % 10 == 0:\n",
    "        print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14c14edf0>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 62  38]\n",
      " [ 14 190]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.62      0.70       100\n",
      "           1       0.83      0.93      0.88       204\n",
      "\n",
      "    accuracy                           0.83       304\n",
      "   macro avg       0.82      0.78      0.79       304\n",
      "weighted avg       0.83      0.83      0.82       304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred_list))\n",
    "print(classification_report(y_test, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = random.sample(noun_phrases, 1000)\n",
    "X_samples = []\n",
    "for sample in test_samples:\n",
    "    X_samples.append(embeddings[sample.replace(' ', '_')])\n",
    "sample_data = TestDataset(torch.FloatTensor(np.array(X_samples, dtype=np.float64)))\n",
    "sample_loader = DataLoader(dataset=sample_data, batch_size=1)\n",
    "\n",
    "extracted = {}\n",
    "not_extracted = {}\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, X_batch in enumerate(sample_loader):\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        if y_pred_tag.cpu().numpy()[0][0] == 1:\n",
    "            extracted[test_samples[i]] = y_test_pred.item()\n",
    "        else:\n",
    "            not_extracted[test_samples[i]] = y_test_pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted = {k: v for k, v in sorted(extracted.items(), key=lambda x: x[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sparse': 0.9999926090240479,\n",
       " 'tree': 0.9999743700027466,\n",
       " 'respect': 0.999961256980896,\n",
       " 'instance': 0.9999594688415527,\n",
       " 'expert': 0.999951958656311,\n",
       " 'question': 0.9998536109924316,\n",
       " 'scales': 0.9998339414596558,\n",
       " 'rule': 0.9996981620788574,\n",
       " 'risk': 0.999600350856781,\n",
       " 'regret': 0.9995524287223816,\n",
       " 'solution': 0.9995087385177612,\n",
       " 'image': 0.9993941783905029,\n",
       " 'program': 0.9992573857307434,\n",
       " 'category': 0.9992153644561768,\n",
       " 'sequence': 0.9992087483406067,\n",
       " 'train': 0.9991551637649536,\n",
       " 'learning': 0.99897301197052,\n",
       " 'example': 0.9989666938781738,\n",
       " 'region': 0.9989173412322998,\n",
       " 'subject': 0.9988896250724792,\n",
       " 'background': 0.9988873600959778,\n",
       " 'representation': 0.9984329342842102,\n",
       " 'mapping': 0.998414158821106,\n",
       " 'inference': 0.9983004927635193,\n",
       " 'kernel': 0.998121440410614,\n",
       " 'population': 0.9980890154838562,\n",
       " 'platform': 0.9980329871177673,\n",
       " 'perception': 0.9977343082427979,\n",
       " 'hardware': 0.9971739053726196,\n",
       " 'ontology': 0.9970349073410034,\n",
       " 'framework': 0.9967393279075623,\n",
       " 'chain': 0.9967343807220459,\n",
       " 'traffic': 0.9966122508049011,\n",
       " 'deals': 0.9966117739677429,\n",
       " 'linear': 0.9965920448303223,\n",
       " 'face': 0.9964543581008911,\n",
       " 'collection': 0.9963667392730713,\n",
       " 'interpretation': 0.9962010979652405,\n",
       " 'english': 0.9961150884628296,\n",
       " 'extension': 0.9960616230964661,\n",
       " 'neighbors': 0.9960158467292786,\n",
       " 'f': 0.9960120916366577,\n",
       " 'digital': 0.9958765506744385,\n",
       " 'advice': 0.9958468079566956,\n",
       " 't': 0.9956669807434082,\n",
       " 'problems': 0.9952359795570374,\n",
       " 'cost': 0.9951619505882263,\n",
       " 'viewpoint': 0.9945273995399475,\n",
       " 'semantics': 0.9942699670791626,\n",
       " 'user': 0.9942578077316284,\n",
       " 'video': 0.9940071105957031,\n",
       " 'specification': 0.993809163570404,\n",
       " 'output': 0.9937699437141418,\n",
       " 'means': 0.9936112761497498,\n",
       " 'distributed': 0.9930223822593689,\n",
       " 'optical flow': 0.9927884936332703,\n",
       " 'robustness': 0.9923555850982666,\n",
       " 'manner': 0.9921361207962036,\n",
       " '100': 0.9914767146110535,\n",
       " 'sampling': 0.9913971424102783,\n",
       " 'split': 0.989458441734314,\n",
       " 'part': 0.9892895817756653,\n",
       " 'opportunities': 0.988964319229126,\n",
       " 'purpose': 0.9885154366493225,\n",
       " 'forests': 0.9881145358085632,\n",
       " 'parameter': 0.9878917932510376,\n",
       " '32': 0.9875510334968567,\n",
       " 'log': 0.9870607852935791,\n",
       " 'activation function': 0.9870041012763977,\n",
       " 'resource': 0.9864392280578613,\n",
       " 'word': 0.9862848520278931,\n",
       " 'go': 0.986221194267273,\n",
       " 'cameras': 0.9861307740211487,\n",
       " 'epsilon': 0.9851518273353577,\n",
       " 'fashion': 0.9847080707550049,\n",
       " 'approximate': 0.9839205145835876,\n",
       " 'the likelihood': 0.9837660789489746,\n",
       " 'n': 0.9836589097976685,\n",
       " 'deployment': 0.9835712909698486,\n",
       " 'media': 0.9831862449645996,\n",
       " 'sarsa': 0.9829602241516113,\n",
       " 'appearance': 0.9826552867889404,\n",
       " 'semantic segmentation': 0.9824877381324768,\n",
       " 'de': 0.9824144244194031,\n",
       " 'language modeling': 0.9824093580245972,\n",
       " 'service': 0.9821619391441345,\n",
       " 'usage': 0.9817774295806885,\n",
       " 'participants': 0.9809804558753967,\n",
       " 'patch': 0.9808995723724365,\n",
       " 'object recognition': 0.9808878898620605,\n",
       " 'notion': 0.9806804060935974,\n",
       " 'iii': 0.9806591272354126,\n",
       " 'non stationary': 0.9806375503540039,\n",
       " 'imaging': 0.9803433418273926,\n",
       " 'object classification': 0.9794669151306152,\n",
       " 'localization': 0.9790951013565063,\n",
       " 'manipulation': 0.9785727262496948,\n",
       " 'annotation': 0.9778025150299072,\n",
       " 'segments': 0.9774006009101868,\n",
       " 'indicators': 0.9773813486099243,\n",
       " 'u': 0.9770420789718628,\n",
       " 'game': 0.9768314957618713,\n",
       " 'language': 0.9767996668815613,\n",
       " 'matlab': 0.9767698645591736,\n",
       " '13': 0.976718008518219,\n",
       " 'the basis': 0.976655900478363,\n",
       " 'tensorflow': 0.9765871167182922,\n",
       " 'sketches': 0.976161539554596,\n",
       " 'importance': 0.9760293364524841,\n",
       " 'cycle': 0.975231945514679,\n",
       " 'a combination': 0.9748275876045227,\n",
       " 'discrete variables': 0.9746720790863037,\n",
       " 'research': 0.9738376140594482,\n",
       " 'genetic algorithms': 0.9736915826797485,\n",
       " 'manifolds': 0.9736263751983643,\n",
       " 'deep learning models': 0.9730823040008545,\n",
       " 'large scale problems': 0.9729147553443909,\n",
       " 'toolbox': 0.9725780487060547,\n",
       " 'convolution': 0.9724094867706299,\n",
       " 'variations': 0.9721383452415466,\n",
       " 'cognition': 0.9715742468833923,\n",
       " 'omega': 0.9714257121086121,\n",
       " 'the target distribution': 0.9707800149917603,\n",
       " 'incremental': 0.97074294090271,\n",
       " 'metadata': 0.9700379967689514,\n",
       " 'fourier': 0.9695075750350952,\n",
       " 'natural language': 0.9694939851760864,\n",
       " 'indexing': 0.9689297080039978,\n",
       " 'interests': 0.9682362079620361,\n",
       " 'vanilla': 0.9680246114730835,\n",
       " 'a unified framework': 0.9676941633224487,\n",
       " 'peer': 0.9673728942871094,\n",
       " 'differential equations': 0.9672707319259644,\n",
       " 'a result': 0.9668909907341003,\n",
       " 'internet': 0.9667249917984009,\n",
       " 'crowds': 0.9655170440673828,\n",
       " 'medical imaging': 0.9652662873268127,\n",
       " 'optimization problems': 0.965061604976654,\n",
       " 'counts': 0.9645910859107971,\n",
       " 'edge detection': 0.9645877480506897,\n",
       " 'extractors': 0.964158296585083,\n",
       " 'developers': 0.9639723896980286,\n",
       " 'strengths': 0.9631611704826355,\n",
       " 'sentiments': 0.9624699354171753,\n",
       " 'hidden units': 0.9623388051986694,\n",
       " 'pose': 0.9622131586074829,\n",
       " 'delayed': 0.9617785811424255,\n",
       " 'discrimination': 0.9612130522727966,\n",
       " 'privacy': 0.9612087607383728,\n",
       " 'medical diagnosis': 0.961076557636261,\n",
       " 'past observations': 0.960992693901062,\n",
       " 'model uncertainty': 0.9600740671157837,\n",
       " 'gibbs': 0.9600338935852051,\n",
       " 'norms': 0.9599975943565369,\n",
       " 'terrain': 0.9599511623382568,\n",
       " 'dissimilarity': 0.959761381149292,\n",
       " 'discharge': 0.9591798186302185,\n",
       " 'the scene': 0.958903968334198,\n",
       " 'pathology': 0.9583136439323425,\n",
       " 'neuroscience': 0.957832396030426,\n",
       " 'pipelines': 0.957662045955658,\n",
       " 'image generation': 0.9572941660881042,\n",
       " 'isolation': 0.9569635987281799,\n",
       " 'atari': 0.9569399356842041,\n",
       " 'conversations': 0.9569293260574341,\n",
       " 'chemical': 0.9566019773483276,\n",
       " 'places': 0.9563977718353271,\n",
       " 'questions': 0.9562695622444153,\n",
       " 'reality': 0.9555336833000183,\n",
       " 'qa': 0.9549550414085388,\n",
       " 'practical applications': 0.9546442627906799,\n",
       " 'delays': 0.9544879198074341,\n",
       " 'sentiment analysis': 0.9542196393013,\n",
       " 'statistical learning': 0.954091489315033,\n",
       " 'multiple layers': 0.9527428150177002,\n",
       " 'lstd': 0.9524020552635193,\n",
       " 'lstm': 0.9519362449645996,\n",
       " 'p x': 0.951890230178833,\n",
       " 'surveillance': 0.9514632225036621,\n",
       " 'segment': 0.9513916969299316,\n",
       " 'monotonicity': 0.9511937499046326,\n",
       " 'study': 0.9506702423095703,\n",
       " 'registration': 0.9505857229232788,\n",
       " 'dynamics': 0.9504656195640564,\n",
       " 'genomics': 0.9498782753944397,\n",
       " 'concept': 0.9489877820014954,\n",
       " 'mathcal': 0.948091447353363,\n",
       " 'noisy observations': 0.9478216171264648,\n",
       " 'different languages': 0.9467605352401733,\n",
       " 'weather': 0.9467304348945618,\n",
       " 'translations': 0.9464974403381348,\n",
       " 'gestures': 0.9464724659919739,\n",
       " 'sign': 0.9463453888893127,\n",
       " 'desired properties': 0.9457294344902039,\n",
       " 'natural language generation': 0.9455687999725342,\n",
       " 'parallel': 0.9454267621040344,\n",
       " 'a linear combination': 0.945336639881134,\n",
       " 'programming': 0.9451800584793091,\n",
       " 'human actions': 0.9442412257194519,\n",
       " 'model accuracy': 0.9438008666038513,\n",
       " 'drug': 0.9432787299156189,\n",
       " 'texture': 0.9432721734046936,\n",
       " 'the construction': 0.9421482086181641,\n",
       " 'geometry': 0.9413648843765259,\n",
       " 'hyperplane': 0.9409072995185852,\n",
       " 'populations': 0.9407563805580139,\n",
       " 'smartphones': 0.9402220845222473,\n",
       " 'simulation': 0.9395391941070557,\n",
       " 'social media': 0.9394475221633911,\n",
       " 'mobile devices': 0.9387001991271973,\n",
       " 'construction': 0.9375143051147461,\n",
       " 'a vehicle': 0.9368410706520081,\n",
       " 'infty': 0.9358848333358765,\n",
       " 'sphere': 0.9352001547813416,\n",
       " 'vector representations': 0.9349892139434814,\n",
       " 'keyword': 0.9341058731079102,\n",
       " 'fire': 0.9338449835777283,\n",
       " 'computer vision tasks': 0.9337896704673767,\n",
       " 'similar performance': 0.9331815838813782,\n",
       " 'time and space': 0.9330626130104065,\n",
       " 'statistics': 0.9329416155815125,\n",
       " 'records': 0.9329229593276978,\n",
       " 'motivation': 0.932762861251831,\n",
       " 'a hierarchy': 0.9325847029685974,\n",
       " 'a wide spectrum': 0.9325804710388184,\n",
       " 'material': 0.9323194026947021,\n",
       " 'reward': 0.931857705116272,\n",
       " 'lsh': 0.9312987327575684,\n",
       " 'fairness': 0.9311829209327698,\n",
       " 'a means': 0.9310615062713623,\n",
       " 'its variants': 0.9308066368103027,\n",
       " 'advertisers': 0.9305340647697449,\n",
       " 'experience replay': 0.9299570918083191,\n",
       " 'management': 0.928593099117279,\n",
       " 'rpca': 0.9282532334327698,\n",
       " 'compatibility': 0.9279654622077942,\n",
       " 'stationary points': 0.9277797937393188,\n",
       " 'semantic parsing': 0.9276149272918701,\n",
       " 'loopy': 0.927181601524353,\n",
       " 'pegasos': 0.9268426299095154,\n",
       " 'sentence classification': 0.9265207052230835,\n",
       " 'their environment': 0.925967812538147,\n",
       " 'discretization': 0.9258129596710205,\n",
       " 'validation': 0.9247552156448364,\n",
       " 'a scene': 0.9244144558906555,\n",
       " 'the expectation': 0.9238407611846924,\n",
       " 'the diagnosis': 0.9233786463737488,\n",
       " 'classification problems': 0.9232951402664185,\n",
       " 'correlation': 0.9229459762573242,\n",
       " 'naive bayes': 0.9223676323890686,\n",
       " 'an attention mechanism': 0.9220938682556152,\n",
       " 'human': 0.9212558269500732,\n",
       " 'clothes': 0.9209403395652771,\n",
       " 'walk': 0.9208858609199524,\n",
       " 'threats': 0.9201306104660034,\n",
       " 'a real time': 0.9200360178947449,\n",
       " 'input sequences': 0.9197479486465454,\n",
       " 'word representations': 0.9196816086769104,\n",
       " 'observed variables': 0.9196810126304626,\n",
       " 'map som': 0.9196032285690308,\n",
       " 'exploration and exploitation': 0.9195001125335693,\n",
       " 'the ontology': 0.9194847941398621,\n",
       " 'an important tool': 0.9187004566192627,\n",
       " 'multi armed bandit problem': 0.917641818523407,\n",
       " 'a variety of tasks': 0.9172409772872925,\n",
       " 'damage': 0.9171979427337646,\n",
       " 'a subroutine': 0.9170336127281189,\n",
       " 'model parameters': 0.9168561100959778,\n",
       " 'caffe': 0.9164680242538452,\n",
       " 'option': 0.9156809449195862,\n",
       " 'attention mechanisms': 0.9155101180076599,\n",
       " 'knowledge graphs': 0.9148980975151062,\n",
       " 'expectation maximization': 0.9148639440536499,\n",
       " 'stochastic gradient descent': 0.9140113592147827,\n",
       " 'the study': 0.9130299687385559,\n",
       " 'interpretability': 0.9126501083374023,\n",
       " 'positions': 0.9121352434158325,\n",
       " 'customers': 0.9121350646018982,\n",
       " 'each component': 0.9116436839103699,\n",
       " 'lifelong learning': 0.911314070224762,\n",
       " 'deep neural network architectures': 0.9106659889221191,\n",
       " 'cnn': 0.9097328186035156,\n",
       " 'text': 0.9093344211578369,\n",
       " 'an abundance': 0.9080929756164551,\n",
       " 'code': 0.9079457521438599,\n",
       " 'meek': 0.906976044178009,\n",
       " 'advance': 0.9061288833618164,\n",
       " 'neural architectures': 0.9057363867759705,\n",
       " 'language learning': 0.9057351350784302,\n",
       " 'an important task': 0.9056087136268616,\n",
       " 'smile': 0.9054431915283203,\n",
       " 'much attention': 0.9048208594322205,\n",
       " 'cho': 0.9038949608802795,\n",
       " 'concentration': 0.9038028120994568,\n",
       " 'network datasets': 0.9037477374076843,\n",
       " 'exploding gradients': 0.9028107523918152,\n",
       " '80': 0.902353823184967,\n",
       " 'negative samples': 0.9023266434669495,\n",
       " 'science': 0.9020790457725525,\n",
       " 'penalty': 0.9017553329467773,\n",
       " 'chinese': 0.9013828635215759,\n",
       " 'sequence learning': 0.9012994766235352,\n",
       " 'plda': 0.9010916352272034,\n",
       " 'nonconvex': 0.9009549617767334,\n",
       " 'genetic algorithm': 0.9009192585945129,\n",
       " 'side information': 0.9006235599517822,\n",
       " 'notions': 0.9004758596420288,\n",
       " 'judgments': 0.9002019166946411,\n",
       " 'parallel corpora': 0.9000610709190369,\n",
       " 'tumors': 0.8999983072280884,\n",
       " 'fmri': 0.899803876876831,\n",
       " 'series': 0.8995030522346497,\n",
       " 'the realm': 0.8994354605674744,\n",
       " 'the flow': 0.8990448117256165,\n",
       " 'generalization abilities': 0.8988780975341797,\n",
       " 'falls': 0.8985578417778015,\n",
       " 'learning machines': 0.8981959819793701,\n",
       " 'a broad class': 0.8981338739395142,\n",
       " 'logs': 0.8980198502540588,\n",
       " 'object localization': 0.8968076705932617,\n",
       " 'universal perturbations': 0.8966450095176697,\n",
       " 'sequence labeling tasks': 0.8961182832717896,\n",
       " 'learning rates': 0.8945392370223999,\n",
       " 'language modelling': 0.8940288424491882,\n",
       " 'the exploration': 0.8923553824424744,\n",
       " 'advisors': 0.8913154006004333,\n",
       " 'likelihood': 0.8912215828895569,\n",
       " 'a subset': 0.8910449147224426,\n",
       " 'bayesian optimization': 0.8907096982002258,\n",
       " 'insights': 0.8901306390762329,\n",
       " 'correntropy': 0.8898951411247253,\n",
       " 'the face': 0.8895959854125977,\n",
       " 'autonomous vehicles': 0.8892461061477661,\n",
       " 'natural language understanding': 0.8884338736534119,\n",
       " 'non trivial': 0.8883756399154663,\n",
       " 'soft constraints': 0.887843668460846,\n",
       " 'stores': 0.8864191174507141,\n",
       " 'research fields': 0.8862491846084595,\n",
       " 'input variables': 0.8861205577850342,\n",
       " 'multiple labels': 0.8860496878623962,\n",
       " 'tilde': 0.8859490156173706,\n",
       " 'global optimization': 0.8857519030570984,\n",
       " 'research papers': 0.8855648636817932,\n",
       " 'an important part': 0.8853855133056641,\n",
       " 'evolution strategies': 0.8850476145744324,\n",
       " 'the statistics': 0.8849306702613831,\n",
       " 'short': 0.8847405314445496,\n",
       " 'times': 0.88462895154953,\n",
       " 'methodology': 0.8845105767250061,\n",
       " 'a prior distribution': 0.8843554258346558,\n",
       " 'pso': 0.88402259349823,\n",
       " 'caption generation': 0.8836677670478821,\n",
       " 'the domain': 0.8829420208930969,\n",
       " 'a patient': 0.8823274374008179,\n",
       " 'a convolutional neural network cnn': 0.882280170917511,\n",
       " 'relation': 0.8817753791809082,\n",
       " 'phones': 0.8816812634468079,\n",
       " 'human raters': 0.8806982040405273,\n",
       " 'scene understanding': 0.8805965781211853,\n",
       " 'visual understanding': 0.8805068135261536,\n",
       " 'services': 0.8804722428321838,\n",
       " 'user feedback': 0.8802728652954102,\n",
       " 'a markov decision process': 0.8797406554222107,\n",
       " 'text recognition': 0.8797227144241333,\n",
       " 'atp': 0.878601610660553,\n",
       " 'lsa': 0.8773389458656311,\n",
       " 'risk management': 0.8769517540931702,\n",
       " 'image compression': 0.876822829246521,\n",
       " 'latent vectors': 0.8767700791358948,\n",
       " 'counterexample': 0.8766002058982849,\n",
       " 'matrices': 0.8760292530059814,\n",
       " 'wood': 0.8758987188339233,\n",
       " 'yelp': 0.8758874535560608,\n",
       " 'fcm': 0.8748950362205505,\n",
       " 'machine intelligence': 0.8747112154960632,\n",
       " 'conservation': 0.873706042766571,\n",
       " 'image understanding': 0.8730435967445374,\n",
       " 'soccer': 0.8723496198654175,\n",
       " 'raw pixels': 0.8722046613693237,\n",
       " 'satellite images': 0.8719111084938049,\n",
       " 'mxnet': 0.8713755011558533,\n",
       " 'a general approach': 0.8711204528808594,\n",
       " 'the way': 0.8707624077796936,\n",
       " 'text to speech': 0.8702362775802612,\n",
       " 'bayesian neural networks': 0.8701648712158203,\n",
       " 'each observation': 0.8699389100074768,\n",
       " 'pillars': 0.8692358732223511,\n",
       " 'computational neuroscience': 0.8688834309577942,\n",
       " 'a convolutional layer': 0.8687971830368042,\n",
       " 'random forests': 0.868596076965332,\n",
       " 'regulation': 0.8682017922401428,\n",
       " 'popular algorithms': 0.8677942752838135,\n",
       " 'degrees of freedom': 0.8674420714378357,\n",
       " 'a question': 0.8671786189079285,\n",
       " 'binary classification problems': 0.8670347332954407,\n",
       " 'machine learning': 0.8663435578346252,\n",
       " 'k armed bandit problem': 0.8659676909446716,\n",
       " 'deep learning research': 0.8648884892463684,\n",
       " 'its representation': 0.8647471070289612,\n",
       " 'parameter learning': 0.8642616271972656,\n",
       " 'arguments': 0.8640195727348328,\n",
       " 'dynamic bayesian networks': 0.8634753823280334,\n",
       " 'an agent': 0.8627668619155884,\n",
       " 'dog': 0.8625105023384094,\n",
       " 'local learning rules': 0.8624286651611328,\n",
       " 'a flexible framework': 0.8622382283210754,\n",
       " 'their predictions': 0.8614320158958435,\n",
       " 'dl models': 0.8614056706428528,\n",
       " 'character level': 0.8602612018585205,\n",
       " 'the scale': 0.8601653575897217,\n",
       " 'temporal data': 0.8596248626708984,\n",
       " 'a discriminative model': 0.8595767021179199,\n",
       " 'the sequence': 0.8592604994773865,\n",
       " 'suggestions': 0.85869300365448,\n",
       " 'latent variable models': 0.8583236336708069,\n",
       " 'forecasts': 0.8572973012924194,\n",
       " 'expectations': 0.8572614789009094,\n",
       " 'bnn': 0.8571395874023438,\n",
       " 'navigation': 0.8569971919059753,\n",
       " 'bdeu': 0.8568911552429199,\n",
       " 'opinion mining': 0.8566862344741821,\n",
       " 'support vector machines': 0.856662929058075,\n",
       " 'a basis': 0.855797529220581,\n",
       " 'a new neural architecture': 0.8553797006607056,\n",
       " 'its memory': 0.8551512360572815,\n",
       " 'conceptual': 0.8548712730407715,\n",
       " 'necessary and sufficient conditions': 0.8537164330482483,\n",
       " 'learnability': 0.853467583656311,\n",
       " 'statistical inference': 0.8534347414970398,\n",
       " 'credit': 0.8533379435539246,\n",
       " 'machine learning models': 0.8530572056770325,\n",
       " 'latent structure': 0.8523520827293396,\n",
       " 'feature engineering': 0.8522591590881348,\n",
       " 'a deep': 0.8522583246231079,\n",
       " 'residual networks': 0.8522406220436096,\n",
       " 'joint training': 0.852226197719574,\n",
       " 'partial observability': 0.8505905270576477,\n",
       " 'feedforward networks': 0.8502131700515747,\n",
       " 'minecraft': 0.8500515818595886,\n",
       " 'data collection': 0.8498353362083435,\n",
       " 'two challenges': 0.8495635390281677,\n",
       " 'the contextual bandit': 0.849214494228363,\n",
       " 'complex datasets': 0.8489188551902771,\n",
       " 'an open problem': 0.8480241894721985,\n",
       " 'a lot of attention': 0.8480059504508972,\n",
       " 'empowerment': 0.847713053226471,\n",
       " 'rapid development': 0.8468009829521179,\n",
       " 'a powerful framework': 0.8463810682296753,\n",
       " 'causal relationships': 0.8463297486305237,\n",
       " 'a classifier': 0.8463027477264404,\n",
       " 'feed forward neural networks': 0.8461796045303345,\n",
       " 'tags': 0.8459604978561401,\n",
       " 'rotational symmetry': 0.8458553552627563,\n",
       " 'each user': 0.8457369804382324,\n",
       " 'parts': 0.8453246355056763,\n",
       " 'compositionality': 0.8451696634292603,\n",
       " 'gaussian mixture models': 0.84507155418396,\n",
       " 'a complex': 0.8445248603820801,\n",
       " 'leverage': 0.8444469571113586,\n",
       " 'the image': 0.8443487286567688,\n",
       " 'shallow ones': 0.8440086841583252,\n",
       " 'error detection': 0.8439683318138123,\n",
       " 'synapses': 0.843612551689148,\n",
       " 'increasing interest': 0.8429778814315796,\n",
       " 'independence': 0.842584490776062,\n",
       " 'unseen environments': 0.8424839973449707,\n",
       " 'innovation': 0.8411881327629089,\n",
       " 'highly efficient deep neural networks': 0.841034471988678,\n",
       " 'human learning': 0.8401773571968079,\n",
       " 'the evidence': 0.8399114608764648,\n",
       " 'a set of assumptions': 0.8393046855926514,\n",
       " 'efficacy': 0.839156448841095,\n",
       " 'each individual': 0.8377796411514282,\n",
       " 'the output sequence': 0.8374543786048889,\n",
       " 'strings': 0.8373257517814636,\n",
       " 'evolutionary algorithms': 0.8364850282669067,\n",
       " 'delayed feedback': 0.8364776968955994,\n",
       " 'long sequences': 0.8354780077934265,\n",
       " 'each datapoint': 0.8354505896568298,\n",
       " 'feed forward networks': 0.8352397084236145,\n",
       " 'sequence transduction': 0.8348884582519531,\n",
       " 'structured data': 0.8346184492111206,\n",
       " 'background knowledge': 0.8343864679336548,\n",
       " 'the de facto standard': 0.8340482711791992,\n",
       " '2001': 0.8339589834213257,\n",
       " 'model free and model': 0.8339409828186035,\n",
       " 'a novel class': 0.8334423303604126,\n",
       " 'the pixels': 0.8319401144981384,\n",
       " 'friendly': 0.8312481641769409,\n",
       " 'quantum physics': 0.8312070369720459,\n",
       " 'reinforcement learning problems': 0.8306552767753601,\n",
       " 'possible classes': 0.8304684162139893,\n",
       " 'accelerometers': 0.8294021487236023,\n",
       " 'combinatorial optimization problems': 0.8285389542579651,\n",
       " 'multiple sources': 0.8283200263977051,\n",
       " 'computer chess': 0.8275594115257263,\n",
       " 'transparency': 0.8263778686523438,\n",
       " 'feature representation': 0.8260964751243591,\n",
       " 'the flexibility': 0.8258609175682068,\n",
       " 'image captioning': 0.8257724642753601,\n",
       " 'maximum': 0.8254377841949463,\n",
       " 'the objective functions': 0.8253185749053955,\n",
       " 'a multi layer perceptron': 0.8250415325164795,\n",
       " 'a margin': 0.8245974779129028,\n",
       " 'widespread adoption': 0.8236704468727112,\n",
       " 'aide': 0.8235156536102295,\n",
       " 'the sentence': 0.8233574032783508,\n",
       " 'malaria': 0.8220270872116089,\n",
       " 'class imbalance': 0.8213098049163818,\n",
       " 'popular methods': 0.8207727074623108,\n",
       " 'managers': 0.8207488656044006,\n",
       " 'fda': 0.8206568956375122,\n",
       " 'permutation': 0.8200228214263916,\n",
       " 'partitioning': 0.8187008500099182,\n",
       " 'inequalities': 0.8184804916381836,\n",
       " 'probability densities': 0.8183044791221619,\n",
       " 'relational databases': 0.818071186542511,\n",
       " 'input samples': 0.8176131844520569,\n",
       " 'the problem': 0.8171318769454956,\n",
       " 'vi': 0.8171164393424988,\n",
       " 'partial information': 0.8156599998474121,\n",
       " 'exploratory data analysis': 0.815524160861969,\n",
       " 'high dimensional state spaces': 0.8143717646598816,\n",
       " 'ensemble methods': 0.8143686652183533,\n",
       " 'pre training': 0.8141303658485413,\n",
       " 'image question': 0.812592625617981,\n",
       " 'dynamic systems': 0.8124848008155823,\n",
       " 'the computations': 0.8120498657226562,\n",
       " 'image recognition tasks': 0.8120381832122803,\n",
       " 'image and text': 0.8117232918739319,\n",
       " 'hyperspectral images': 0.8115612864494324,\n",
       " 'shortcomings': 0.8112652897834778,\n",
       " 'the road': 0.810396134853363,\n",
       " 'random fields': 0.8103067874908447,\n",
       " 'visual datasets': 0.8099572062492371,\n",
       " 'gms': 0.8099396228790283,\n",
       " 'computations': 0.808411717414856,\n",
       " 'the best of our knowledge': 0.807597279548645,\n",
       " 'drifts': 0.8074693083763123,\n",
       " 'forums': 0.8071332573890686,\n",
       " 'china': 0.8071219325065613,\n",
       " 'the representation': 0.8066443204879761,\n",
       " 'fpgas': 0.8057528138160706,\n",
       " 'adversarial attacks': 0.8041355609893799,\n",
       " 'human perception': 0.8029079437255859,\n",
       " 'local neighborhoods': 0.80286705493927,\n",
       " 'execution': 0.8022469878196716,\n",
       " 'a reinforcement learning agent': 0.8017274141311646,\n",
       " 'remarkable success': 0.8015387654304504,\n",
       " 'a sequence of tasks': 0.7995572686195374,\n",
       " 'the information loss': 0.7982854843139648,\n",
       " 'wda': 0.7980991005897522,\n",
       " 'gssl': 0.7980332374572754,\n",
       " 'a fundamental building block': 0.7970536351203918,\n",
       " 'the latent spaces': 0.7962884306907654,\n",
       " 'the most': 0.7955641150474548,\n",
       " 'accordance': 0.7954686880111694,\n",
       " 'a generator': 0.7944186329841614,\n",
       " 'sequence modeling tasks': 0.7942454814910889,\n",
       " 'broad applications': 0.7936105728149414,\n",
       " 'synaptic weights': 0.7926182746887207,\n",
       " 'the targets': 0.79206383228302,\n",
       " 'non stationary environments': 0.7916783690452576,\n",
       " 'classifier performance': 0.7913765907287598,\n",
       " 'real users': 0.7913063168525696,\n",
       " 'the recognition': 0.7908706665039062,\n",
       " 'lack of interpretability': 0.7905365824699402,\n",
       " 'multi label classification': 0.7884735465049744,\n",
       " 'observed data': 0.7883880138397217,\n",
       " 'this thesis': 0.7882653474807739,\n",
       " 'the ways': 0.7871040105819702,\n",
       " 'fingerprints': 0.7869740128517151,\n",
       " 'the parameters': 0.7866647839546204,\n",
       " 'towers': 0.7866148352622986,\n",
       " 'natural languages': 0.7864704728126526,\n",
       " 'dynet': 0.7853145003318787,\n",
       " 'multiple types': 0.7845902442932129,\n",
       " 'the shortcomings': 0.784355878829956,\n",
       " 'animals': 0.7836682796478271,\n",
       " 'multiple objects': 0.7808473110198975,\n",
       " 'many applications': 0.779987633228302,\n",
       " 'an unknown environment': 0.7792003750801086,\n",
       " 'two steps': 0.7780252695083618,\n",
       " 'newton s method': 0.777651846408844,\n",
       " 'ntl': 0.7776129841804504,\n",
       " 'strong results': 0.7775161862373352,\n",
       " 'each edge': 0.7768332958221436,\n",
       " 'incomplete data': 0.7767091989517212,\n",
       " 'local optima': 0.776667058467865,\n",
       " 'a variational approximation': 0.7765452265739441,\n",
       " 'various settings': 0.7761110663414001,\n",
       " 'policy gradient': 0.775302529335022,\n",
       " 'bioner': 0.7742921113967896,\n",
       " 'mortality': 0.7727751135826111,\n",
       " 'sparsity': 0.772331953048706,\n",
       " 'kinds': 0.7722641825675964,\n",
       " 'sparse models': 0.7720791697502136,\n",
       " 'new questions': 0.770717203617096,\n",
       " 'sga': 0.7702736854553223,\n",
       " 'the learning algorithm': 0.7701464891433716,\n",
       " 'meaningful features': 0.7698859572410583,\n",
       " 'the discriminator': 0.7696949243545532,\n",
       " 'a novel technique': 0.7696548700332642,\n",
       " 'low rank': 0.7694205641746521,\n",
       " 'new tasks': 0.7689749598503113,\n",
       " 'transitions': 0.7688896656036377,\n",
       " 'an algorithm': 0.7680367231369019,\n",
       " 'recommendations': 0.7671298384666443,\n",
       " 'a large number': 0.7664141058921814,\n",
       " 'the maximum': 0.7663503289222717,\n",
       " 'super convergence': 0.7652397155761719,\n",
       " 'good performance': 0.7638173699378967,\n",
       " 'a series': 0.7636881470680237,\n",
       " 'training images': 0.7635806798934937,\n",
       " 'modeling sequences': 0.7633939385414124,\n",
       " 'computational intelligence': 0.7626799941062927,\n",
       " 'lvms': 0.7623412013053894,\n",
       " 'a wide array': 0.7621877789497375,\n",
       " 'a target domain': 0.7616394758224487,\n",
       " 'sentence representations': 0.7607847452163696,\n",
       " 'an autoencoder': 0.7600594758987427,\n",
       " 'generative modeling': 0.7598571181297302,\n",
       " 'multiple sensors': 0.759476363658905,\n",
       " 'deep nets': 0.7587078213691711,\n",
       " 'kernel machines': 0.7578398585319519,\n",
       " 'a hypothesis': 0.7553766965866089,\n",
       " 'distributed representations': 0.7552461624145508,\n",
       " 'a linear transformation': 0.7548959255218506,\n",
       " 'this chapter': 0.7541972398757935,\n",
       " 'transition probabilities': 0.754024863243103,\n",
       " 'marginal inference': 0.7536486983299255,\n",
       " 'fundamental concepts': 0.753261387348175,\n",
       " 'a state of the art': 0.7526336908340454,\n",
       " 'the query': 0.7517420649528503,\n",
       " 'the predictor': 0.7516623139381409,\n",
       " 'promising performance': 0.7502046823501587,\n",
       " 'training and testing': 0.7499991059303284,\n",
       " 'the tasks': 0.7472382187843323,\n",
       " 'a theory': 0.7472118139266968,\n",
       " 'other words': 0.7470702528953552,\n",
       " 'textbf': 0.7469870448112488,\n",
       " 'the prediction': 0.7469024062156677,\n",
       " 'train models': 0.7460768222808838,\n",
       " 'cues': 0.7459189891815186,\n",
       " 'five years': 0.7455168962478638,\n",
       " 'short texts': 0.7453979849815369,\n",
       " 'targets': 0.7433202862739563,\n",
       " 'training error': 0.7432204484939575,\n",
       " 'the behavior': 0.7429625988006592,\n",
       " 'bottom': 0.7423710227012634,\n",
       " 'a single neural network': 0.7422877550125122,\n",
       " 'a framework': 0.7420093417167664,\n",
       " 'tremendous progress': 0.7417468428611755,\n",
       " 'a novel way': 0.7406929731369019,\n",
       " 'converse': 0.7404952645301819,\n",
       " 'a knowledge': 0.739751935005188,\n",
       " 'the latent representations': 0.7396705746650696,\n",
       " 'mil': 0.7396076917648315,\n",
       " 'cmr': 0.7386885285377502,\n",
       " 'a program': 0.7384161949157715,\n",
       " 'common methods': 0.737808883190155,\n",
       " 'the number of parameters': 0.736985981464386,\n",
       " 'mapper': 0.7365218997001648,\n",
       " 'limitations': 0.7364699840545654,\n",
       " 'computational models': 0.7359681129455566,\n",
       " 'gmm': 0.7355906367301941,\n",
       " 'channels': 0.7351190447807312,\n",
       " 'generalisation': 0.733924150466919,\n",
       " 'the connectivity': 0.7333793044090271,\n",
       " 'a practical approach': 0.7332121133804321,\n",
       " 'principle': 0.7330123782157898,\n",
       " 'bayesian network structures': 0.7318046689033508,\n",
       " 'network performance': 0.7311693429946899,\n",
       " 'persons': 0.730426013469696,\n",
       " 'the dependence': 0.7296530604362488,\n",
       " 'the behaviour': 0.7286200523376465,\n",
       " 'functions': 0.7276700735092163,\n",
       " 'goodness': 0.7263100147247314,\n",
       " 'complex data': 0.7261353135108948,\n",
       " 'long short term memory lstm units': 0.7254241108894348,\n",
       " 'causal inference': 0.7248090505599976,\n",
       " 'teams': 0.7245461344718933,\n",
       " 'rbms': 0.7231735587120056,\n",
       " 'the effects': 0.7229615449905396,\n",
       " 'two algorithms': 0.7227388024330139,\n",
       " 'the first one': 0.721991777420044,\n",
       " 'each set': 0.7208195924758911,\n",
       " 'painting': 0.7202725410461426,\n",
       " 'variational autoencoders': 0.7189602851867676,\n",
       " 'small datasets': 0.718056857585907,\n",
       " 'hyperparameter optimization': 0.716907262802124,\n",
       " 'distant supervision': 0.7166124582290649,\n",
       " 'their parameters': 0.716183602809906,\n",
       " 'a vocabulary': 0.7159432768821716,\n",
       " 'sparse representations': 0.7136636972427368,\n",
       " 'successes': 0.7134140729904175,\n",
       " 'every iteration': 0.7127822637557983,\n",
       " 'lspi': 0.7125681042671204,\n",
       " 'the content': 0.7114656567573547,\n",
       " 'rr': 0.7111476063728333,\n",
       " 'neighbourhood': 0.7101646065711975,\n",
       " 'anns': 0.7100328803062439,\n",
       " 'the network parameters': 0.7087909579277039,\n",
       " 'reranking': 0.707859992980957,\n",
       " 'automatic speech recognition asr': 0.7074037790298462,\n",
       " 'learners': 0.7066441178321838,\n",
       " 'things': 0.706440806388855,\n",
       " 'widespread use': 0.7062579393386841,\n",
       " 'compositional': 0.7051172256469727,\n",
       " 'the next token': 0.7043615579605103,\n",
       " 'an important challenge': 0.704252302646637,\n",
       " 'their variants': 0.7026848196983337,\n",
       " 'historical data': 0.7025587558746338,\n",
       " 'its size': 0.7015578150749207,\n",
       " 'growth': 0.7010083198547363,\n",
       " 'a special': 0.70023113489151,\n",
       " 'smt': 0.6996995806694031,\n",
       " 'vectors': 0.696955144405365,\n",
       " 'specialized hardware': 0.69615638256073,\n",
       " 'irl': 0.6961546540260315,\n",
       " 'a new approach': 0.6961448192596436,\n",
       " 'leakage': 0.6960037350654602,\n",
       " 'the past years': 0.6954662203788757,\n",
       " 'workers': 0.6947668194770813,\n",
       " 'feature extraction': 0.6945909261703491,\n",
       " 'multiple modes': 0.6930415630340576,\n",
       " 'bots': 0.6924794316291809,\n",
       " 'thousands': 0.6898996829986572,\n",
       " 'image and sentence': 0.6887975931167603,\n",
       " 'infogan': 0.6882677674293518,\n",
       " 'a large body': 0.6882544755935669,\n",
       " 'such classifiers': 0.6878548264503479,\n",
       " 'local minima': 0.6860520243644714,\n",
       " 'training data': 0.6842721104621887,\n",
       " 'a number': 0.6841856837272644,\n",
       " 'auto': 0.6829845309257507,\n",
       " 'the posterior': 0.6810891032218933,\n",
       " 'the two modalities': 0.680487334728241,\n",
       " 'the inputs': 0.6786754727363586,\n",
       " 'the input image': 0.6756337881088257,\n",
       " 'neural representations': 0.6748750805854797,\n",
       " 'supervised data': 0.6747536659240723,\n",
       " 'annotated training data': 0.6732074618339539,\n",
       " 'debate': 0.6725620031356812,\n",
       " 'the first method': 0.671545684337616,\n",
       " 'output examples': 0.6693520545959473,\n",
       " 'the infinite horizon': 0.6689040660858154,\n",
       " 'segmentations': 0.6687619686126709,\n",
       " 'the artificial intelligence': 0.6663554310798645,\n",
       " 'the requirements': 0.6658311486244202,\n",
       " 'a confidence': 0.665183424949646,\n",
       " 'states and actions': 0.6650020480155945,\n",
       " 'sub': 0.6641921401023865,\n",
       " 'the issue': 0.6637476086616516,\n",
       " 'the feature space': 0.662509560585022,\n",
       " 'related tasks': 0.662351131439209,\n",
       " 'the incremental': 0.6607754826545715,\n",
       " 'wild': 0.6600148677825928,\n",
       " 'a bayesian approach': 0.6599535942077637,\n",
       " 'recent work': 0.6593428254127502,\n",
       " 'different types': 0.6592218279838562,\n",
       " 'supervised information': 0.6588036417961121,\n",
       " 'the standard': 0.6555123329162598,\n",
       " 'klog': 0.6520097851753235,\n",
       " 'very good performance': 0.6518559455871582,\n",
       " 'the present work': 0.6484915614128113,\n",
       " 'priors': 0.6459107398986816,\n",
       " 'iterative methods': 0.6442307233810425,\n",
       " 'the entities': 0.643887460231781,\n",
       " 'the system': 0.6434328556060791,\n",
       " 'spam filtering': 0.6397036910057068,\n",
       " 'random variables': 0.6392979621887207,\n",
       " 'captions': 0.6385459899902344,\n",
       " 'simulators': 0.6385324597358704,\n",
       " 'considerable attention': 0.637424886226654,\n",
       " 'neural models': 0.6371679306030273,\n",
       " 'neural enquirer': 0.6361899375915527,\n",
       " 'long term dependencies': 0.6360817551612854,\n",
       " 'i vectors': 0.6351369619369507,\n",
       " 'a new model': 0.633560836315155,\n",
       " 'novel objects': 0.6333271861076355,\n",
       " 'loading': 0.6333090662956238,\n",
       " 'a convolutional network': 0.6294211745262146,\n",
       " 'matters': 0.6277950406074524,\n",
       " 'complex models': 0.623881459236145,\n",
       " 'new ones': 0.6232314705848694,\n",
       " 'anomalies': 0.6218276619911194,\n",
       " 'the solutions': 0.6209520697593689,\n",
       " 'the actions': 0.6209407448768616,\n",
       " 'building': 0.619417130947113,\n",
       " 'compact representations': 0.6183335781097412,\n",
       " 'prediction quality': 0.6166865825653076,\n",
       " 'selection methods': 0.6136096119880676,\n",
       " 'conditional probability': 0.6123448610305786,\n",
       " 'charts': 0.6120474934577942,\n",
       " 'real world datasets': 0.5992192625999451,\n",
       " 'he': 0.5975317358970642,\n",
       " 'a novel formulation': 0.5974516868591309,\n",
       " 'important class': 0.5957773923873901,\n",
       " 'black box optimization': 0.5943206548690796,\n",
       " 'care': 0.5896381735801697,\n",
       " 'a novel end': 0.5876012444496155,\n",
       " 'auctions': 0.5853363275527954,\n",
       " 'primal': 0.5850760340690613,\n",
       " 'imagenet': 0.5849754810333252,\n",
       " 'recent papers': 0.5848574638366699,\n",
       " 'the inverse': 0.5829775929450989,\n",
       " 'novel methods': 0.5797004699707031,\n",
       " 'learning algorithm': 0.5792047381401062,\n",
       " 'the utilization': 0.5734439492225647,\n",
       " 'auto encoders': 0.5681696534156799,\n",
       " 'the objectives': 0.5681663751602173,\n",
       " 'eda': 0.5672317147254944,\n",
       " 'the design': 0.5669710636138916,\n",
       " 'neural qa': 0.5584998726844788,\n",
       " 'the source': 0.556347668170929,\n",
       " 'regimes': 0.5562911033630371,\n",
       " 'informative features': 0.550546407699585,\n",
       " 'the existing methods': 0.5467631220817566,\n",
       " 'coco': 0.5454061031341553,\n",
       " 'the training of deep neural networks': 0.544105589389801,\n",
       " 'search engines': 0.5334097743034363,\n",
       " 'the target domain': 0.5261486768722534,\n",
       " 'focus': 0.5238409638404846,\n",
       " 'the suitability': 0.5225930213928223,\n",
       " 'the art neural networks': 0.5188605785369873,\n",
       " 'particles': 0.518572986125946,\n",
       " 'a new architecture': 0.5182510614395142,\n",
       " 'the key challenges': 0.5180069804191589,\n",
       " 'discovery': 0.516893744468689,\n",
       " 'traditional machine': 0.5164811015129089,\n",
       " 'modern machine': 0.5145233869552612,\n",
       " 'fewer iterations': 0.5114763379096985,\n",
       " 'mine': 0.5101981163024902,\n",
       " 'the statistical power': 0.5096299052238464,\n",
       " 'a central role': 0.5073515772819519,\n",
       " 'scarcity': 0.5054118037223816,\n",
       " 'certain conditions': 0.5013009905815125}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_extracted = {k: v for k, v in sorted(not_extracted.items(), key=lambda x: x[1], reverse=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 7.509563637597694e-09,\n",
       " 'that': 9.885157226108277e-08,\n",
       " 'they': 2.9219418706816214e-07,\n",
       " 'examples': 2.0417508039827226e-06,\n",
       " 'no': 6.233447948034154e-06,\n",
       " 'variables': 6.705967280140612e-06,\n",
       " 'less': 1.1556920071598142e-05,\n",
       " 'classifiers': 2.4256296455860138e-05,\n",
       " 'large': 5.9070596762467176e-05,\n",
       " 'size': 6.213351298356429e-05,\n",
       " 'much': 0.0002085403975797817,\n",
       " 'clusters': 0.00026979201356880367,\n",
       " 'the same': 0.00031023111660033464,\n",
       " 'a novel': 0.0003152258868794888,\n",
       " 'limited': 0.00033621781039983034,\n",
       " 'humans': 0.0004211677878629416,\n",
       " 'time': 0.0006050258525647223,\n",
       " 'few': 0.0012957359431311488,\n",
       " 'pairs': 0.0013289264170452952,\n",
       " 'two': 0.0013801483437418938,\n",
       " 'embeddings': 0.0016081316862255335,\n",
       " 'adaptation': 0.0018283353419974446,\n",
       " 'general': 0.001859933719970286,\n",
       " 'three': 0.0018736894708126783,\n",
       " 'ones': 0.0020113487262278795,\n",
       " 'common': 0.002660851925611496,\n",
       " 'increases': 0.002756701782345772,\n",
       " 'the current state': 0.0031826773192733526,\n",
       " 'ability': 0.004174321889877319,\n",
       " 'particular': 0.0044441730715334415,\n",
       " 'learning methods': 0.005339717026799917,\n",
       " 'improvement': 0.005361965857446194,\n",
       " 'art': 0.005941392853856087,\n",
       " 'there': 0.006314059719443321,\n",
       " 'gaussian': 0.006625788751989603,\n",
       " 'dnn': 0.0069477553479373455,\n",
       " 'distributions': 0.008303927257657051,\n",
       " 'classification': 0.008361267857253551,\n",
       " 'modeling': 0.009464451111853123,\n",
       " 'unsupervised': 0.009623776189982891,\n",
       " 'quality': 0.011520229279994965,\n",
       " 'the features': 0.015889106318354607,\n",
       " 'underlying': 0.018611295148730278,\n",
       " 'rnn': 0.01951390691101551,\n",
       " 'frameworks': 0.02116807922720909,\n",
       " 'component': 0.022103821858763695,\n",
       " 'performance': 0.02240290492773056,\n",
       " 'the efficacy': 0.023195594549179077,\n",
       " 'tuning': 0.027140550315380096,\n",
       " 'literature': 0.030187003314495087,\n",
       " 'existing state': 0.03266017511487007,\n",
       " 'then': 0.03688385337591171,\n",
       " 'light': 0.03831513226032257,\n",
       " 'itself': 0.03898904100060463,\n",
       " 'multipliers': 0.04169734939932823,\n",
       " 'outliers': 0.04474342614412308,\n",
       " 'modalities': 0.04646702855825424,\n",
       " 'superiority': 0.04679834842681885,\n",
       " 'categories': 0.04683368653059006,\n",
       " 'inter': 0.04876645281910896,\n",
       " 'the original': 0.05392768234014511,\n",
       " 'changes': 0.05859411880373955,\n",
       " 'benchmarks': 0.05866920202970505,\n",
       " 'training examples': 0.0607040636241436,\n",
       " 'a way': 0.06677544116973877,\n",
       " 'simulations': 0.09165040403604507,\n",
       " 'alternatives': 0.09450375288724899,\n",
       " 'the art models': 0.09483199566602707,\n",
       " 'supervision': 0.09986941516399384,\n",
       " 'invariant features': 0.11079826951026917,\n",
       " '3d': 0.11681612581014633,\n",
       " 'fact': 0.13514049351215363,\n",
       " 'kernel hilbert': 0.155992329120636,\n",
       " 'an order of magnitude': 0.16548024117946625,\n",
       " 'cca': 0.1734902262687683,\n",
       " 'none': 0.18431732058525085,\n",
       " 'the amount': 0.18573404848575592,\n",
       " 'hboa': 0.20203575491905212,\n",
       " 'players': 0.20227168500423431,\n",
       " 'estimation': 0.20653745532035828,\n",
       " 'developments': 0.208437979221344,\n",
       " 'instability': 0.20900477468967438,\n",
       " 'outputs': 0.21188701689243317,\n",
       " 'the next': 0.21448509395122528,\n",
       " 'four': 0.21733640134334564,\n",
       " 'catastrophic': 0.2190180867910385,\n",
       " 'invariances': 0.22905407845973969,\n",
       " 'the other hand': 0.23079128563404083,\n",
       " 'prediction models': 0.24262143671512604,\n",
       " 'deep q': 0.2476600706577301,\n",
       " 'impressive results': 0.25173965096473694,\n",
       " 'optimizers': 0.2529279589653015,\n",
       " 'many researchers': 0.2622450590133667,\n",
       " 'place': 0.26284679770469666,\n",
       " 'an effective way': 0.2630273401737213,\n",
       " 'powerful models': 0.26625266671180725,\n",
       " 'settings': 0.27079564332962036,\n",
       " 'a large variety': 0.2708013951778412,\n",
       " 'publications': 0.2788156569004059,\n",
       " 'generative models': 0.28557994961738586,\n",
       " 'convnets': 0.29661479592323303,\n",
       " 'the ones': 0.29663804173469543,\n",
       " 'limits': 0.29794788360595703,\n",
       " 'a novel approach': 0.3062872290611267,\n",
       " 'hierarchical reinforcement': 0.30739811062812805,\n",
       " 'demonstration': 0.3223034143447876,\n",
       " 'unlabeled examples': 0.3235204219818115,\n",
       " 'the one': 0.32970839738845825,\n",
       " 'meaningful representations': 0.33105912804603577,\n",
       " 'favor': 0.3338285982608795,\n",
       " '2016': 0.33439743518829346,\n",
       " 'texts': 0.3344111740589142,\n",
       " 'the video': 0.3367150127887726,\n",
       " 'every day': 0.3516875207424164,\n",
       " 'quantification': 0.36058905720710754,\n",
       " 'the predictions': 0.36223629117012024,\n",
       " 'drl': 0.36525848507881165,\n",
       " 'the importance': 0.366645485162735,\n",
       " 'regularities': 0.37044912576675415,\n",
       " 'test accuracy': 0.3727949857711792,\n",
       " 'a small set': 0.3732532858848572,\n",
       " 'bugs': 0.37733370065689087,\n",
       " 'predictors': 0.38026487827301025,\n",
       " 'generalization ability': 0.3835054934024811,\n",
       " 'the test': 0.3853375017642975,\n",
       " 'the framework': 0.38571780920028687,\n",
       " 'gwas': 0.3914712965488434,\n",
       " 'the data': 0.3978893756866455,\n",
       " 'machine learning tasks': 0.39873307943344116,\n",
       " 'the analysis': 0.40237605571746826,\n",
       " 'mislabeled': 0.403685986995697,\n",
       " 'prediction': 0.4100523293018341,\n",
       " 'deeper and wider': 0.4121902883052826,\n",
       " 'image text': 0.41418883204460144,\n",
       " 'the value': 0.41431325674057007,\n",
       " 'large quantities': 0.41668957471847534,\n",
       " 'belief networks': 0.4212169349193573,\n",
       " 'stochastic': 0.42293840646743774,\n",
       " 'a natural way': 0.44579970836639404,\n",
       " 'the art systems': 0.4460993707180023,\n",
       " 'many areas': 0.44723203778266907,\n",
       " 'unsolved': 0.44784486293792725,\n",
       " 'causal models': 0.44811487197875977,\n",
       " 'the network': 0.4504739046096802,\n",
       " 'optimizations': 0.45107895135879517,\n",
       " 'dl': 0.45374614000320435,\n",
       " 'prior methods': 0.4546011686325073,\n",
       " 'optimization algorithms': 0.4554068148136139,\n",
       " 'abstractions': 0.45678478479385376,\n",
       " 'text or images': 0.4576587378978729,\n",
       " 'better generalization': 0.4585266709327698,\n",
       " 'ladder': 0.4594802260398865,\n",
       " 'powerful tools': 0.4599868953227997,\n",
       " 'the quality': 0.460440456867218,\n",
       " 'the latent code': 0.46300435066223145,\n",
       " 'constraints': 0.4673337936401367,\n",
       " 'a good representation': 0.47014668583869934,\n",
       " 'dr': 0.47188472747802734,\n",
       " 'neural network architecture': 0.4841635525226593,\n",
       " 'gcns': 0.4922143220901489,\n",
       " 'shared representations': 0.4922332167625427,\n",
       " 'the outputs': 0.49312543869018555}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(extracted, open(f'data/pem_extracted_vs_{VECTOR_SIZE}_ws_{WINDOW_SIZE}_nl_{NUM_LAYERS}_mf_{MIN_FREQUENCY}_e_{EPOCHS}.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pem",
   "language": "python",
   "name": "pem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
