{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "twenty-billy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Classifier implementation (model architecture, training, testing, etc.) derived from\n",
    "#     https://towardsdatascience.com/pytorch-tabular-binary-classification-a0368da5bb89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "square-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "behind-track",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PHRASE_LEN = 6\n",
    "VECTOR_SIZE = 200\n",
    "WINDOW_SIZE = 10\n",
    "NUM_LAYERS = 5\n",
    "\n",
    "MIN_FREQUENCY = 5\n",
    "\n",
    "SHOULD_EXTRACT_NOUN_PHRASES = False\n",
    "SHOULD_GENERATE_UNDERSCORED_CORPUS = False\n",
    "SHOULD_TRAIN_WORD2VEC_MODEL = False\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "instructional-utilization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 arxiv abstracts.\n"
     ]
    }
   ],
   "source": [
    "with open('data/arxiv_abstracts_10000.txt', 'r') as f:\n",
    "    arxiv_abstracts = f.read().split('\\n')[:-1]\n",
    "    arxiv_abstracts_raw = '\\n'.join(arxiv_abstracts)\n",
    "    f.close()\n",
    "print(f'Loaded {len(arxiv_abstracts)} arxiv abstracts.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "greatest-baghdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1900 negative samples.\n"
     ]
    }
   ],
   "source": [
    "negative_samples = pickle.load(open('data/negative_samples.pkl', 'rb'))\n",
    "print(f'Loaded {len(negative_samples)} negative samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "closing-leader",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1900 positive samples.\n"
     ]
    }
   ],
   "source": [
    "positive_samples = pickle.load(open('data/positive_samples.pkl', 'rb'))\n",
    "print(f'Loaded {len(positive_samples)} positive samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "binding-hospital",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_phrase(tree_str, label):\n",
    "    phrases = []\n",
    "    trees = Tree.fromstring(tree_str)\n",
    "    for tree in trees:\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == label:\n",
    "                t = subtree\n",
    "                t = ' '.join(t.leaves())\n",
    "                phrases.append(t)\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "temporal-missile",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOULD_EXTRACT_NOUN_PHRASES:\n",
    "    nlp = StanfordCoreNLP('data/stanford-corenlp-4.1.0')\n",
    "    noun_phrases = []\n",
    "    for i, abstract in enumerate(arxiv_abstracts):\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f'Extracting noun phrases from abstract {i + 1} of {len(arxiv_abstracts)}')\n",
    "            pickle.dump(noun_phrases, open('data/noun_phrases.pkl', 'wb'))\n",
    "        try:\n",
    "            tree_str = nlp.parse(abstract)\n",
    "            noun_phrases.extend(extract_phrase(tree_str, 'NP'))\n",
    "        except Exception:\n",
    "            pass\n",
    "    noun_phrases = [np for np in list(set(noun_phrases)) if len(np.split()) <= MAX_PHRASE_LEN]\n",
    "    pickle.dump(noun_phrases, open('data/noun_phrases.pkl', 'wb'))\n",
    "noun_phrases = pickle.load(open('data/noun_phrases.pkl', 'rb'))\n",
    "noun_phrases = [np for np in list(set(noun_phrases)) if len(np.split()) <= MAX_PHRASE_LEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "challenging-cabin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_phrase_in_corpus(corpus, phrase):\n",
    "    s_idx = corpus.find(phrase)\n",
    "    e_idx = s_idx + len(phrase)\n",
    "    if s_idx != -1 and \\\n",
    "       (s_idx == 0 or corpus[s_idx - 1] in (string.punctuation + ' ')) and \\\n",
    "       (e_idx == len(corpus) or corpus[e_idx] in (string.punctuation + ' ')):\n",
    "        return (s_idx, e_idx)\n",
    "    return (-1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "tested-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOULD_GENERATE_UNDERSCORED_CORPUS:\n",
    "    corpus = arxiv_abstracts_raw[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "pursuant-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOULD_GENERATE_UNDERSCORED_CORPUS:\n",
    "    for i, positive_sample in enumerate(positive_samples):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Replacing positive_sample {i + 1} of {len(positive_samples)}')\n",
    "        found_indices = set()\n",
    "        while find_phrase_in_corpus(corpus, positive_sample) != (-1, -1) and find_phrase_in_corpus(corpus, positive_sample)[0] not in found_indices:\n",
    "            s_idx, e_idx = find_phrase_in_corpus(corpus, positive_sample)\n",
    "            found_indices.add(s_idx)\n",
    "            corpus = corpus[:s_idx] + positive_sample.replace(' ', '_') + corpus[e_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "spectacular-graduation",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOULD_GENERATE_UNDERSCORED_CORPUS:\n",
    "    for i, negative_sample in enumerate(negative_samples):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Replacing negative_sample {i + 1} of {len(negative_samples)}')\n",
    "        found_indices = set()\n",
    "        while find_phrase_in_corpus(corpus, negative_sample) != (-1, -1) and find_phrase_in_corpus(corpus, negative_sample)[0] not in found_indices:\n",
    "            s_idx, e_idx = find_phrase_in_corpus(corpus, negative_sample)\n",
    "            found_indices.add(s_idx)\n",
    "            corpus = corpus[:s_idx] + negative_sample.replace(' ', '_') + corpus[e_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "incorporated-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOULD_GENERATE_UNDERSCORED_CORPUS:\n",
    "    for i, noun_phrase in enumerate(noun_phrases):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Replacing noun_phrase {i + 1} of {len(noun_phrases)}')\n",
    "        found_indices = set()\n",
    "        while find_phrase_in_corpus(corpus, noun_phrase) != (-1, -1) and find_phrase_in_corpus(corpus, noun_phrase)[0] not in found_indices:\n",
    "            s_idx, e_idx = find_phrase_in_corpus(corpus, noun_phrase)\n",
    "            found_indices.add(s_idx)\n",
    "            corpus = corpus[:s_idx] + noun_phrase.replace(' ', '_') + corpus[e_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "defensive-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOULD_GENERATE_UNDERSCORED_CORPUS:\n",
    "    pickle.dump(corpus, open('data/underscored_corpus.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sixth-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "underscored_corpus = pickle.load(open('data/underscored_corpus.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "noticed-bridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOULD_TRAIN_WORD2VEC_MODEL:\n",
    "    underscored_corpus_data = []\n",
    "    for i in sent_tokenize(underscored_corpus):\n",
    "        temp = []\n",
    "        for j in word_tokenize(i):\n",
    "            temp.append(j.lower())\n",
    "        underscored_corpus_data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "driven-investment",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOULD_TRAIN_WORD2VEC_MODEL:\n",
    "    word2vec_model = Word2Vec(underscored_corpus_data, min_count=1, window=WINDOW_SIZE, size=VECTOR_SIZE)\n",
    "    word2vec_model.save(f'data/word2vec_model_vs_{VECTOR_SIZE}_ws_{WINDOW_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "difficult-spiritual",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec.load(f'data/word2vec_model_vs_{VECTOR_SIZE}_ws_{WINDOW_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "elder-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token for token in list(word2vec_model.wv.vocab.keys())]\n",
    "embeddings = {token: word2vec_model.wv[token] for token in tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adequate-pakistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_samples = [ps for ps in positive_samples if ps.replace(' ', '_') in embeddings and word2vec_model.wv.vocab[ps.replace(' ', '_')].count >= MIN_FREQUENCY]\n",
    "negative_samples = [ns for ns in negative_samples if ns.replace(' ', '_') in embeddings and word2vec_model.wv.vocab[ns.replace(' ', '_')].count >= MIN_FREQUENCY]\n",
    "\n",
    "positive_samples = positive_samples[:min(len(positive_samples), len(negative_samples))]\n",
    "negative_samples = negative_samples[:min(len(positive_samples), len(negative_samples))]\n",
    "\n",
    "ps_set = set(positive_samples)\n",
    "ns_set = set(negative_samples)\n",
    "\n",
    "noun_phrases = [np for np in noun_phrases if np not in ps_set and np not in ns_set and np.replace(' ', '_') in embeddings and word2vec_model.wv.vocab[np.replace(' ', '_')].count >= MIN_FREQUENCY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "criminal-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for phrase in positive_samples:\n",
    "    X.append(embeddings[phrase.replace(' ', '_')])\n",
    "    y.append(1)\n",
    "for phrase in negative_samples:\n",
    "    X.append(embeddings[phrase.replace(' ', '_')])\n",
    "    y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "automated-antibody",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = list(zip(X, y))\n",
    "random.shuffle(c)\n",
    "X, y = zip(*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "announced-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "appointed-blues",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "\n",
    "        self.layer_1 = nn.Linear(VECTOR_SIZE, 128)\n",
    "        \n",
    "        self.layers = []\n",
    "        for _ in range(NUM_LAYERS - 1):\n",
    "            self.layers.append(nn.Linear(128, 128))\n",
    "        \n",
    "        self.layer_out = nn.Linear(128, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = self.relu(layer(x))\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "distributed-spirit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryClassifier(\n",
       "  (layer_1): Linear(in_features=200, out_features=128, bias=True)\n",
       "  (layer_out): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BinaryClassifier()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "applicable-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "logical-incentive",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "train_data = TrainDataset(torch.FloatTensor(np.array(X_train, dtype=np.float64)), \n",
    "                          torch.FloatTensor(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "persistent-dynamics",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "\n",
    "test_data = TestDataset(torch.FloatTensor(np.array(X_test, dtype=np.float64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "disturbed-frederick",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "every-science",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "mathematical-biodiversity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010: | Loss: 0.63246 | Acc: 67.440\n",
      "Epoch 020: | Loss: 0.59203 | Acc: 68.560\n",
      "Epoch 030: | Loss: 0.56759 | Acc: 70.760\n",
      "Epoch 040: | Loss: 0.55131 | Acc: 71.960\n",
      "Epoch 050: | Loss: 0.53677 | Acc: 72.600\n",
      "Epoch 060: | Loss: 0.52975 | Acc: 73.040\n",
      "Epoch 070: | Loss: 0.51633 | Acc: 74.040\n",
      "Epoch 080: | Loss: 0.50473 | Acc: 75.040\n",
      "Epoch 090: | Loss: 0.49867 | Acc: 75.160\n",
      "Epoch 100: | Loss: 0.49279 | Acc: 75.200\n"
     ]
    }
   ],
   "source": [
    "epoch_losses = []\n",
    "for e in range(1, EPOCHS + 1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "\n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    epoch_losses.append(epoch_loss / len(train_loader)) \n",
    "\n",
    "    if e % 10 == 0:\n",
    "        print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "saving-stadium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7faf1f32a5e0>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "alpha-football",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "competent-spirit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[153  47]\n",
      " [ 50 139]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.77      0.76       200\n",
      "           1       0.75      0.74      0.74       189\n",
      "\n",
      "    accuracy                           0.75       389\n",
      "   macro avg       0.75      0.75      0.75       389\n",
      "weighted avg       0.75      0.75      0.75       389\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred_list))\n",
    "print(classification_report(y_test, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "signed-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = random.sample(noun_phrases, 1000)\n",
    "X_samples = []\n",
    "for sample in test_samples:\n",
    "    X_samples.append(embeddings[sample.replace(' ', '_')])\n",
    "sample_data = TestDataset(torch.FloatTensor(np.array(X_samples, dtype=np.float64)))\n",
    "sample_loader = DataLoader(dataset=sample_data, batch_size=1)\n",
    "\n",
    "extracted = {}\n",
    "not_extracted = {}\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, X_batch in enumerate(sample_loader):\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        if y_pred_tag.cpu().numpy()[0][0] == 1:\n",
    "            extracted[test_samples[i]] = y_test_pred.item()\n",
    "        else:\n",
    "            not_extracted[test_samples[i]] = y_test_pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "tropical-delight",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = set(['a', 'the', 'an'])\n",
    "extracted = {k: v for k, v in sorted(extracted.items(), key=lambda x: x[1], reverse=True) if k.split()[0] not in articles}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "frequent-desire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': 0.9999172687530518,\n",
       " 'paper': 0.9989681243896484,\n",
       " 'strategy': 0.9988766312599182,\n",
       " 'robot': 0.9970023036003113,\n",
       " 'building': 0.9907234311103821,\n",
       " 'topic': 0.9877485632896423,\n",
       " 'sparsity': 0.9834427237510681,\n",
       " 'd': 0.9834275841712952,\n",
       " 'fashion': 0.9829161167144775,\n",
       " 'approximate': 0.9814698696136475,\n",
       " 'person': 0.9810019135475159,\n",
       " 'cost': 0.9793145656585693,\n",
       " 'tensor': 0.9792962074279785,\n",
       " 'research': 0.9712300300598145,\n",
       " 'general': 0.9704872965812683,\n",
       " 'policy': 0.9692484736442566,\n",
       " 'w': 0.9653392434120178,\n",
       " '0': 0.9595668315887451,\n",
       " 'right': 0.9565486311912537,\n",
       " 'estimation': 0.9524744153022766,\n",
       " 'incremental': 0.9508026838302612,\n",
       " 'it': 0.9490135908126831,\n",
       " 'r': 0.9439978003501892,\n",
       " 'gradient': 0.934786319732666,\n",
       " 'science': 0.929459810256958,\n",
       " 'media': 0.9241527915000916,\n",
       " 'general purpose': 0.9212682247161865,\n",
       " 'corpus': 0.9205535650253296,\n",
       " 'this gap': 0.9197768568992615,\n",
       " 'markov': 0.9136368632316589,\n",
       " 'selection': 0.9027211666107178,\n",
       " 'background': 0.8938397169113159,\n",
       " 'crf': 0.8919385671615601,\n",
       " 'sample': 0.883114755153656,\n",
       " 'response': 0.8814322352409363,\n",
       " 'explanation': 0.8794811367988586,\n",
       " 'imaging': 0.8749668598175049,\n",
       " 'rise': 0.8729295134544373,\n",
       " 'line': 0.872821033000946,\n",
       " 'relevance': 0.8701542615890503,\n",
       " 'distinct': 0.8678210377693176,\n",
       " '6': 0.8671270608901978,\n",
       " 'multimodal': 0.8670343160629272,\n",
       " 'bandits': 0.8657655119895935,\n",
       " 'evolution': 0.8631789684295654,\n",
       " 'tune': 0.8562034368515015,\n",
       " 'problem': 0.8553633093833923,\n",
       " 'svm': 0.8527781963348389,\n",
       " 'identity': 0.851759135723114,\n",
       " 'play': 0.8487329483032227,\n",
       " 'sketch': 0.8479601144790649,\n",
       " 'fisher': 0.8467410206794739,\n",
       " 'fuse': 0.8425687551498413,\n",
       " 'component analysis': 0.8420618772506714,\n",
       " 'logistic regression': 0.8404162526130676,\n",
       " 'analytics': 0.8385039567947388,\n",
       " 'co': 0.8260604739189148,\n",
       " 'discrimination': 0.8223969340324402,\n",
       " 'mlp': 0.8208237886428833,\n",
       " 'extracts': 0.8141277432441711,\n",
       " 'real world': 0.8138020634651184,\n",
       " 'beam': 0.8084099292755127,\n",
       " 'low rank': 0.8060426712036133,\n",
       " 'value function': 0.8014166355133057,\n",
       " 'this issue': 0.799991250038147,\n",
       " 'matrices': 0.7979308366775513,\n",
       " 'large amount': 0.7919034361839294,\n",
       " 'pool': 0.7866413593292236,\n",
       " 'their corresponding': 0.7857658267021179,\n",
       " 'denoising': 0.7833644151687622,\n",
       " 'physics': 0.7790684700012207,\n",
       " 'disparity': 0.7760913372039795,\n",
       " 'maximization': 0.7744736671447754,\n",
       " 'multiplication': 0.7732263803482056,\n",
       " 'places': 0.7675291299819946,\n",
       " 'cca': 0.7671341896057129,\n",
       " 'participants': 0.7657080292701721,\n",
       " 'long standing': 0.7582643032073975,\n",
       " 'mine': 0.7548642158508301,\n",
       " 'id': 0.7532557249069214,\n",
       " 'iterates': 0.7529618740081787,\n",
       " 'variational approximation': 0.7496363520622253,\n",
       " 'inlier': 0.7496260404586792,\n",
       " 'manifolds': 0.7494024634361267,\n",
       " 'under': 0.7462239861488342,\n",
       " 'human activities': 0.7390572428703308,\n",
       " 'noise ratio': 0.7372877597808838,\n",
       " 'great': 0.7370981574058533,\n",
       " 'reason': 0.7360208630561829,\n",
       " 'bag of words': 0.7352941632270813,\n",
       " 'attracting': 0.7198279500007629,\n",
       " 'markov decision': 0.714036226272583,\n",
       " 'kernel hilbert spaces': 0.7110787630081177,\n",
       " 'collections': 0.7087499499320984,\n",
       " 'transition': 0.7076858282089233,\n",
       " 'developers': 0.7057403922080994,\n",
       " 'two algorithms': 0.7042635083198547,\n",
       " 'infinity': 0.7038019299507141,\n",
       " 'noun phrases': 0.7017941474914551,\n",
       " 'es': 0.6990454196929932,\n",
       " 'radio': 0.6970445513725281,\n",
       " 'marginal probabilities': 0.6951811909675598,\n",
       " 'ability': 0.6926435828208923,\n",
       " 'innovation': 0.6923555135726929,\n",
       " 'evolution strategies': 0.6866605877876282,\n",
       " 'q sigma': 0.6842000484466553,\n",
       " 'concentration': 0.6839686036109924,\n",
       " 'deformations': 0.6810376644134521,\n",
       " 'dissimilarity': 0.6772100925445557,\n",
       " 'regression model': 0.6735252737998962,\n",
       " 'vanishing': 0.673091471195221,\n",
       " 'fire': 0.6699212193489075,\n",
       " 'processes': 0.6685181856155396,\n",
       " 'image and text': 0.6678209900856018,\n",
       " 'dl': 0.6673164367675781,\n",
       " 'context': 0.6663923263549805,\n",
       " 'ranks': 0.6614649891853333,\n",
       " 'character level': 0.6602292656898499,\n",
       " 'loading': 0.660005509853363,\n",
       " 'nesterov s': 0.6521448493003845,\n",
       " 'shift': 0.6478900909423828,\n",
       " 'square loss': 0.6456851363182068,\n",
       " 'point processes': 0.6446141004562378,\n",
       " 'image and sentence': 0.6413201689720154,\n",
       " 'large number of parameters': 0.6397599577903748,\n",
       " 'discriminative learning': 0.6368923187255859,\n",
       " 'meta learning': 0.6365437507629395,\n",
       " 'convnet': 0.635787308216095,\n",
       " 'handwriting': 0.6348390579223633,\n",
       " 'biometric': 0.6321216821670532,\n",
       " 'theta 1': 0.6309835314750671,\n",
       " 'its application': 0.6299776434898376,\n",
       " 'static': 0.6291345953941345,\n",
       " 'counterexample': 0.6290552616119385,\n",
       " 'sfa': 0.6239048838615417,\n",
       " 'newton s': 0.6208345293998718,\n",
       " 'pictures': 0.6207451224327087,\n",
       " 'expected reward': 0.620058000087738,\n",
       " 'large scale': 0.6149429082870483,\n",
       " 'nn': 0.6146552562713623,\n",
       " 'lexicon based': 0.6123376488685608,\n",
       " 'cooperation': 0.6117979884147644,\n",
       " 'cma': 0.6111149787902832,\n",
       " 'industry': 0.6110958456993103,\n",
       " 'gmm kernel': 0.6101107597351074,\n",
       " 'gmm': 0.6081387996673584,\n",
       " 'hyperparameter optimization': 0.6064388751983643,\n",
       " 'misclassification': 0.6024521589279175,\n",
       " 'smile': 0.6002286076545715,\n",
       " 'neural network training': 0.5959241986274719,\n",
       " 'recurrent neural network rnn': 0.5940375328063965,\n",
       " 'convex loss functions': 0.592565655708313,\n",
       " 'ucb1': 0.5913552045822144,\n",
       " 'generative adversarial network': 0.5879244804382324,\n",
       " 'updates': 0.5878617167472839,\n",
       " 'cars': 0.5864129662513733,\n",
       " 'set functions': 0.5826771259307861,\n",
       " 'visual object recognition': 0.5808568596839905,\n",
       " 'top': 0.580689013004303,\n",
       " 'their training': 0.5801413655281067,\n",
       " 'dogs': 0.579810619354248,\n",
       " 'taxis': 0.5791218876838684,\n",
       " 'paintings': 0.5769791007041931,\n",
       " 'surgery': 0.5769639611244202,\n",
       " 'weight vector': 0.5751155614852905,\n",
       " 'inferring': 0.5741755366325378,\n",
       " 'vi': 0.5731944441795349,\n",
       " 'noisy images': 0.5727101564407349,\n",
       " 'deep learning framework': 0.5720470547676086,\n",
       " 'sample complexity': 0.5712632536888123,\n",
       " 'booking': 0.5694352984428406,\n",
       " 'torch': 0.5651960968971252,\n",
       " 'large systems': 0.5644027590751648,\n",
       " 'non linear functions': 0.5549372434616089,\n",
       " 'phase retrieval': 0.5537552833557129,\n",
       " 'simple gradient': 0.5533815026283264,\n",
       " 'useful information': 0.5520824193954468,\n",
       " 'covariate': 0.5475139617919922,\n",
       " 'lfms': 0.5448424816131592,\n",
       " 'unsupervised feature learning': 0.5426940321922302,\n",
       " 'leakage': 0.542305052280426,\n",
       " 'structured data': 0.5397626161575317,\n",
       " 'video clips': 0.5378025770187378,\n",
       " 'supervised machine learning': 0.5375795960426331,\n",
       " 'black box optimization': 0.5335726737976074,\n",
       " 'diverse domains': 0.5335288047790527,\n",
       " 'event data': 0.5333110690116882,\n",
       " 'label propagation': 0.5327379703521729,\n",
       " 'drug discovery': 0.5318132638931274,\n",
       " 'learning machine elm': 0.5303736329078674,\n",
       " 'kws': 0.5302145481109619,\n",
       " 'semantic distance': 0.5279614329338074,\n",
       " 'rank minimization': 0.5269848704338074,\n",
       " 'video understanding': 0.525798499584198,\n",
       " 'image classifiers': 0.5230361819267273,\n",
       " 'visits': 0.521550178527832,\n",
       " 'three ways': 0.5210855007171631,\n",
       " 'cross entropy loss': 0.5202858448028564,\n",
       " 'rgb images': 0.5191381573677063,\n",
       " 'introduction': 0.5171067714691162,\n",
       " 'each input': 0.5157191157341003,\n",
       " 'their relationships': 0.51566082239151,\n",
       " 'temporal data': 0.5132160186767578,\n",
       " 'multi armed bandit mab': 0.5118433237075806,\n",
       " 'mfcc': 0.5065991282463074,\n",
       " 'permutations': 0.5005978941917419}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "starting-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_extracted = {k: v for k, v in sorted(not_extracted.items(), key=lambda x: x[1], reverse=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "animal-catch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'these methods': 0.0020317884627729654,\n",
       " 'the parameters': 0.0021229954436421394,\n",
       " 'dnns': 0.002636364195495844,\n",
       " 'our experiments': 0.0028326683677732944,\n",
       " 'weights': 0.003019964322447777,\n",
       " 'improvements': 0.0032591468188911676,\n",
       " 'experiments': 0.003640766255557537,\n",
       " 'those': 0.0037784907035529613,\n",
       " 'rules': 0.004754955880343914,\n",
       " 'samples': 0.005390053614974022,\n",
       " 'the features': 0.005754165351390839,\n",
       " 'a way': 0.010382863692939281,\n",
       " 'sizes': 0.012345620431005955,\n",
       " 'a few': 0.012364320456981659,\n",
       " 'hand': 0.012531294487416744,\n",
       " 'the training': 0.013096151873469353,\n",
       " 'relationships': 0.0158639308065176,\n",
       " 'attempts': 0.017722323536872864,\n",
       " 'trajectories': 0.020897231996059418,\n",
       " 'extensive experiments': 0.02256322093307972,\n",
       " 'policies': 0.02272098697721958,\n",
       " 'the representations': 0.023188861086964607,\n",
       " 'the existence': 0.030600933358073235,\n",
       " 'that': 0.0307770986109972,\n",
       " 'rewards': 0.033471789211034775,\n",
       " 'rnns': 0.03458833321928978,\n",
       " 'benchmark datasets': 0.035683389753103256,\n",
       " 'the art performance': 0.0365140326321125,\n",
       " 'objects': 0.038591429591178894,\n",
       " 'modalities': 0.03944020718336105,\n",
       " 'words': 0.039877817034721375,\n",
       " 'the literature': 0.04368004575371742,\n",
       " 'practice': 0.046176765114068985,\n",
       " 'ensembles': 0.04856813699007034,\n",
       " 'several state': 0.049219924956560135,\n",
       " 'machines': 0.04931049421429634,\n",
       " 'corpora': 0.05111946538090706,\n",
       " 'evidence': 0.053047362715005875,\n",
       " 'databases': 0.054504938423633575,\n",
       " 'frameworks': 0.05648210272192955,\n",
       " 'the range': 0.059169575572013855,\n",
       " 'preferences': 0.060297973453998566,\n",
       " 'literature': 0.06301144510507584,\n",
       " 'the dynamics': 0.06516590714454651,\n",
       " 'games': 0.06699305772781372,\n",
       " 'cues': 0.06729681044816971,\n",
       " 'the accuracy': 0.06833594292402267,\n",
       " 'ways': 0.07078427076339722,\n",
       " 'existing state': 0.07407684624195099,\n",
       " 'behaviors': 0.07416699081659317,\n",
       " 'this kind': 0.07434017211198807,\n",
       " 'none': 0.07598306238651276,\n",
       " 'the characteristics': 0.07617367058992386,\n",
       " 'the art algorithms': 0.0793011263012886,\n",
       " 'multiple tasks': 0.0809808149933815,\n",
       " 'complex': 0.08147479593753815,\n",
       " 'the sense': 0.08291938900947571,\n",
       " 'learners': 0.08496861904859543,\n",
       " 'characteristics': 0.08592437952756882,\n",
       " 'differences': 0.08816730976104736,\n",
       " 'the existing methods': 0.08977905660867691,\n",
       " 'characters': 0.09085258841514587,\n",
       " 'statements': 0.09139645844697952,\n",
       " 'the best results': 0.09353476762771606,\n",
       " 'this task': 0.09360478818416595,\n",
       " 'some': 0.09466405212879181,\n",
       " 'language models': 0.09548979252576828,\n",
       " 'the source': 0.09592704474925995,\n",
       " 'nonlinearities': 0.09814417362213135,\n",
       " 'advantages': 0.09967632591724396,\n",
       " 'the constraints': 0.102772057056427,\n",
       " 'discriminative features': 0.10284746438264847,\n",
       " 'classification tasks': 0.10291416943073273,\n",
       " 'a fixed': 0.10747003555297852,\n",
       " 'pieces': 0.10859271138906479,\n",
       " 'great success': 0.11015458405017853,\n",
       " 'pipelines': 0.11047372967004776,\n",
       " 'different kinds': 0.11076419055461884,\n",
       " 'the help': 0.11099462956190109,\n",
       " 'vectors': 0.11179705709218979,\n",
       " 'the other': 0.11231257021427155,\n",
       " 'atari games': 0.11322847753763199,\n",
       " 'evaluation metrics': 0.11324311792850494,\n",
       " 'a probabilistic model': 0.1141703650355339,\n",
       " 'the ones': 0.11709145456552505,\n",
       " 'computations': 0.11759475618600845,\n",
       " 'a significant amount': 0.11856088787317276,\n",
       " 'a review': 0.12122341245412827,\n",
       " 'small amounts': 0.12207435816526413,\n",
       " 'symmetries': 0.12497294694185257,\n",
       " 'the dictionary': 0.12562541663646698,\n",
       " 'image datasets': 0.12964695692062378,\n",
       " 'the regions': 0.1323784738779068,\n",
       " 'phrases': 0.13264045119285583,\n",
       " 'reward': 0.13285812735557556,\n",
       " 'each round': 0.13430911302566528,\n",
       " 'the effects': 0.1356644481420517,\n",
       " 'the latter': 0.13643617928028107,\n",
       " 'labor': 0.1365031749010086,\n",
       " 'complex models': 0.13688109815120697,\n",
       " 'the decision making process': 0.13730229437351227,\n",
       " 'the experts': 0.13749176263809204,\n",
       " 'signals': 0.13753418624401093,\n",
       " 'the normalization': 0.13753782212734222,\n",
       " 'network architectures': 0.13857406377792358,\n",
       " 'the geometry': 0.13894115388393402,\n",
       " 'times': 0.14203251898288727,\n",
       " 'the benefits': 0.14230729639530182,\n",
       " 'the dependencies': 0.14694799482822418,\n",
       " 'nature': 0.14733785390853882,\n",
       " 'the topics': 0.1473521739244461,\n",
       " 'stochastic gradients': 0.15033701062202454,\n",
       " 'successful applications': 0.15062282979488373,\n",
       " 'the correspondence': 0.15129584074020386,\n",
       " 'other domains': 0.1521773636341095,\n",
       " 'temporal information': 0.1522144377231598,\n",
       " 'past observations': 0.15237505733966827,\n",
       " 'parts': 0.15302923321723938,\n",
       " 'the advantage': 0.1530327945947647,\n",
       " 'the domains': 0.1535857617855072,\n",
       " 'training images': 0.15430378913879395,\n",
       " 'two methods': 0.15512728691101074,\n",
       " 'training data': 0.15626542270183563,\n",
       " 'deep learning architectures': 0.15690404176712036,\n",
       " 'detection algorithms': 0.15731194615364075,\n",
       " 'losses': 0.15858608484268188,\n",
       " 'contextual information': 0.15905697643756866,\n",
       " 'the eigenvalues': 0.16012759506702423,\n",
       " 'a small amount': 0.16037578880786896,\n",
       " 'billions': 0.16056133806705475,\n",
       " 'individual words': 0.16189861297607422,\n",
       " 'the heart': 0.1631135642528534,\n",
       " 'future observations': 0.16346831619739532,\n",
       " 'kinds': 0.16400615870952606,\n",
       " 'fingerprints': 0.16447234153747559,\n",
       " 'facts': 0.16536752879619598,\n",
       " 'regularization methods': 0.16773325204849243,\n",
       " 'lateral connections': 0.17297104001045227,\n",
       " '2d images': 0.1748502254486084,\n",
       " 'the interface': 0.17664828896522522,\n",
       " 'the heterogeneity': 0.17708347737789154,\n",
       " 'these problems': 0.17751742899417877,\n",
       " 'such domains': 0.1778194010257721,\n",
       " 'graph structures': 0.1795431226491928,\n",
       " 'an effective way': 0.17991596460342407,\n",
       " 'the methodology': 0.18032394349575043,\n",
       " 'variational approximations': 0.18209660053253174,\n",
       " 'each neuron': 0.18262174725532532,\n",
       " 'the disease': 0.18292418122291565,\n",
       " 'two properties': 0.18303139507770538,\n",
       " 'synthetic images': 0.18508243560791016,\n",
       " 'the statistics': 0.18548744916915894,\n",
       " 'hundreds': 0.18611209094524384,\n",
       " 'the prior distribution': 0.18640542030334473,\n",
       " 'large numbers': 0.1865498572587967,\n",
       " 'computer vision applications': 0.18710674345493317,\n",
       " 'generalization ability': 0.1871335357427597,\n",
       " 'a decade': 0.18821094930171967,\n",
       " 'a means': 0.18857698142528534,\n",
       " 'possible classes': 0.19039249420166016,\n",
       " 'the functions': 0.19335302710533142,\n",
       " 'the flexibility': 0.1941567361354828,\n",
       " 'best': 0.19445128738880157,\n",
       " 'the learning process': 0.19474124908447266,\n",
       " 'both training and testing': 0.19494476914405823,\n",
       " 'visual features': 0.19559741020202637,\n",
       " 'error rates': 0.19593115150928497,\n",
       " 'cognitive processes': 0.19709688425064087,\n",
       " 'teams': 0.19730260968208313,\n",
       " 'several domains': 0.19753439724445343,\n",
       " 'kernel methods': 0.19755825400352478,\n",
       " 'small perturbations': 0.19778135418891907,\n",
       " 'the training process': 0.19919970631599426,\n",
       " 'a new state': 0.2000643014907837,\n",
       " 'good accuracy': 0.20063084363937378,\n",
       " 'small sets': 0.20071139931678772,\n",
       " 'the groups': 0.20156757533550262,\n",
       " 'attention maps': 0.20187914371490479,\n",
       " 'bits': 0.2018951177597046,\n",
       " 'a variety of applications': 0.20204995572566986,\n",
       " 'initializations': 0.20215298235416412,\n",
       " 'a novel method': 0.20248940587043762,\n",
       " 'the input data': 0.203881174325943,\n",
       " 'the convolutional layers': 0.2045365422964096,\n",
       " 'efficiency': 0.2050189971923828,\n",
       " 'a single model': 0.20506009459495544,\n",
       " 'a given task': 0.2064620554447174,\n",
       " 'criticality': 0.20779237151145935,\n",
       " 'a generic framework': 0.20813658833503723,\n",
       " 'this knowledge': 0.20816288888454437,\n",
       " 'the model space': 0.2089224010705948,\n",
       " 'marginals': 0.20928068459033966,\n",
       " 'the variability': 0.20933909714221954,\n",
       " 'function evaluations': 0.20934821665287018,\n",
       " 'the reward function': 0.20939478278160095,\n",
       " 'the regime': 0.2096729278564453,\n",
       " 'a neural network': 0.2100609987974167,\n",
       " 'a vector space': 0.211112380027771,\n",
       " 'many scenarios': 0.21152931451797485,\n",
       " 'an investigation': 0.2121005654335022,\n",
       " 'the individual level': 0.21243923902511597,\n",
       " 'an important tool': 0.21258315443992615,\n",
       " 'negative examples': 0.21287265419960022,\n",
       " 'two models': 0.21303172409534454,\n",
       " 'this distribution': 0.21343091130256653,\n",
       " 'the learning': 0.21356694400310516,\n",
       " 'the art neural networks': 0.21361832320690155,\n",
       " 'invariant representations': 0.21374471485614777,\n",
       " 'sensor modalities': 0.21383342146873474,\n",
       " 'weakly supervised classification': 0.2140432447195053,\n",
       " 'the self': 0.21449382603168488,\n",
       " 'the feature space': 0.21480940282344818,\n",
       " 'different characteristics': 0.21503010392189026,\n",
       " 'two ways': 0.2153329998254776,\n",
       " 'incomplete data': 0.21593308448791504,\n",
       " 'the sequence': 0.21593932807445526,\n",
       " 'posterior probabilities': 0.21636389195919037,\n",
       " 'high accuracy': 0.21640309691429138,\n",
       " 'four types': 0.21752075850963593,\n",
       " 'critical applications': 0.21890152990818024,\n",
       " 'breakthroughs': 0.21895043551921844,\n",
       " 'brain regions': 0.21971914172172546,\n",
       " 'a fixed number': 0.2207333743572235,\n",
       " 'a generative adversarial network': 0.22073787450790405,\n",
       " 'its robustness': 0.22082531452178955,\n",
       " 'prototypes': 0.22084647417068481,\n",
       " 'the expressiveness': 0.22218109667301178,\n",
       " 'minecraft': 0.22263053059577942,\n",
       " 'fpgas': 0.22312533855438232,\n",
       " 'mutual information': 0.22354906797409058,\n",
       " 'surfaces': 0.22416536509990692,\n",
       " 'generative networks': 0.22444796562194824,\n",
       " 'mixed data': 0.2249879390001297,\n",
       " 'a causal graph': 0.22600622475147247,\n",
       " 'a new framework': 0.22632500529289246,\n",
       " 'transition probabilities': 0.22691521048545837,\n",
       " 'each point': 0.2272048443555832,\n",
       " 'matching problems': 0.227212056517601,\n",
       " 'complementary information': 0.22724856436252594,\n",
       " 'distribution algorithms': 0.2276817262172699,\n",
       " 'edge devices': 0.2277524173259735,\n",
       " 'random noise': 0.22810113430023193,\n",
       " 'an abundance': 0.22818365693092346,\n",
       " 'binary classifiers': 0.22930645942687988,\n",
       " 'inverse problems': 0.22935979068279266,\n",
       " 'the unsupervised learning': 0.23023609817028046,\n",
       " 'the recent past': 0.2309904843568802,\n",
       " 'a reinforcement learning agent': 0.23182182013988495,\n",
       " 'complex concepts': 0.2319948971271515,\n",
       " 'internal states': 0.23211686313152313,\n",
       " 'researches': 0.23212465643882751,\n",
       " 'the art deep neural networks': 0.232158362865448,\n",
       " 'human supervision': 0.23276495933532715,\n",
       " 'publications': 0.23400773108005524,\n",
       " 'alignments': 0.23405537009239197,\n",
       " 'the organization': 0.2342349886894226,\n",
       " 'time bptt': 0.23483498394489288,\n",
       " 'density estimation': 0.23513256013393402,\n",
       " 'batches': 0.23555269837379456,\n",
       " 'the existing method': 0.23602114617824554,\n",
       " 'morphologically rich languages': 0.23640574514865875,\n",
       " 'lms': 0.23655109107494354,\n",
       " 'hierarchical priors': 0.2366141676902771,\n",
       " 'a low dimensional space': 0.2376209795475006,\n",
       " 'their users': 0.23765237629413605,\n",
       " 'depth estimation': 0.23804233968257904,\n",
       " 'new algorithms': 0.23806335031986237,\n",
       " 'one language': 0.23883254826068878,\n",
       " 'gradient information': 0.23896652460098267,\n",
       " 'machine learning problems': 0.23910407721996307,\n",
       " 'convex relaxations': 0.23937468230724335,\n",
       " 'selection algorithms': 0.24033735692501068,\n",
       " 'the prediction model': 0.24043549597263336,\n",
       " 'the clustering process': 0.24061475694179535,\n",
       " 'broad applications': 0.2407054454088211,\n",
       " 'this respect': 0.24079982936382294,\n",
       " 'industrial applications': 0.24108342826366425,\n",
       " 'the parts': 0.24120299518108368,\n",
       " 'computer vision algorithms': 0.24143964052200317,\n",
       " 'common methods': 0.2417483627796173,\n",
       " 'calculation': 0.24222683906555176,\n",
       " 'new users': 0.24276447296142578,\n",
       " 'a number of methods': 0.24280045926570892,\n",
       " 'a new approach': 0.24280640482902527,\n",
       " 'full use': 0.24329790472984314,\n",
       " 'application areas': 0.24372826516628265,\n",
       " 'human actions': 0.2442099004983902,\n",
       " 'subtasks': 0.24502365291118622,\n",
       " 'the trajectories': 0.24515190720558167,\n",
       " 'the temporal structure': 0.24548809230327606,\n",
       " 'imitation': 0.24567818641662598,\n",
       " 'references': 0.24593956768512726,\n",
       " 'computing resources': 0.24662157893180847,\n",
       " 'limited applicability': 0.24697411060333252,\n",
       " 'an unsupervised fashion': 0.24697667360305786,\n",
       " 'random forests': 0.24702420830726624,\n",
       " 'new concepts': 0.24710017442703247,\n",
       " 'novel applications': 0.24713563919067383,\n",
       " 'camera images': 0.2476550042629242,\n",
       " 'a joint distribution': 0.24841471016407013,\n",
       " 'general intelligence': 0.24944698810577393,\n",
       " 'background knowledge': 0.25009727478027344,\n",
       " 'the learning algorithm': 0.25015559792518616,\n",
       " 'translation quality': 0.2507593333721161,\n",
       " 'gssl': 0.25125953555107117,\n",
       " 'relational similarity': 0.25196751952171326,\n",
       " 'a classification task': 0.2531921863555908,\n",
       " 'background subtraction': 0.253263920545578,\n",
       " 'several ways': 0.2535366117954254,\n",
       " 'neural network parameters': 0.2536376416683197,\n",
       " 'their decisions': 0.2541518211364746,\n",
       " 'deep nets': 0.2546672224998474,\n",
       " 'different representations': 0.2549353241920471,\n",
       " 'the similarity': 0.25528714060783386,\n",
       " 'a machine learning approach': 0.2554776072502136,\n",
       " 'regions': 0.2555321753025055,\n",
       " 'a source domain': 0.25564801692962646,\n",
       " 'the practicality': 0.2557045817375183,\n",
       " 'the outliers': 0.2562616765499115,\n",
       " 'paramount importance': 0.25633636116981506,\n",
       " 'length mdl principle': 0.2566719651222229,\n",
       " 'standard classification': 0.25692716240882874,\n",
       " 'exploration algorithms': 0.2581382989883423,\n",
       " 'such classifiers': 0.25840136408805847,\n",
       " 'these tools': 0.25844335556030273,\n",
       " 'the classification problem': 0.2602144479751587,\n",
       " 'a theoretical analysis': 0.26028314232826233,\n",
       " 'feature descriptors': 0.26062139868736267,\n",
       " 'the dae': 0.2607589364051819,\n",
       " 'their parameters': 0.2612389028072357,\n",
       " 'a new representation': 0.26135700941085815,\n",
       " 'classification rules': 0.26139283180236816,\n",
       " 'the grammar': 0.26162222027778625,\n",
       " 'a given set': 0.26170846819877625,\n",
       " 'mini batches': 0.26246073842048645,\n",
       " 'similarity information': 0.26344993710517883,\n",
       " 'the computational time': 0.2636103630065918,\n",
       " 'a lot of attention': 0.26395002007484436,\n",
       " 'large models': 0.2645128667354584,\n",
       " 'autonomous agents': 0.2645176351070404,\n",
       " 'its size': 0.2647978365421295,\n",
       " 'object reconstruction': 0.26500171422958374,\n",
       " 'the temporal dynamics': 0.26541873812675476,\n",
       " 'improved performance': 0.26550164818763733,\n",
       " 'agreement': 0.26584580540657043,\n",
       " 'electronic medical records': 0.2659875750541687,\n",
       " 'research fields': 0.26613926887512207,\n",
       " 'the means': 0.26660534739494324,\n",
       " 'a general methodology': 0.2668295204639435,\n",
       " 'outstanding performance': 0.2670995593070984,\n",
       " 'this study': 0.2676764726638794,\n",
       " 'research papers': 0.2678045928478241,\n",
       " 'a gaussian process gp': 0.26842084527015686,\n",
       " 'annotators': 0.269400030374527,\n",
       " 'the latency': 0.26978200674057007,\n",
       " 'the optimal policy': 0.2704162895679474,\n",
       " 'extrema': 0.2707996964454651,\n",
       " 'small changes': 0.2709522247314453,\n",
       " 'interpretable machine': 0.2710513174533844,\n",
       " 'reconstruction methods': 0.27192720770835876,\n",
       " 'a given data set': 0.2721961438655853,\n",
       " 'a natural generalization': 0.2733403742313385,\n",
       " 'optimal behavior': 0.27342134714126587,\n",
       " 'convergence properties': 0.2735695242881775,\n",
       " 'significant attention': 0.2742113173007965,\n",
       " 'each feature': 0.27477529644966125,\n",
       " 'high dimensional spaces': 0.27555567026138306,\n",
       " 'the video': 0.2760685980319977,\n",
       " 'a finite number': 0.27608761191368103,\n",
       " 'the progression': 0.2768251597881317,\n",
       " 'clinical decision support': 0.2769562900066376,\n",
       " 'a multitude': 0.2772037982940674,\n",
       " 'supervised learning tasks': 0.2780887186527252,\n",
       " 'the entire image': 0.2790457308292389,\n",
       " 'a correspondence': 0.27918246388435364,\n",
       " 'sparse approximation': 0.2792961001396179,\n",
       " 'a common approach': 0.2796079218387604,\n",
       " 'pairs of examples': 0.27963966131210327,\n",
       " 'computed tomography ct': 0.2800334095954895,\n",
       " 'entities and relations': 0.2813017666339874,\n",
       " 'a modification': 0.28169891238212585,\n",
       " 'speech enhancement': 0.2830156683921814,\n",
       " 'zeros': 0.28303271532058716,\n",
       " 'a set of parameters': 0.28332918882369995,\n",
       " 'clustering techniques': 0.2835499942302704,\n",
       " 'optimal policies': 0.2840985655784607,\n",
       " 'growth': 0.28428539633750916,\n",
       " 'two representations': 0.2845732867717743,\n",
       " 'the extent': 0.2847042381763458,\n",
       " 'stationarity': 0.285166472196579,\n",
       " 'advisors': 0.2854349613189697,\n",
       " 'unique challenges': 0.288185179233551,\n",
       " 'a major advantage': 0.28945109248161316,\n",
       " 'single images': 0.2900259792804718,\n",
       " 'hyperplanes': 0.29040247201919556,\n",
       " 'misclassifications': 0.29100707173347473,\n",
       " 'factors': 0.29139378666877747,\n",
       " 'dempster shafer theory': 0.29171258211135864,\n",
       " 'an efficient solution': 0.292518675327301,\n",
       " 'bins': 0.2925986051559448,\n",
       " 'regression tasks': 0.29263195395469666,\n",
       " 'this short paper': 0.2928347587585449,\n",
       " 'a dynamical system': 0.2935970425605774,\n",
       " 'natural language descriptions': 0.2938879132270813,\n",
       " 'rich representations': 0.29397761821746826,\n",
       " 'two distributions': 0.2952228784561157,\n",
       " 'embedding methods': 0.2968631982803345,\n",
       " 'subpopulations': 0.29703956842422485,\n",
       " 'relations': 0.29715967178344727,\n",
       " 'recent times': 0.2972021698951721,\n",
       " 'the expressive power': 0.29750993847846985,\n",
       " 'optimal actions': 0.297994464635849,\n",
       " 'eeg signals': 0.2985537052154541,\n",
       " 'exact inference': 0.299701452255249,\n",
       " 'intrinsic motivation': 0.2997341752052307,\n",
       " 'multi armed bandits': 0.2999027371406555,\n",
       " 'stochastic algorithms': 0.2999404966831207,\n",
       " 'large state spaces': 0.2999992370605469,\n",
       " 'similarity measures': 0.3002549111843109,\n",
       " 'training strategies': 0.3019288182258606,\n",
       " 'voice': 0.3021984398365021,\n",
       " 'the applicability': 0.30223503708839417,\n",
       " 'regret bounds': 0.3026672899723053,\n",
       " '39': 0.3031125068664551,\n",
       " 'format': 0.3031187653541565,\n",
       " 'content recommendation': 0.30329230427742004,\n",
       " 'the network architecture': 0.30358338356018066,\n",
       " 'sparse signal recovery': 0.30396875739097595,\n",
       " 'hilbert spaces': 0.3043729364871979,\n",
       " 'this difficulty': 0.30479565262794495,\n",
       " 'an active area of research': 0.30537599325180054,\n",
       " 'dynamic systems': 0.3055484890937805,\n",
       " 'the big data': 0.30573293566703796,\n",
       " 'a single gpu': 0.3058713376522064,\n",
       " 'neural turing machines': 0.30592429637908936,\n",
       " 'similar performance': 0.3060375452041626,\n",
       " 'real world data': 0.30648013949394226,\n",
       " 'the large amount': 0.307087242603302,\n",
       " 'weight vectors': 0.3070891499519348,\n",
       " 'non experts': 0.3074475824832916,\n",
       " 'every iteration': 0.3077464699745178,\n",
       " 'metric learning': 0.30860185623168945,\n",
       " 'a technique': 0.3090355098247528,\n",
       " 'its inputs': 0.31084102392196655,\n",
       " 'sparse filtering': 0.3116631507873535,\n",
       " 'motion capture data': 0.3123096227645874,\n",
       " 'residuals': 0.31259575486183167,\n",
       " 'such processes': 0.31260642409324646,\n",
       " 'a learning agent': 0.31263333559036255,\n",
       " 'actionable': 0.31285974383354187,\n",
       " 'constant memory': 0.31365737318992615,\n",
       " 'their high computational cost': 0.31367355585098267,\n",
       " 'mild conditions': 0.3139505386352539,\n",
       " 'dirichlet allocation lda': 0.31447553634643555,\n",
       " 'a simple': 0.3161100447177887,\n",
       " 'bugs': 0.3169795274734497,\n",
       " 'first order methods': 0.3175148069858551,\n",
       " 'noisier': 0.3176476061344147,\n",
       " 'compositional': 0.31806087493896484,\n",
       " 'new ones': 0.3183123767375946,\n",
       " 'ambiguity': 0.31924012303352356,\n",
       " 'data space': 0.3192797005176544,\n",
       " 'a single modality': 0.3195938467979431,\n",
       " 'new domains': 0.31987276673316956,\n",
       " 'performance and energy efficiency': 0.31997957825660706,\n",
       " 'an active learning algorithm': 0.3200969099998474,\n",
       " 'latent': 0.32018476724624634,\n",
       " 'the local geometry': 0.32052791118621826,\n",
       " 'this point': 0.3207325041294098,\n",
       " 'important features': 0.3211033344268799,\n",
       " 'human visual attention': 0.32124650478363037,\n",
       " 'regression methods': 0.3219897150993347,\n",
       " 'the cnn': 0.3220789432525635,\n",
       " 'a critical role': 0.32210028171539307,\n",
       " 'key aspects': 0.322357177734375,\n",
       " 'test accuracy': 0.32331743836402893,\n",
       " 'diabetic retinopathy': 0.3235136866569519,\n",
       " 'annotated training data': 0.32496845722198486,\n",
       " 'the subset': 0.32536646723747253,\n",
       " 'a time': 0.3253999948501587,\n",
       " 'a max margin': 0.32544460892677307,\n",
       " 'temporal patterns': 0.3275623619556427,\n",
       " 'a new data set': 0.3281104564666748,\n",
       " 'the key challenges': 0.32814517617225647,\n",
       " 'electronic health records ehr': 0.32861852645874023,\n",
       " 'the use of deep neural networks': 0.32861897349357605,\n",
       " 'the role': 0.32890328764915466,\n",
       " 'programming asp': 0.3290572166442871,\n",
       " 'depths': 0.32933753728866577,\n",
       " 'conditional rbms': 0.32954156398773193,\n",
       " 'natural language questions': 0.32995760440826416,\n",
       " 'a supervised machine': 0.3306887149810791,\n",
       " 'different resolutions': 0.333575963973999,\n",
       " 'statistical tests': 0.3340667188167572,\n",
       " 'swmh': 0.3341691493988037,\n",
       " 'hidden causes': 0.3345484435558319,\n",
       " 'causal directions': 0.3345661163330078,\n",
       " 'linguistics': 0.33466798067092896,\n",
       " 'a practical algorithm': 0.3353022038936615,\n",
       " 'neural representations': 0.3354201912879944,\n",
       " 'the roles': 0.33571046590805054,\n",
       " 'the performance of the system': 0.33618873357772827,\n",
       " 'pillars': 0.3363114297389984,\n",
       " 'learning parameters': 0.3373793959617615,\n",
       " 'large matrices': 0.3376489281654358,\n",
       " 'medical diagnosis': 0.3376593291759491,\n",
       " 'new questions': 0.33790352940559387,\n",
       " 'the hyperparameters': 0.3385831117630005,\n",
       " 'the number of training examples': 0.33901262283325195,\n",
       " 'different strategies': 0.3391894996166229,\n",
       " 'the policy': 0.3395956754684448,\n",
       " 'side effects': 0.33970117568969727,\n",
       " 'the high level': 0.3398333787918091,\n",
       " 'a mixture model': 0.3407284617424011,\n",
       " 'the construction': 0.3408200442790985,\n",
       " 'various settings': 0.34109964966773987,\n",
       " 'the number of linear regions': 0.34132927656173706,\n",
       " 'a critical bottleneck': 0.3425605595111847,\n",
       " 'the study': 0.3437640070915222,\n",
       " 'its accuracy': 0.3442079722881317,\n",
       " 'the main goal': 0.34434348344802856,\n",
       " '2048': 0.34524229168891907,\n",
       " 'weight parameters': 0.3452918529510498,\n",
       " 'a novel formulation': 0.34532374143600464,\n",
       " 'multiple labels': 0.3459455966949463,\n",
       " 'lexicons': 0.34600457549095154,\n",
       " 'imbalanced problems': 0.3460828959941864,\n",
       " 'a virtual environment': 0.3462331295013428,\n",
       " 'pattern classification': 0.34624552726745605,\n",
       " 'a scoring': 0.34810853004455566,\n",
       " 'prism': 0.3484257161617279,\n",
       " 'panel': 0.3485395908355713,\n",
       " 'over parametrization': 0.3491119146347046,\n",
       " 'shortcomings': 0.3491385281085968,\n",
       " 'clustering and classification': 0.3497557044029236,\n",
       " 'one variable': 0.35028019547462463,\n",
       " 'the tracking': 0.35095641016960144,\n",
       " 'the severity': 0.35104283690452576,\n",
       " 'the state': 0.3514159619808197,\n",
       " 'sparse rewards': 0.3518911302089691,\n",
       " 'sample efficiency': 0.3530345559120178,\n",
       " 'modern machine': 0.35381144285202026,\n",
       " 'object detectors': 0.3538735508918762,\n",
       " 'subgraphs': 0.3540691137313843,\n",
       " 'the latent space': 0.35446155071258545,\n",
       " 'the sharing': 0.35501012206077576,\n",
       " 'the web': 0.35639485716819763,\n",
       " 'multi label classification': 0.3564069867134094,\n",
       " 'strokes': 0.35709068179130554,\n",
       " 'diagnosis': 0.35714006423950195,\n",
       " 'many applications': 0.3586258292198181,\n",
       " 'natural languages': 0.3597545325756073,\n",
       " 'sea': 0.3597564995288849,\n",
       " 'the reduction': 0.36131522059440613,\n",
       " 'articles': 0.36156919598579407,\n",
       " 'astrophysics': 0.36254286766052246,\n",
       " 'the spirit': 0.36265280842781067,\n",
       " 'ample': 0.36342501640319824,\n",
       " 'unsupervised domain': 0.36388859152793884,\n",
       " 'severity': 0.3655736446380615,\n",
       " 'a promising alternative': 0.36722928285598755,\n",
       " 'maximum likelihood estimation': 0.3674502372741699,\n",
       " 'big data analytics': 0.36884281039237976,\n",
       " 'any learning algorithm': 0.36917856335639954,\n",
       " 'mixture models': 0.3694847822189331,\n",
       " 'the dialogue': 0.3696005642414093,\n",
       " 'the ambient': 0.37026622891426086,\n",
       " 'clustering algorithms': 0.37042561173439026,\n",
       " 'training and evaluation': 0.3705510199069977,\n",
       " 'rnn': 0.370755136013031,\n",
       " 'the discovery': 0.37117651104927063,\n",
       " 'the coefficients': 0.3714688718318939,\n",
       " 'information fusion': 0.3719811737537384,\n",
       " 'the implementation': 0.3726447820663452,\n",
       " 'data instances': 0.372789204120636,\n",
       " 'malaria': 0.37284064292907715,\n",
       " 'a distributed representation': 0.37349748611450195,\n",
       " 'rf data': 0.37379297614097595,\n",
       " 'an evolutionary algorithm': 0.3738844692707062,\n",
       " 'engineered features': 0.374420702457428,\n",
       " 'a loss': 0.3745158314704895,\n",
       " 'support vector regression': 0.3759663701057434,\n",
       " 'video prediction': 0.3765367269515991,\n",
       " 'a variety of tasks': 0.3779112994670868,\n",
       " 'the cumulative regret': 0.37820473313331604,\n",
       " 'cmr': 0.3783843517303467,\n",
       " 'parameter estimates': 0.3798479735851288,\n",
       " 'interventions': 0.37986278533935547,\n",
       " 'sensory data': 0.380297988653183,\n",
       " 'recent developments': 0.3816344738006592,\n",
       " 'a component': 0.3827560245990753,\n",
       " 'their values': 0.38279828429222107,\n",
       " 'two new algorithms': 0.3830842971801758,\n",
       " 'the building blocks': 0.383088618516922,\n",
       " 'paq8': 0.3833678364753723,\n",
       " 'one approach': 0.3843894600868225,\n",
       " 'the sources': 0.38465559482574463,\n",
       " 'latent vectors': 0.38524672389030457,\n",
       " 'neural network rnn': 0.38547199964523315,\n",
       " 'one layer': 0.3860130310058594,\n",
       " 'sims': 0.38619673252105713,\n",
       " 'regularization strategies': 0.38693711161613464,\n",
       " 'the thesis': 0.3872321844100952,\n",
       " 'the relational structure': 0.38766300678253174,\n",
       " 'speech data': 0.3881074786186218,\n",
       " 'an important application': 0.3884759843349457,\n",
       " 'motion segmentation': 0.3890412449836731,\n",
       " 'the precision matrix': 0.38933321833610535,\n",
       " 'additive models': 0.3899644613265991,\n",
       " 'the gmm': 0.39108341932296753,\n",
       " 'two aspects': 0.39200282096862793,\n",
       " 'a significant speedup': 0.3923538327217102,\n",
       " 'the decision': 0.39350104331970215,\n",
       " 'the appearance': 0.39542099833488464,\n",
       " 'similarity learning': 0.39542827010154724,\n",
       " 'a real': 0.3965720534324646,\n",
       " 'the practice': 0.3991450369358063,\n",
       " 'the inherent structure': 0.3996276557445526,\n",
       " 'latent structure': 0.40023574233055115,\n",
       " 'singular values': 0.400713711977005,\n",
       " 'low quality': 0.40211793780326843,\n",
       " 'vanishing and exploding gradients': 0.40226414799690247,\n",
       " 'acoustics': 0.4044939875602722,\n",
       " 'long sequences': 0.40462231636047363,\n",
       " 'closed form expressions': 0.40479394793510437,\n",
       " 'a gaussian': 0.40504351258277893,\n",
       " 'a computational model': 0.40551629662513733,\n",
       " 'the forward and backward': 0.40557199716567993,\n",
       " 'training generative models': 0.4057019352912903,\n",
       " 'binary features': 0.4066496789455414,\n",
       " 'unfairness': 0.4069691598415375,\n",
       " 'trial and error': 0.4071381390094757,\n",
       " 'sequence prediction': 0.40732938051223755,\n",
       " 'data privacy': 0.40803781151771545,\n",
       " 'the situation': 0.4082324504852295,\n",
       " 'the presence of outliers': 0.40927931666374207,\n",
       " 'the training loss': 0.4096740186214447,\n",
       " 'eight': 0.4099498987197876,\n",
       " 'loopy belief propagation': 0.4100438058376312,\n",
       " 'stragglers': 0.411548376083374,\n",
       " 'many others': 0.41220822930336,\n",
       " 'different contexts': 0.41252392530441284,\n",
       " 'share information': 0.412741094827652,\n",
       " 'the memory': 0.4128996431827545,\n",
       " 'log linear models': 0.4134223759174347,\n",
       " 'texture images': 0.4151041805744171,\n",
       " 'a deep convolutional neural network cnn': 0.4151751399040222,\n",
       " 'document images': 0.4161764681339264,\n",
       " 'iterative algorithms': 0.4164523482322693,\n",
       " 'new methods': 0.4169977307319641,\n",
       " 'covariate shift': 0.4173376262187958,\n",
       " 'beam search': 0.41796138882637024,\n",
       " 'a confidence': 0.4179927408695221,\n",
       " 'dialog state tracking': 0.41991952061653137,\n",
       " 'a kind': 0.42057064175605774,\n",
       " 'unified approach': 0.42086708545684814,\n",
       " 'a sparsity constraint': 0.421241819858551,\n",
       " 'a good model': 0.421642929315567,\n",
       " 'proteins': 0.4219406843185425,\n",
       " 'a difficult task': 0.42211684584617615,\n",
       " 'a complex': 0.4221533238887787,\n",
       " 'the spatial': 0.42299267649650574,\n",
       " 'a critical step': 0.42383792996406555,\n",
       " 'cyclegan': 0.4254871606826782,\n",
       " 'the test data': 0.42552027106285095,\n",
       " 'two kernels': 0.42604300379753113,\n",
       " 'different fields': 0.42638126015663147,\n",
       " 'accounts': 0.4272533357143402,\n",
       " 'dictionary learning': 0.42754852771759033,\n",
       " 'narratives': 0.4277099668979645,\n",
       " 'loss minimization': 0.4285691976547241,\n",
       " 'fear': 0.42973941564559937,\n",
       " 'regularized maximum likelihood': 0.4308648705482483,\n",
       " 'links': 0.43131041526794434,\n",
       " 'human perception': 0.43179619312286377,\n",
       " 'multivariate data': 0.43287453055381775,\n",
       " 'learning rates': 0.4331551492214203,\n",
       " 'model uncertainty': 0.4352361857891083,\n",
       " 'similar problems': 0.4352499544620514,\n",
       " 'the present study': 0.435524046421051,\n",
       " 'the face': 0.4355645477771759,\n",
       " 'the second order statistics': 0.43669068813323975,\n",
       " 'super resolution sr': 0.4372614026069641,\n",
       " 'further': 0.4380204677581787,\n",
       " 'next frame prediction': 0.43846842646598816,\n",
       " 'an effective tool': 0.44124552607536316,\n",
       " 'diagnoses': 0.4417482614517212,\n",
       " 'text corpora': 0.44201424717903137,\n",
       " 'graphical models': 0.4437284767627716,\n",
       " 'galaxies': 0.44457918405532837,\n",
       " 'vae': 0.4446847140789032,\n",
       " 'china': 0.44657453894615173,\n",
       " 'a new variant': 0.4469855725765228,\n",
       " 'existing': 0.4472220838069916,\n",
       " 'a convolutional layer': 0.44755589962005615,\n",
       " 'discrete optimization': 0.4476865530014038,\n",
       " 'emotion recognition': 0.4521954357624054,\n",
       " 'disease': 0.4525233507156372,\n",
       " 'the weaknesses': 0.45275282859802246,\n",
       " 'human accuracy': 0.45335838198661804,\n",
       " 'stochastic convex optimization': 0.45387977361679077,\n",
       " 'clustering problems': 0.4561501145362854,\n",
       " 'opinion mining': 0.4579354524612427,\n",
       " 'local neighborhoods': 0.45817646384239197,\n",
       " 'multimodal learning': 0.45821529626846313,\n",
       " 'the roc curve': 0.45904868841171265,\n",
       " 'optimal regret': 0.4591119587421417,\n",
       " 'vast amounts': 0.45978760719299316,\n",
       " '4': 0.4599872827529907,\n",
       " 'votes': 0.46003249287605286,\n",
       " 'mapper': 0.46068328619003296,\n",
       " 'a modified version': 0.46134984493255615,\n",
       " 'structural properties': 0.4623408019542694,\n",
       " 'connection structure': 0.462883859872818,\n",
       " 'two dimensions': 0.4630909860134125,\n",
       " 'each hidden layer': 0.46333765983581543,\n",
       " 'the encoder': 0.4652608335018158,\n",
       " 'descriptors': 0.4664117693901062,\n",
       " 'yelp': 0.46728968620300293,\n",
       " 'economics': 0.4682789742946625,\n",
       " 'densenets': 0.46891745924949646,\n",
       " 'high dimensional data analysis': 0.46904999017715454,\n",
       " 'noise injection': 0.46991369128227234,\n",
       " 'youtube videos': 0.4702329635620117,\n",
       " 'all arms': 0.47026270627975464,\n",
       " 'non gaussian': 0.470267653465271,\n",
       " 'philosophy': 0.47045332193374634,\n",
       " 'the biases': 0.4705997407436371,\n",
       " 'drifts': 0.47346872091293335,\n",
       " 'a speaker': 0.47376200556755066,\n",
       " 'the player s': 0.4739052355289459,\n",
       " 'gatys': 0.47453728318214417,\n",
       " 'density estimators': 0.4745728075504303,\n",
       " 'density estimation tasks': 0.4755667746067047,\n",
       " 'anns': 0.4758099913597107,\n",
       " 'inference and learning': 0.47622984647750854,\n",
       " 'related applications': 0.47749975323677063,\n",
       " 'traffic congestion': 0.4781431257724762,\n",
       " 'deformation': 0.47869357466697693,\n",
       " 'the receptive field': 0.4803459644317627,\n",
       " 'visual tracking': 0.48121437430381775,\n",
       " 'structure learning': 0.48197969794273376,\n",
       " 'convex surrogates': 0.48270878195762634,\n",
       " 'awareness': 0.48335790634155273,\n",
       " 'large graphs': 0.48394981026649475,\n",
       " 'recent applications': 0.48453471064567566,\n",
       " 'cgans': 0.4850679636001587,\n",
       " 'a learning problem': 0.4855327606201172,\n",
       " 'invariance': 0.48592522740364075,\n",
       " 'mnih': 0.4863077402114868,\n",
       " 'ggamp': 0.4865283966064453,\n",
       " 'recommender systems': 0.4870162904262543,\n",
       " 'monolingual': 0.48889869451522827,\n",
       " 'posteriors': 0.48899099230766296,\n",
       " 'the images': 0.4896508455276489,\n",
       " 'modeling': 0.49037253856658936,\n",
       " 'point cloud': 0.4905284345149994,\n",
       " 'a range': 0.49125468730926514,\n",
       " 'principal components': 0.4921096861362457,\n",
       " 'glitches': 0.49414539337158203,\n",
       " 'feature importance': 0.4952481985092163,\n",
       " 'he': 0.49671345949172974,\n",
       " 'series prediction': 0.4984850287437439,\n",
       " 'inspection': 0.4985957443714142,\n",
       " 'a real world': 0.49896910786628723,\n",
       " 'the model size': 0.4996028244495392}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "handy-basketball",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(extracted, open(f'data/pem_extracted_vs_{VECTOR_SIZE}_ws_{WINDOW_SIZE}_nl_{NUM_LAYERS}_mf_{MIN_FREQUENCY}_e_{EPOCHS}.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pem",
   "language": "python",
   "name": "pem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
