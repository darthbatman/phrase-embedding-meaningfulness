{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dutch-struggle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Classifier implementation (model architecture, training, testing, etc.) derived from\n",
    "#     https://towardsdatascience.com/pytorch-tabular-binary-classification-a0368da5bb89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "portable-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import string\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "distant-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PHRASE_LEN = 6\n",
    "VECTOR_SIZE = 200\n",
    "WINDOW_SIZE = 10\n",
    "NUM_LAYERS = 5\n",
    "\n",
    "MIN_FREQUENCY = 5\n",
    "\n",
    "SHOULD_EXTRACT_NOUN_PHRASES = False\n",
    "SHOULD_GENERATE_UNDERSCORED_CORPUS = True\n",
    "SHOULD_TRAIN_WORD2VEC_MODEL = True\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "choice-manufacturer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 arxiv abstracts.\n"
     ]
    }
   ],
   "source": [
    "with open('data/arxiv_abstracts_10000.txt', 'r') as f:\n",
    "    arxiv_abstracts = f.read().split('\\n')[:-1]\n",
    "    arxiv_abstracts_raw = '\\n'.join(arxiv_abstracts)\n",
    "    f.close()\n",
    "print(f'Loaded {len(arxiv_abstracts)} arxiv abstracts.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "stone-membrane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 672 negative samples.\n"
     ]
    }
   ],
   "source": [
    "negative_samples = pickle.load(open('data/negative_samples.pkl', 'rb'))\n",
    "print(f'Loaded {len(negative_samples)} negative samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "composed-contrast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 672 positive samples.\n"
     ]
    }
   ],
   "source": [
    "positive_samples = pickle.load(open('data/positive_samples.pkl', 'rb'))\n",
    "print(f'Loaded {len(positive_samples)} positive samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "detailed-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_phrase(tree_str, label):\n",
    "    phrases = []\n",
    "    trees = Tree.fromstring(tree_str)\n",
    "    for tree in trees:\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == label:\n",
    "                t = subtree\n",
    "                t = ' '.join(t.leaves())\n",
    "                phrases.append(t)\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "conservative-chinese",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOULD_EXTRACT_NOUN_PHRASES:\n",
    "    nlp = StanfordCoreNLP('data/stanford-corenlp-4.1.0')\n",
    "    noun_phrases = []\n",
    "    for i, abstract in enumerate(arxiv_abstracts):\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f'Extracting noun phrases from abstract {i + 1} of {len(arxiv_abstracts)}')\n",
    "            pickle.dump(noun_phrases, open('data/noun_phrases.pkl', 'wb'))\n",
    "        try:\n",
    "            tree_str = nlp.parse(abstract)\n",
    "            noun_phrases.extend(extract_phrase(tree_str, 'NP'))\n",
    "        except Exception:\n",
    "            pass\n",
    "    noun_phrases = [np for np in list(set(noun_phrases)) if len(np.split()) <= MAX_PHRASE_LEN]\n",
    "    pickle.dump(noun_phrases, open('data/noun_phrases.pkl', 'wb'))\n",
    "noun_phrases = pickle.load(open('data/noun_phrases.pkl', 'rb'))\n",
    "noun_phrases = [np for np in list(set(noun_phrases)) if len(np.split()) <= MAX_PHRASE_LEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "blind-technical",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_phrase_in_corpus(corpus, phrase):\n",
    "    s_idx = corpus.find(phrase)\n",
    "    e_idx = s_idx + len(phrase)\n",
    "    if s_idx != -1 and \\\n",
    "       (s_idx == 0 or corpus[s_idx - 1] in (string.punctuation + ' ')) and \\\n",
    "       (e_idx == len(corpus) or corpus[e_idx] in (string.punctuation + ' ')):\n",
    "        return (s_idx, e_idx)\n",
    "    return (-1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "stunning-canberra",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOULD_GENERATE_UNDERSCORED_CORPUS:\n",
    "    corpus = arxiv_abstracts_raw[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "referenced-sudan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing positive_sample 100 of 672\n",
      "Replacing positive_sample 200 of 672\n",
      "Replacing positive_sample 300 of 672\n",
      "Replacing positive_sample 400 of 672\n",
      "Replacing positive_sample 500 of 672\n",
      "Replacing positive_sample 600 of 672\n"
     ]
    }
   ],
   "source": [
    "if SHOULD_GENERATE_UNDERSCORED_CORPUS:\n",
    "    for i, positive_sample in enumerate(positive_samples):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Replacing positive_sample {i + 1} of {len(positive_samples)}')\n",
    "        found_indices = set()\n",
    "        while find_phrase_in_corpus(corpus, positive_sample) != (-1, -1) and find_phrase_in_corpus(corpus, positive_sample)[0] not in found_indices:\n",
    "            s_idx, e_idx = find_phrase_in_corpus(corpus, positive_sample)\n",
    "            found_indices.add(s_idx)\n",
    "            corpus = corpus[:s_idx] + positive_sample.replace(' ', '_') + corpus[e_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dominant-imaging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing negative_sample 100 of 672\n",
      "Replacing negative_sample 200 of 672\n",
      "Replacing negative_sample 300 of 672\n",
      "Replacing negative_sample 400 of 672\n",
      "Replacing negative_sample 500 of 672\n",
      "Replacing negative_sample 600 of 672\n"
     ]
    }
   ],
   "source": [
    "if SHOULD_GENERATE_UNDERSCORED_CORPUS:\n",
    "    for i, negative_sample in enumerate(negative_samples):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Replacing negative_sample {i + 1} of {len(negative_samples)}')\n",
    "        found_indices = set()\n",
    "        while find_phrase_in_corpus(corpus, negative_sample) != (-1, -1) and find_phrase_in_corpus(corpus, negative_sample)[0] not in found_indices:\n",
    "            s_idx, e_idx = find_phrase_in_corpus(corpus, negative_sample)\n",
    "            found_indices.add(s_idx)\n",
    "            corpus = corpus[:s_idx] + negative_sample.replace(' ', '_') + corpus[e_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "collectible-witch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing noun_phrase 100 of 38952\n",
      "Replacing noun_phrase 200 of 38952\n",
      "Replacing noun_phrase 300 of 38952\n",
      "Replacing noun_phrase 400 of 38952\n",
      "Replacing noun_phrase 500 of 38952\n",
      "Replacing noun_phrase 600 of 38952\n",
      "Replacing noun_phrase 700 of 38952\n",
      "Replacing noun_phrase 800 of 38952\n",
      "Replacing noun_phrase 900 of 38952\n",
      "Replacing noun_phrase 1000 of 38952\n",
      "Replacing noun_phrase 1100 of 38952\n",
      "Replacing noun_phrase 1200 of 38952\n",
      "Replacing noun_phrase 1300 of 38952\n",
      "Replacing noun_phrase 1400 of 38952\n",
      "Replacing noun_phrase 1500 of 38952\n",
      "Replacing noun_phrase 1600 of 38952\n",
      "Replacing noun_phrase 1700 of 38952\n",
      "Replacing noun_phrase 1800 of 38952\n",
      "Replacing noun_phrase 1900 of 38952\n",
      "Replacing noun_phrase 2000 of 38952\n",
      "Replacing noun_phrase 2100 of 38952\n",
      "Replacing noun_phrase 2200 of 38952\n",
      "Replacing noun_phrase 2300 of 38952\n",
      "Replacing noun_phrase 2400 of 38952\n",
      "Replacing noun_phrase 2500 of 38952\n",
      "Replacing noun_phrase 2600 of 38952\n",
      "Replacing noun_phrase 2700 of 38952\n",
      "Replacing noun_phrase 2800 of 38952\n",
      "Replacing noun_phrase 2900 of 38952\n",
      "Replacing noun_phrase 3000 of 38952\n",
      "Replacing noun_phrase 3100 of 38952\n",
      "Replacing noun_phrase 3200 of 38952\n",
      "Replacing noun_phrase 3300 of 38952\n",
      "Replacing noun_phrase 3400 of 38952\n",
      "Replacing noun_phrase 3500 of 38952\n",
      "Replacing noun_phrase 3600 of 38952\n",
      "Replacing noun_phrase 3700 of 38952\n",
      "Replacing noun_phrase 3800 of 38952\n",
      "Replacing noun_phrase 3900 of 38952\n",
      "Replacing noun_phrase 4000 of 38952\n",
      "Replacing noun_phrase 4100 of 38952\n",
      "Replacing noun_phrase 4200 of 38952\n",
      "Replacing noun_phrase 4300 of 38952\n",
      "Replacing noun_phrase 4400 of 38952\n",
      "Replacing noun_phrase 4500 of 38952\n",
      "Replacing noun_phrase 4600 of 38952\n",
      "Replacing noun_phrase 4700 of 38952\n",
      "Replacing noun_phrase 4800 of 38952\n",
      "Replacing noun_phrase 4900 of 38952\n",
      "Replacing noun_phrase 5000 of 38952\n",
      "Replacing noun_phrase 5100 of 38952\n",
      "Replacing noun_phrase 5200 of 38952\n",
      "Replacing noun_phrase 5300 of 38952\n",
      "Replacing noun_phrase 5400 of 38952\n",
      "Replacing noun_phrase 5500 of 38952\n",
      "Replacing noun_phrase 5600 of 38952\n",
      "Replacing noun_phrase 5700 of 38952\n",
      "Replacing noun_phrase 5800 of 38952\n",
      "Replacing noun_phrase 5900 of 38952\n",
      "Replacing noun_phrase 6000 of 38952\n",
      "Replacing noun_phrase 6100 of 38952\n",
      "Replacing noun_phrase 6200 of 38952\n",
      "Replacing noun_phrase 6300 of 38952\n",
      "Replacing noun_phrase 6400 of 38952\n",
      "Replacing noun_phrase 6500 of 38952\n",
      "Replacing noun_phrase 6600 of 38952\n",
      "Replacing noun_phrase 6700 of 38952\n",
      "Replacing noun_phrase 6800 of 38952\n",
      "Replacing noun_phrase 6900 of 38952\n",
      "Replacing noun_phrase 7000 of 38952\n",
      "Replacing noun_phrase 7100 of 38952\n",
      "Replacing noun_phrase 7200 of 38952\n",
      "Replacing noun_phrase 7300 of 38952\n",
      "Replacing noun_phrase 7400 of 38952\n",
      "Replacing noun_phrase 7500 of 38952\n",
      "Replacing noun_phrase 7600 of 38952\n",
      "Replacing noun_phrase 7700 of 38952\n",
      "Replacing noun_phrase 7800 of 38952\n",
      "Replacing noun_phrase 7900 of 38952\n",
      "Replacing noun_phrase 8000 of 38952\n",
      "Replacing noun_phrase 8100 of 38952\n",
      "Replacing noun_phrase 8200 of 38952\n",
      "Replacing noun_phrase 8300 of 38952\n",
      "Replacing noun_phrase 8400 of 38952\n",
      "Replacing noun_phrase 8500 of 38952\n",
      "Replacing noun_phrase 8600 of 38952\n",
      "Replacing noun_phrase 8700 of 38952\n",
      "Replacing noun_phrase 8800 of 38952\n",
      "Replacing noun_phrase 8900 of 38952\n",
      "Replacing noun_phrase 9000 of 38952\n",
      "Replacing noun_phrase 9100 of 38952\n",
      "Replacing noun_phrase 9200 of 38952\n",
      "Replacing noun_phrase 9300 of 38952\n",
      "Replacing noun_phrase 9400 of 38952\n",
      "Replacing noun_phrase 9500 of 38952\n",
      "Replacing noun_phrase 9600 of 38952\n",
      "Replacing noun_phrase 9700 of 38952\n",
      "Replacing noun_phrase 9800 of 38952\n",
      "Replacing noun_phrase 9900 of 38952\n",
      "Replacing noun_phrase 10000 of 38952\n",
      "Replacing noun_phrase 10100 of 38952\n",
      "Replacing noun_phrase 10200 of 38952\n",
      "Replacing noun_phrase 10300 of 38952\n",
      "Replacing noun_phrase 10400 of 38952\n",
      "Replacing noun_phrase 10500 of 38952\n",
      "Replacing noun_phrase 10600 of 38952\n",
      "Replacing noun_phrase 10700 of 38952\n",
      "Replacing noun_phrase 10800 of 38952\n",
      "Replacing noun_phrase 10900 of 38952\n",
      "Replacing noun_phrase 11000 of 38952\n",
      "Replacing noun_phrase 11100 of 38952\n",
      "Replacing noun_phrase 11200 of 38952\n",
      "Replacing noun_phrase 11300 of 38952\n",
      "Replacing noun_phrase 11400 of 38952\n",
      "Replacing noun_phrase 11500 of 38952\n",
      "Replacing noun_phrase 11600 of 38952\n",
      "Replacing noun_phrase 11700 of 38952\n",
      "Replacing noun_phrase 11800 of 38952\n",
      "Replacing noun_phrase 11900 of 38952\n",
      "Replacing noun_phrase 12000 of 38952\n",
      "Replacing noun_phrase 12100 of 38952\n",
      "Replacing noun_phrase 12200 of 38952\n",
      "Replacing noun_phrase 12300 of 38952\n",
      "Replacing noun_phrase 12400 of 38952\n",
      "Replacing noun_phrase 12500 of 38952\n",
      "Replacing noun_phrase 12600 of 38952\n",
      "Replacing noun_phrase 12700 of 38952\n",
      "Replacing noun_phrase 12800 of 38952\n",
      "Replacing noun_phrase 12900 of 38952\n",
      "Replacing noun_phrase 13000 of 38952\n",
      "Replacing noun_phrase 13100 of 38952\n",
      "Replacing noun_phrase 13200 of 38952\n",
      "Replacing noun_phrase 13300 of 38952\n",
      "Replacing noun_phrase 13400 of 38952\n",
      "Replacing noun_phrase 13500 of 38952\n",
      "Replacing noun_phrase 13600 of 38952\n",
      "Replacing noun_phrase 13700 of 38952\n",
      "Replacing noun_phrase 13800 of 38952\n",
      "Replacing noun_phrase 13900 of 38952\n",
      "Replacing noun_phrase 14000 of 38952\n",
      "Replacing noun_phrase 14100 of 38952\n",
      "Replacing noun_phrase 14200 of 38952\n",
      "Replacing noun_phrase 14300 of 38952\n",
      "Replacing noun_phrase 14400 of 38952\n",
      "Replacing noun_phrase 14500 of 38952\n",
      "Replacing noun_phrase 14600 of 38952\n",
      "Replacing noun_phrase 14700 of 38952\n",
      "Replacing noun_phrase 14800 of 38952\n",
      "Replacing noun_phrase 14900 of 38952\n",
      "Replacing noun_phrase 15000 of 38952\n",
      "Replacing noun_phrase 15100 of 38952\n",
      "Replacing noun_phrase 15200 of 38952\n",
      "Replacing noun_phrase 15300 of 38952\n",
      "Replacing noun_phrase 15400 of 38952\n",
      "Replacing noun_phrase 15500 of 38952\n",
      "Replacing noun_phrase 15600 of 38952\n",
      "Replacing noun_phrase 15700 of 38952\n",
      "Replacing noun_phrase 15800 of 38952\n",
      "Replacing noun_phrase 15900 of 38952\n",
      "Replacing noun_phrase 16000 of 38952\n",
      "Replacing noun_phrase 16100 of 38952\n",
      "Replacing noun_phrase 16200 of 38952\n",
      "Replacing noun_phrase 16300 of 38952\n",
      "Replacing noun_phrase 16400 of 38952\n",
      "Replacing noun_phrase 16500 of 38952\n",
      "Replacing noun_phrase 16600 of 38952\n",
      "Replacing noun_phrase 16700 of 38952\n",
      "Replacing noun_phrase 16800 of 38952\n",
      "Replacing noun_phrase 16900 of 38952\n",
      "Replacing noun_phrase 17000 of 38952\n",
      "Replacing noun_phrase 17100 of 38952\n",
      "Replacing noun_phrase 17200 of 38952\n",
      "Replacing noun_phrase 17300 of 38952\n",
      "Replacing noun_phrase 17400 of 38952\n",
      "Replacing noun_phrase 17500 of 38952\n",
      "Replacing noun_phrase 17600 of 38952\n",
      "Replacing noun_phrase 17700 of 38952\n",
      "Replacing noun_phrase 17800 of 38952\n",
      "Replacing noun_phrase 17900 of 38952\n",
      "Replacing noun_phrase 18000 of 38952\n",
      "Replacing noun_phrase 18100 of 38952\n",
      "Replacing noun_phrase 18200 of 38952\n",
      "Replacing noun_phrase 18300 of 38952\n",
      "Replacing noun_phrase 18400 of 38952\n",
      "Replacing noun_phrase 18500 of 38952\n",
      "Replacing noun_phrase 18600 of 38952\n",
      "Replacing noun_phrase 18700 of 38952\n",
      "Replacing noun_phrase 18800 of 38952\n",
      "Replacing noun_phrase 18900 of 38952\n",
      "Replacing noun_phrase 19000 of 38952\n",
      "Replacing noun_phrase 19100 of 38952\n",
      "Replacing noun_phrase 19200 of 38952\n",
      "Replacing noun_phrase 19300 of 38952\n",
      "Replacing noun_phrase 19400 of 38952\n",
      "Replacing noun_phrase 19500 of 38952\n",
      "Replacing noun_phrase 19600 of 38952\n",
      "Replacing noun_phrase 19700 of 38952\n",
      "Replacing noun_phrase 19800 of 38952\n",
      "Replacing noun_phrase 19900 of 38952\n",
      "Replacing noun_phrase 20000 of 38952\n",
      "Replacing noun_phrase 20100 of 38952\n",
      "Replacing noun_phrase 20200 of 38952\n",
      "Replacing noun_phrase 20300 of 38952\n",
      "Replacing noun_phrase 20400 of 38952\n",
      "Replacing noun_phrase 20500 of 38952\n",
      "Replacing noun_phrase 20600 of 38952\n",
      "Replacing noun_phrase 20700 of 38952\n",
      "Replacing noun_phrase 20800 of 38952\n",
      "Replacing noun_phrase 20900 of 38952\n",
      "Replacing noun_phrase 21000 of 38952\n",
      "Replacing noun_phrase 21100 of 38952\n",
      "Replacing noun_phrase 21200 of 38952\n",
      "Replacing noun_phrase 21300 of 38952\n",
      "Replacing noun_phrase 21400 of 38952\n",
      "Replacing noun_phrase 21500 of 38952\n",
      "Replacing noun_phrase 21600 of 38952\n",
      "Replacing noun_phrase 21700 of 38952\n",
      "Replacing noun_phrase 21800 of 38952\n",
      "Replacing noun_phrase 21900 of 38952\n",
      "Replacing noun_phrase 22000 of 38952\n",
      "Replacing noun_phrase 22100 of 38952\n",
      "Replacing noun_phrase 22200 of 38952\n",
      "Replacing noun_phrase 22300 of 38952\n",
      "Replacing noun_phrase 22400 of 38952\n",
      "Replacing noun_phrase 22500 of 38952\n",
      "Replacing noun_phrase 22600 of 38952\n",
      "Replacing noun_phrase 22700 of 38952\n",
      "Replacing noun_phrase 22800 of 38952\n",
      "Replacing noun_phrase 22900 of 38952\n",
      "Replacing noun_phrase 23000 of 38952\n",
      "Replacing noun_phrase 23100 of 38952\n",
      "Replacing noun_phrase 23200 of 38952\n",
      "Replacing noun_phrase 23300 of 38952\n",
      "Replacing noun_phrase 23400 of 38952\n",
      "Replacing noun_phrase 23500 of 38952\n",
      "Replacing noun_phrase 23600 of 38952\n",
      "Replacing noun_phrase 23700 of 38952\n",
      "Replacing noun_phrase 23800 of 38952\n",
      "Replacing noun_phrase 23900 of 38952\n",
      "Replacing noun_phrase 24000 of 38952\n",
      "Replacing noun_phrase 24100 of 38952\n",
      "Replacing noun_phrase 24200 of 38952\n",
      "Replacing noun_phrase 24300 of 38952\n",
      "Replacing noun_phrase 24400 of 38952\n",
      "Replacing noun_phrase 24500 of 38952\n",
      "Replacing noun_phrase 24600 of 38952\n",
      "Replacing noun_phrase 24700 of 38952\n",
      "Replacing noun_phrase 24800 of 38952\n",
      "Replacing noun_phrase 24900 of 38952\n",
      "Replacing noun_phrase 25000 of 38952\n",
      "Replacing noun_phrase 25100 of 38952\n",
      "Replacing noun_phrase 25200 of 38952\n",
      "Replacing noun_phrase 25300 of 38952\n",
      "Replacing noun_phrase 25400 of 38952\n",
      "Replacing noun_phrase 25500 of 38952\n",
      "Replacing noun_phrase 25600 of 38952\n",
      "Replacing noun_phrase 25700 of 38952\n",
      "Replacing noun_phrase 25800 of 38952\n",
      "Replacing noun_phrase 25900 of 38952\n",
      "Replacing noun_phrase 26000 of 38952\n",
      "Replacing noun_phrase 26100 of 38952\n",
      "Replacing noun_phrase 26200 of 38952\n",
      "Replacing noun_phrase 26300 of 38952\n",
      "Replacing noun_phrase 26400 of 38952\n",
      "Replacing noun_phrase 26500 of 38952\n",
      "Replacing noun_phrase 26600 of 38952\n",
      "Replacing noun_phrase 26700 of 38952\n",
      "Replacing noun_phrase 26800 of 38952\n",
      "Replacing noun_phrase 26900 of 38952\n",
      "Replacing noun_phrase 27000 of 38952\n",
      "Replacing noun_phrase 27100 of 38952\n",
      "Replacing noun_phrase 27200 of 38952\n",
      "Replacing noun_phrase 27300 of 38952\n",
      "Replacing noun_phrase 27400 of 38952\n",
      "Replacing noun_phrase 27500 of 38952\n",
      "Replacing noun_phrase 27600 of 38952\n",
      "Replacing noun_phrase 27700 of 38952\n",
      "Replacing noun_phrase 27800 of 38952\n",
      "Replacing noun_phrase 27900 of 38952\n",
      "Replacing noun_phrase 28000 of 38952\n",
      "Replacing noun_phrase 28100 of 38952\n",
      "Replacing noun_phrase 28200 of 38952\n",
      "Replacing noun_phrase 28300 of 38952\n",
      "Replacing noun_phrase 28400 of 38952\n",
      "Replacing noun_phrase 28500 of 38952\n",
      "Replacing noun_phrase 28600 of 38952\n",
      "Replacing noun_phrase 28700 of 38952\n",
      "Replacing noun_phrase 28800 of 38952\n",
      "Replacing noun_phrase 28900 of 38952\n",
      "Replacing noun_phrase 29000 of 38952\n",
      "Replacing noun_phrase 29100 of 38952\n",
      "Replacing noun_phrase 29200 of 38952\n",
      "Replacing noun_phrase 29300 of 38952\n",
      "Replacing noun_phrase 29400 of 38952\n",
      "Replacing noun_phrase 29500 of 38952\n",
      "Replacing noun_phrase 29600 of 38952\n",
      "Replacing noun_phrase 29700 of 38952\n",
      "Replacing noun_phrase 29800 of 38952\n",
      "Replacing noun_phrase 29900 of 38952\n",
      "Replacing noun_phrase 30000 of 38952\n",
      "Replacing noun_phrase 30100 of 38952\n",
      "Replacing noun_phrase 30200 of 38952\n",
      "Replacing noun_phrase 30300 of 38952\n",
      "Replacing noun_phrase 30400 of 38952\n",
      "Replacing noun_phrase 30500 of 38952\n",
      "Replacing noun_phrase 30600 of 38952\n",
      "Replacing noun_phrase 30700 of 38952\n",
      "Replacing noun_phrase 30800 of 38952\n",
      "Replacing noun_phrase 30900 of 38952\n",
      "Replacing noun_phrase 31000 of 38952\n",
      "Replacing noun_phrase 31100 of 38952\n",
      "Replacing noun_phrase 31200 of 38952\n",
      "Replacing noun_phrase 31300 of 38952\n",
      "Replacing noun_phrase 31400 of 38952\n",
      "Replacing noun_phrase 31500 of 38952\n",
      "Replacing noun_phrase 31600 of 38952\n",
      "Replacing noun_phrase 31700 of 38952\n",
      "Replacing noun_phrase 31800 of 38952\n",
      "Replacing noun_phrase 31900 of 38952\n",
      "Replacing noun_phrase 32000 of 38952\n",
      "Replacing noun_phrase 32100 of 38952\n",
      "Replacing noun_phrase 32200 of 38952\n",
      "Replacing noun_phrase 32300 of 38952\n",
      "Replacing noun_phrase 32400 of 38952\n",
      "Replacing noun_phrase 32500 of 38952\n",
      "Replacing noun_phrase 32600 of 38952\n",
      "Replacing noun_phrase 32700 of 38952\n",
      "Replacing noun_phrase 32800 of 38952\n",
      "Replacing noun_phrase 32900 of 38952\n",
      "Replacing noun_phrase 33000 of 38952\n",
      "Replacing noun_phrase 33100 of 38952\n",
      "Replacing noun_phrase 33200 of 38952\n",
      "Replacing noun_phrase 33300 of 38952\n",
      "Replacing noun_phrase 33400 of 38952\n",
      "Replacing noun_phrase 33500 of 38952\n",
      "Replacing noun_phrase 33600 of 38952\n",
      "Replacing noun_phrase 33700 of 38952\n",
      "Replacing noun_phrase 33800 of 38952\n",
      "Replacing noun_phrase 33900 of 38952\n",
      "Replacing noun_phrase 34000 of 38952\n",
      "Replacing noun_phrase 34100 of 38952\n",
      "Replacing noun_phrase 34200 of 38952\n",
      "Replacing noun_phrase 34300 of 38952\n",
      "Replacing noun_phrase 34400 of 38952\n",
      "Replacing noun_phrase 34500 of 38952\n",
      "Replacing noun_phrase 34600 of 38952\n",
      "Replacing noun_phrase 34700 of 38952\n",
      "Replacing noun_phrase 34800 of 38952\n",
      "Replacing noun_phrase 34900 of 38952\n",
      "Replacing noun_phrase 35000 of 38952\n",
      "Replacing noun_phrase 35100 of 38952\n",
      "Replacing noun_phrase 35200 of 38952\n",
      "Replacing noun_phrase 35300 of 38952\n",
      "Replacing noun_phrase 35400 of 38952\n",
      "Replacing noun_phrase 35500 of 38952\n",
      "Replacing noun_phrase 35600 of 38952\n",
      "Replacing noun_phrase 35700 of 38952\n",
      "Replacing noun_phrase 35800 of 38952\n",
      "Replacing noun_phrase 35900 of 38952\n",
      "Replacing noun_phrase 36000 of 38952\n",
      "Replacing noun_phrase 36100 of 38952\n",
      "Replacing noun_phrase 36200 of 38952\n",
      "Replacing noun_phrase 36300 of 38952\n",
      "Replacing noun_phrase 36400 of 38952\n",
      "Replacing noun_phrase 36500 of 38952\n",
      "Replacing noun_phrase 36600 of 38952\n",
      "Replacing noun_phrase 36700 of 38952\n",
      "Replacing noun_phrase 36800 of 38952\n",
      "Replacing noun_phrase 36900 of 38952\n",
      "Replacing noun_phrase 37000 of 38952\n",
      "Replacing noun_phrase 37100 of 38952\n",
      "Replacing noun_phrase 37200 of 38952\n",
      "Replacing noun_phrase 37300 of 38952\n",
      "Replacing noun_phrase 37400 of 38952\n",
      "Replacing noun_phrase 37500 of 38952\n",
      "Replacing noun_phrase 37600 of 38952\n",
      "Replacing noun_phrase 37700 of 38952\n",
      "Replacing noun_phrase 37800 of 38952\n",
      "Replacing noun_phrase 37900 of 38952\n",
      "Replacing noun_phrase 38000 of 38952\n",
      "Replacing noun_phrase 38100 of 38952\n",
      "Replacing noun_phrase 38200 of 38952\n",
      "Replacing noun_phrase 38300 of 38952\n",
      "Replacing noun_phrase 38400 of 38952\n",
      "Replacing noun_phrase 38500 of 38952\n",
      "Replacing noun_phrase 38600 of 38952\n",
      "Replacing noun_phrase 38700 of 38952\n",
      "Replacing noun_phrase 38800 of 38952\n",
      "Replacing noun_phrase 38900 of 38952\n"
     ]
    }
   ],
   "source": [
    "if SHOULD_GENERATE_UNDERSCORED_CORPUS:\n",
    "    for i, noun_phrase in enumerate(noun_phrases):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Replacing noun_phrase {i + 1} of {len(noun_phrases)}')\n",
    "        found_indices = set()\n",
    "        while find_phrase_in_corpus(corpus, noun_phrase) != (-1, -1) and find_phrase_in_corpus(corpus, noun_phrase)[0] not in found_indices:\n",
    "            s_idx, e_idx = find_phrase_in_corpus(corpus, noun_phrase)\n",
    "            found_indices.add(s_idx)\n",
    "            corpus = corpus[:s_idx] + noun_phrase.replace(' ', '_') + corpus[e_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "former-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOULD_GENERATE_UNDERSCORED_CORPUS:\n",
    "    pickle.dump(corpus, open('data/underscored_corpus.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "casual-orleans",
   "metadata": {},
   "outputs": [],
   "source": [
    "underscored_corpus = pickle.load(open('data/underscored_corpus.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ignored-miller",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOULD_TRAIN_WORD2VEC_MODEL:\n",
    "    underscored_corpus_data = []\n",
    "    for i in sent_tokenize(underscored_corpus):\n",
    "        temp = []\n",
    "        for j in word_tokenize(i):\n",
    "            temp.append(j.lower())\n",
    "        underscored_corpus_data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "latest-taylor",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOULD_TRAIN_WORD2VEC_MODEL:\n",
    "    word2vec_model = Word2Vec(underscored_corpus_data, min_count=1, window=WINDOW_SIZE, size=VECTOR_SIZE)\n",
    "    word2vec_model.save(f'data/word2vec_model_vs_{VECTOR_SIZE}_ws_{WINDOW_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "chief-howard",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec.load(f'data/word2vec_model_vs_{VECTOR_SIZE}_ws_{WINDOW_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "understood-painting",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token for token in list(word2vec_model.wv.vocab.keys())]\n",
    "embeddings = {token: word2vec_model.wv[token] for token in tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aquatic-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_samples = [ps for ps in positive_samples if ps.replace(' ', '_') in embeddings and word2vec_model.wv.vocab[ps.replace(' ', '_')].count >= MIN_FREQUENCY]\n",
    "negative_samples = [ns for ns in negative_samples if ns.replace(' ', '_') in embeddings and word2vec_model.wv.vocab[ns.replace(' ', '_')].count >= MIN_FREQUENCY]\n",
    "\n",
    "positive_samples = positive_samples[:min(len(positive_samples), len(negative_samples))]\n",
    "negative_samples = negative_samples[:min(len(positive_samples), len(negative_samples))]\n",
    "\n",
    "ps_set = set(positive_samples)\n",
    "ns_set = set(negative_samples)\n",
    "\n",
    "noun_phrases = [np for np in noun_phrases if np not in ps_set and np not in ns_set and np.replace(' ', '_') in embeddings and word2vec_model.wv.vocab[np.replace(' ', '_')].count >= MIN_FREQUENCY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "upset-alfred",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for phrase in positive_samples:\n",
    "    X.append(embeddings[phrase.replace(' ', '_')])\n",
    "    y.append(1)\n",
    "for phrase in negative_samples:\n",
    "    X.append(embeddings[phrase.replace(' ', '_')])\n",
    "    y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "precious-settle",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = list(zip(X, y))\n",
    "random.shuffle(c)\n",
    "X, y = zip(*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "attended-shoulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "partial-hotel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "\n",
    "        self.layer_1 = nn.Linear(VECTOR_SIZE, 128)\n",
    "        \n",
    "        self.layers = []\n",
    "        for _ in range(NUM_LAYERS - 1):\n",
    "            self.layers.append(nn.Linear(128, 128))\n",
    "        \n",
    "        self.layer_out = nn.Linear(128, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = self.relu(layer(x))\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cubic-nation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryClassifier(\n",
       "  (layer_1): Linear(in_features=200, out_features=128, bias=True)\n",
       "  (layer_out): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BinaryClassifier()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "challenging-hybrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "mineral-korean",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "train_data = TrainDataset(torch.FloatTensor(np.array(X_train, dtype=np.float64)), \n",
    "                          torch.FloatTensor(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "martial-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "\n",
    "test_data = TestDataset(torch.FloatTensor(np.array(X_test, dtype=np.float64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "deluxe-mauritius",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "black-salmon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cordless-twenty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010: | Loss: 0.66192 | Acc: 63.923\n",
      "Epoch 020: | Loss: 0.61179 | Acc: 68.385\n",
      "Epoch 030: | Loss: 0.58182 | Acc: 69.692\n",
      "Epoch 040: | Loss: 0.54605 | Acc: 72.538\n",
      "Epoch 050: | Loss: 0.52339 | Acc: 73.308\n",
      "Epoch 060: | Loss: 0.49638 | Acc: 75.308\n",
      "Epoch 070: | Loss: 0.47181 | Acc: 77.462\n",
      "Epoch 080: | Loss: 0.46155 | Acc: 78.385\n",
      "Epoch 090: | Loss: 0.44577 | Acc: 80.538\n",
      "Epoch 100: | Loss: 0.42971 | Acc: 81.923\n"
     ]
    }
   ],
   "source": [
    "epoch_losses = []\n",
    "for e in range(1, EPOCHS + 1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "\n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    epoch_losses.append(epoch_loss / len(train_loader)) \n",
    "\n",
    "    if e % 10 == 0:\n",
    "        print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "scheduled-emergency",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdd2cb8c250>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "significant-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "lesser-clear",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[75 18]\n",
      " [34 69]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.81      0.74        93\n",
      "           1       0.79      0.67      0.73       103\n",
      "\n",
      "    accuracy                           0.73       196\n",
      "   macro avg       0.74      0.74      0.73       196\n",
      "weighted avg       0.74      0.73      0.73       196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred_list))\n",
    "print(classification_report(y_test, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "operating-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = random.sample(noun_phrases, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "smooth-lemon",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_samples = []\n",
    "for sample in test_samples:\n",
    "    X_samples.append(embeddings[sample.replace(' ', '_')])\n",
    "sample_data = TestDataset(torch.FloatTensor(np.array(X_samples, dtype=np.float64)))\n",
    "sample_loader = DataLoader(dataset=sample_data, batch_size=1)\n",
    "\n",
    "extracted = {}\n",
    "not_extracted = {}\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, X_batch in enumerate(sample_loader):\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        if y_pred_tag.cpu().numpy()[0][0] == 1:\n",
    "            extracted[test_samples[i]] = y_test_pred.item()\n",
    "        else:\n",
    "            not_extracted[test_samples[i]] = y_test_pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "moving-semester",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = set(['a', 'the', 'an'])\n",
    "filtered_out = {k: v for k, v in sorted(extracted.items(), key=lambda x: x[1], reverse=True) if k.split()[0] in articles}\n",
    "extracted = {k: v for k, v in sorted(extracted.items(), key=lambda x: x[1], reverse=True) if k.split()[0] not in articles}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fancy-location",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a wide range': 0.9558593034744263,\n",
       " 'the community': 0.6654583215713501,\n",
       " 'an online': 0.6592714190483093,\n",
       " 'an explosion': 0.5625393390655518,\n",
       " 'the classical': 0.5613019466400146,\n",
       " 'the other hand': 0.5581458210945129,\n",
       " 'the framework': 0.5367175936698914,\n",
       " 'the course': 0.5332760214805603}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "mounted-memorabilia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'applications': 0.9999945163726807,\n",
       " 'space': 0.9998693466186523,\n",
       " 'representation': 0.9992013573646545,\n",
       " 'paper': 0.9991937279701233,\n",
       " 'gradient': 0.9982473850250244,\n",
       " 'training': 0.9954337477684021,\n",
       " 'media': 0.9936866164207458,\n",
       " 'unsupervised': 0.9909802079200745,\n",
       " 'language processing': 0.990481972694397,\n",
       " 'short term memory': 0.9897748231887817,\n",
       " 'building': 0.989722728729248,\n",
       " 'development': 0.9894728064537048,\n",
       " 'imaging': 0.986858606338501,\n",
       " 'source': 0.9833688139915466,\n",
       " 'sparse': 0.9799783825874329,\n",
       " 'problems': 0.979804515838623,\n",
       " 'challenging problem': 0.9785484075546265,\n",
       " 'study': 0.9783535599708557,\n",
       " 'success': 0.9755644798278809,\n",
       " 'performance': 0.9753079414367676,\n",
       " 'rate': 0.974626898765564,\n",
       " 'parallel': 0.9679027795791626,\n",
       " 'processing': 0.9674440622329712,\n",
       " 'user': 0.9633289575576782,\n",
       " '2013': 0.9577705264091492,\n",
       " '3d': 0.9571154713630676,\n",
       " 'de': 0.9533666968345642,\n",
       " 'techniques': 0.9531154036521912,\n",
       " 'incremental': 0.9517297744750977,\n",
       " 'light': 0.9495378732681274,\n",
       " 'collection': 0.9480283260345459,\n",
       " 'program': 0.9427058100700378,\n",
       " 'product': 0.9334729313850403,\n",
       " 'em': 0.9291689395904541,\n",
       " 'potential': 0.9277828335762024,\n",
       " 'fmri': 0.9267409443855286,\n",
       " 'parameter': 0.908886194229126,\n",
       " 'test': 0.9072532653808594,\n",
       " 'parameters': 0.9047573804855347,\n",
       " 'speed': 0.9047181010246277,\n",
       " 'life': 0.9023454785346985,\n",
       " 'focus': 0.8997446298599243,\n",
       " 'nearest': 0.8958178162574768,\n",
       " 'factorization': 0.8910502195358276,\n",
       " 'train': 0.8702111840248108,\n",
       " 'paper addresses': 0.870110273361206,\n",
       " 'meta': 0.8679906725883484,\n",
       " 'resnet': 0.8647079467773438,\n",
       " 'procedure': 0.8646913766860962,\n",
       " 'making': 0.8568748831748962,\n",
       " 'kernel learning': 0.8565130829811096,\n",
       " 'backpropagation': 0.853597104549408,\n",
       " 'perspective': 0.8470361232757568,\n",
       " 'direction method': 0.844659149646759,\n",
       " 'loopy': 0.8435847759246826,\n",
       " 'python': 0.8307174444198608,\n",
       " 'image analysis': 0.8130289316177368,\n",
       " 'daily': 0.8119216561317444,\n",
       " 'coco': 0.8090670704841614,\n",
       " 'efficiency': 0.8024399876594543,\n",
       " 'artifacts': 0.7998685836791992,\n",
       " 'great potential': 0.7849013209342957,\n",
       " 'camera': 0.7747865915298462,\n",
       " 'structure learning': 0.7739133834838867,\n",
       " 'retrieval': 0.7710399627685547,\n",
       " 'us': 0.7700423002243042,\n",
       " 'service': 0.7644954919815063,\n",
       " 'date': 0.7531023025512695,\n",
       " 'this field': 0.7469757199287415,\n",
       " 'd': 0.7459473013877869,\n",
       " 'real valued': 0.7245299220085144,\n",
       " 'rnns': 0.7237977385520935,\n",
       " 'large scale problems': 0.7233033180236816,\n",
       " 'vqa': 0.7227032780647278,\n",
       " 'imitation': 0.7204359173774719,\n",
       " 'reward': 0.7152314782142639,\n",
       " 'sorting': 0.7085230350494385,\n",
       " 'cognition': 0.7063941359519958,\n",
       " 'frameworks': 0.7041781544685364,\n",
       " 'improvement': 0.7034732699394226,\n",
       " 'dimension reduction': 0.7021936774253845,\n",
       " 'cryo': 0.7014133930206299,\n",
       " 'matrix factorization nmf': 0.6887403726577759,\n",
       " 'parallelization': 0.6862602829933167,\n",
       " 'mission': 0.6805950999259949,\n",
       " 'concern': 0.6800060868263245,\n",
       " 'random fields': 0.6679009795188904,\n",
       " 'text processing': 0.6674323678016663,\n",
       " 'inverse problems': 0.6664496064186096,\n",
       " 'textit': 0.6621534824371338,\n",
       " 'justification': 0.6600032448768616,\n",
       " 'hyperparameter optimization': 0.6438380479812622,\n",
       " 'deep learning models': 0.6420844793319702,\n",
       " 'em algorithm': 0.640999972820282,\n",
       " 'big data': 0.6384206414222717,\n",
       " 'multi label learning': 0.6364556550979614,\n",
       " 'sentiment classification': 0.6350529789924622,\n",
       " 'advertising': 0.6331364512443542,\n",
       " 'substitute': 0.6189168095588684,\n",
       " 'distributional semantics': 0.6170123219490051,\n",
       " 'language understanding': 0.615611732006073,\n",
       " 'healthcare': 0.6115911602973938,\n",
       " 'replacement': 0.6115420460700989,\n",
       " 'reinforcement learning problems': 0.6112461090087891,\n",
       " '8': 0.6093196272850037,\n",
       " 'findings': 0.6058958172798157,\n",
       " 'successes': 0.5978256464004517,\n",
       " 'optimisation problems': 0.5968174934387207,\n",
       " 'potts': 0.5945146083831787,\n",
       " 'landmark': 0.5944942831993103,\n",
       " 'maximum': 0.5921968221664429,\n",
       " 'deep learning architectures': 0.5882789492607117,\n",
       " 'regularization methods': 0.5862559676170349,\n",
       " 'deep recurrent neural networks': 0.5815267562866211,\n",
       " 'wearables': 0.5759010314941406,\n",
       " 'word2vec': 0.5743988752365112,\n",
       " 'vote': 0.5733862519264221,\n",
       " 'brl': 0.5660235285758972,\n",
       " 'deals': 0.5642309784889221,\n",
       " 'lifelong learning': 0.5632804036140442,\n",
       " 'continuous time': 0.5622448921203613,\n",
       " '3': 0.5580583810806274,\n",
       " 'major challenges': 0.5580411553382874,\n",
       " 'neural architectures': 0.5574377775192261,\n",
       " 'non uniform': 0.5508862137794495,\n",
       " 'complex environments': 0.547878623008728,\n",
       " 'game': 0.5470768809318542,\n",
       " 'multi label': 0.5464088320732117,\n",
       " 'mathematics': 0.5453060269355774,\n",
       " 'developers': 0.5389177203178406,\n",
       " 'dca': 0.5373037457466125,\n",
       " 'nature': 0.5366681814193726,\n",
       " 'temporal data': 0.5363547801971436,\n",
       " 'nearest neighbor knn': 0.5354328155517578,\n",
       " 'messages': 0.534437358379364,\n",
       " 'semantic segmentation': 0.5315018892288208,\n",
       " 'kernel hilbert': 0.529805064201355,\n",
       " 'iterative algorithms': 0.5239630341529846,\n",
       " 'little attention': 0.5227736830711365,\n",
       " 'trial and error': 0.5216459631919861,\n",
       " 'machine learning and statistics': 0.5208209753036499,\n",
       " 'sensor networks': 0.5196017026901245,\n",
       " 'human actions': 0.5193173289299011,\n",
       " 'sa': 0.5186832547187805,\n",
       " 'x': 0.5151340961456299,\n",
       " 'rectified linear unit relu': 0.5138764977455139,\n",
       " 'privacy': 0.5129098296165466,\n",
       " 'parallelize': 0.5095212459564209,\n",
       " 'sublinear': 0.5064459443092346,\n",
       " 'mart': 0.5061168670654297,\n",
       " 'renewed interest': 0.5060555934906006,\n",
       " 'adversarial attacks': 0.5032110810279846,\n",
       " 'computer vision and pattern recognition': 0.5023516416549683,\n",
       " 'spam': 0.5016167759895325,\n",
       " 'risk management': 0.501558244228363,\n",
       " 'various fields': 0.5014074444770813,\n",
       " 'twitter': 0.5009990930557251}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "conceptual-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_extracted = {k: v for k, v in sorted(not_extracted.items(), key=lambda x: x[1], reverse=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "possible-convention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'they': 0.000154579320224002,\n",
       " 'assumptions': 0.015291282907128334,\n",
       " 'the best': 0.034452009946107864,\n",
       " 'relationships': 0.03538006544113159,\n",
       " 'a': 0.03723011910915375,\n",
       " 'datasets': 0.041143011301755905,\n",
       " 'the art results': 0.04471088945865631,\n",
       " 'attributes': 0.05403716862201691,\n",
       " 'structures': 0.05411797761917114,\n",
       " 'networks': 0.05819607898592949,\n",
       " 'distances': 0.06478249281644821,\n",
       " 'units': 0.06704492121934891,\n",
       " 'the features': 0.06835080683231354,\n",
       " 'elements': 0.06894342601299286,\n",
       " 'art': 0.07004833966493607,\n",
       " 'dnns': 0.07039090245962143,\n",
       " 'situations': 0.0716388151049614,\n",
       " 'semantics': 0.0751161202788353,\n",
       " 'scores': 0.07691210508346558,\n",
       " 'strong': 0.08259320259094238,\n",
       " 'computations': 0.08740512281656265,\n",
       " 'aspects': 0.08795308321714401,\n",
       " 'local minima': 0.09652693569660187,\n",
       " 'the robustness': 0.09735438227653503,\n",
       " 'approximations': 0.09749593585729599,\n",
       " 'these networks': 0.10230764001607895,\n",
       " 'levels': 0.1026461273431778,\n",
       " 'explanations': 0.10412026196718216,\n",
       " 'patterns': 0.10901197791099548,\n",
       " 'insights': 0.1114034429192543,\n",
       " 'three': 0.11502189934253693,\n",
       " 'policies': 0.11516546458005905,\n",
       " 'the hidden': 0.12009402364492416,\n",
       " 'options': 0.12165888398885727,\n",
       " 'effects': 0.12189514189958572,\n",
       " 'matrices': 0.12439928203821182,\n",
       " 'facts': 0.12540385127067566,\n",
       " 'the samples': 0.12790486216545105,\n",
       " 'the next': 0.128031924366951,\n",
       " 'hidden states': 0.13360342383384705,\n",
       " 'the model': 0.13455839455127716,\n",
       " 'diversity': 0.14035500586032867,\n",
       " 'new algorithms': 0.14069826900959015,\n",
       " 'details': 0.14115145802497864,\n",
       " 'the users': 0.14324933290481567,\n",
       " 'segmentations': 0.14558763802051544,\n",
       " 'correlations': 0.1461741328239441,\n",
       " 'network architectures': 0.1462555229663849,\n",
       " 'tweets': 0.1481730043888092,\n",
       " 'particles': 0.14821672439575195,\n",
       " 'time steps': 0.14982736110687256,\n",
       " 'the words': 0.14994387328624725,\n",
       " 'a significant improvement': 0.15066629648208618,\n",
       " 'the matching': 0.15090462565422058,\n",
       " 'appearance': 0.15297913551330566,\n",
       " 'the remaining': 0.15505148470401764,\n",
       " 'patches': 0.15652281045913696,\n",
       " 'a new dataset': 0.1567833423614502,\n",
       " 'a black box': 0.1601385623216629,\n",
       " 'significant improvements': 0.16044597327709198,\n",
       " 'targets': 0.16132275760173798,\n",
       " 'symbols': 0.1636316031217575,\n",
       " 'motifs': 0.1637418270111084,\n",
       " 'matches': 0.1638939380645752,\n",
       " 'the ordering': 0.16476169228553772,\n",
       " 'gaussians': 0.164947047829628,\n",
       " 'a minimum': 0.16610196232795715,\n",
       " 'value functions': 0.1668909788131714,\n",
       " 'statements': 0.16735528409481049,\n",
       " 'densities': 0.16770055890083313,\n",
       " 'compositional': 0.16921734809875488,\n",
       " 'invariances': 0.1708803027868271,\n",
       " 'goodness': 0.17176729440689087,\n",
       " 'the dictionary': 0.17190556228160858,\n",
       " 'an edge': 0.1722756326198578,\n",
       " 'the space': 0.17258144915103912,\n",
       " 'its effectiveness': 0.17276939749717712,\n",
       " 'low level features': 0.17302654683589935,\n",
       " 'the relationship': 0.17493994534015656,\n",
       " 'label correlations': 0.1750546395778656,\n",
       " 'molecules': 0.1755671352148056,\n",
       " 'a new type': 0.1756911426782608,\n",
       " 'a given set': 0.17590753734111786,\n",
       " 'the partitions': 0.17612209916114807,\n",
       " 'quantifiers': 0.17658933997154236,\n",
       " 'ambiguity': 0.1770888715982437,\n",
       " 'higher performance': 0.177458256483078,\n",
       " 'a unified framework': 0.17789162695407867,\n",
       " 'the normalization': 0.1781981736421585,\n",
       " 'the points': 0.17884361743927002,\n",
       " 'psrs': 0.17908473312854767,\n",
       " 'a better understanding': 0.17915144562721252,\n",
       " 'image content': 0.17928637564182281,\n",
       " 'the mixture': 0.17977967858314514,\n",
       " 'long range dependencies': 0.1802978515625,\n",
       " 'the strength': 0.1808551698923111,\n",
       " 'a summary': 0.1810448318719864,\n",
       " 'a deep neural network dnn': 0.18114900588989258,\n",
       " 'two nodes': 0.18175800144672394,\n",
       " 'the true distribution': 0.18183967471122742,\n",
       " 'pairwise': 0.18258853256702423,\n",
       " 'related problems': 0.18282145261764526,\n",
       " 'noisy observations': 0.18293066322803497,\n",
       " 'its accuracy': 0.18327759206295013,\n",
       " 'rectifier networks': 0.18370752036571503,\n",
       " 'a diverse set': 0.18377114832401276,\n",
       " 'visual concepts': 0.18424350023269653,\n",
       " 'complex': 0.18446515500545502,\n",
       " 'the capabilities': 0.18452376127243042,\n",
       " 'object classes': 0.18465881049633026,\n",
       " 'training points': 0.18484017252922058,\n",
       " 'this score': 0.18577352166175842,\n",
       " 'equivariance': 0.1859605610370636,\n",
       " 'posterior probabilities': 0.18629367649555206,\n",
       " 'weak supervision': 0.1864369809627533,\n",
       " 'an effective algorithm': 0.1868414580821991,\n",
       " 'different objects': 0.1870996057987213,\n",
       " 'optimal policies': 0.18720585107803345,\n",
       " 'sigma': 0.1873614639043808,\n",
       " 'the missing': 0.1877991110086441,\n",
       " 'changing': 0.1884394884109497,\n",
       " 'this chapter': 0.18855758011341095,\n",
       " 'expense': 0.18857349455356598,\n",
       " 'a bayesian network': 0.1885855346918106,\n",
       " 'utterances': 0.18883384764194489,\n",
       " 'programming asp': 0.1889784336090088,\n",
       " 'a policy': 0.18972034752368927,\n",
       " 'the joint distribution': 0.19052675366401672,\n",
       " 'the batch size': 0.1907808780670166,\n",
       " 'their predictions': 0.19079823791980743,\n",
       " 'the training data': 0.1917157769203186,\n",
       " 'each topic': 0.19195722043514252,\n",
       " 'smoothness assumptions': 0.19202594459056854,\n",
       " 'random noise': 0.19272159039974213,\n",
       " 'the learning performance': 0.19277392327785492,\n",
       " 'the second order statistics': 0.19316960871219635,\n",
       " 'the agreement': 0.19383089244365692,\n",
       " 'the input space': 0.19409377872943878,\n",
       " 'hidden units': 0.19412420690059662,\n",
       " 'the prior knowledge': 0.19426783919334412,\n",
       " 'memories': 0.1944703459739685,\n",
       " 'theoretical results': 0.19468197226524353,\n",
       " 'the sum': 0.19522039592266083,\n",
       " 'their similarity': 0.19525033235549927,\n",
       " 'individual neurons': 0.1954619437456131,\n",
       " 'the prediction error': 0.19551771879196167,\n",
       " 'inference procedures': 0.1958402693271637,\n",
       " 'an investigation': 0.19592779874801636,\n",
       " 'the cluster structure': 0.19703654944896698,\n",
       " 'objects in images': 0.197046160697937,\n",
       " 'the collection': 0.19705742597579956,\n",
       " 'optimization criteria': 0.197087362408638,\n",
       " 'side information': 0.19728754460811615,\n",
       " 'iterations': 0.19740034639835358,\n",
       " 'redundant features': 0.19756096601486206,\n",
       " 'few examples': 0.19777899980545044,\n",
       " 'new results': 0.19792820513248444,\n",
       " 'each individual': 0.1985752135515213,\n",
       " 'all models': 0.19868643581867218,\n",
       " 'a known distribution': 0.19888868927955627,\n",
       " 'dimensionality': 0.19917747378349304,\n",
       " 'attention based models': 0.19922831654548645,\n",
       " 'a prediction': 0.1992795318365097,\n",
       " 'user feedback': 0.19929149746894836,\n",
       " 'the query': 0.19952237606048584,\n",
       " 'the criteria': 0.1997430920600891,\n",
       " 'an autoencoder': 0.20012344419956207,\n",
       " 'structured data': 0.20013971626758575,\n",
       " 'an important issue': 0.20017462968826294,\n",
       " 'a shared representation': 0.20052917301654816,\n",
       " 'a deep learning network': 0.20053721964359283,\n",
       " 'histograms': 0.20059318840503693,\n",
       " 'percepts': 0.20088228583335876,\n",
       " 'the output weights': 0.20096829533576965,\n",
       " 'the input distribution': 0.2010580450296402,\n",
       " 'a sparse linear combination': 0.20123185217380524,\n",
       " 'previous tasks': 0.20154640078544617,\n",
       " 'sparse vectors': 0.20158971846103668,\n",
       " 'sensory data': 0.2019168883562088,\n",
       " 'pu learning': 0.2019209861755371,\n",
       " 'selectivity': 0.202533558011055,\n",
       " 'utility functions': 0.20291243493556976,\n",
       " 'a deep reinforcement': 0.20335917174816132,\n",
       " 'an open challenge': 0.2035929560661316,\n",
       " 'a query': 0.2037418782711029,\n",
       " 'the whole dataset': 0.20398621261119843,\n",
       " 'two representations': 0.20422345399856567,\n",
       " 'a novel architecture': 0.20467878878116608,\n",
       " 'several models': 0.20476852357387543,\n",
       " 'rich information': 0.2053525298833847,\n",
       " 'an image': 0.20535525679588318,\n",
       " 'different words': 0.20551501214504242,\n",
       " 'deeplift': 0.20557627081871033,\n",
       " 'a probability distribution': 0.2058936059474945,\n",
       " 'the modes': 0.2059105932712555,\n",
       " 'learning agents': 0.2059936821460724,\n",
       " 'its complexity': 0.20601047575473785,\n",
       " 'synthetic images': 0.20636768639087677,\n",
       " 'batch normalization': 0.20661772787570953,\n",
       " 'their outputs': 0.20681820809841156,\n",
       " 'structural information': 0.20730704069137573,\n",
       " 'prototypical networks': 0.2073926478624344,\n",
       " 'an online fashion': 0.20745116472244263,\n",
       " 'discrete optimization': 0.20764227211475372,\n",
       " 'fcm': 0.20766417682170868,\n",
       " 'abstract states': 0.2079448401927948,\n",
       " 'a simple baseline': 0.20795556902885437,\n",
       " 'a bridge': 0.20825830101966858,\n",
       " 'a small subset': 0.2086387276649475,\n",
       " 'the running time': 0.20873907208442688,\n",
       " 'large numbers': 0.20898650586605072,\n",
       " 'a new problem': 0.20910532772541046,\n",
       " 'an efficient alternative': 0.20925162732601166,\n",
       " 'each distribution': 0.20983345806598663,\n",
       " 'the problem at hand': 0.2100493609905243,\n",
       " 'lateral connections': 0.21012654900550842,\n",
       " 'neural reasoner': 0.21073168516159058,\n",
       " 'better performance': 0.21129828691482544,\n",
       " 'a vector space': 0.21134574711322784,\n",
       " 'model misspecification': 0.21181750297546387,\n",
       " 'the actions': 0.2119215726852417,\n",
       " 'a random subset': 0.21211236715316772,\n",
       " 'the behavior': 0.21227414906024933,\n",
       " 'single view': 0.21236871182918549,\n",
       " 'disentanglement': 0.21270370483398438,\n",
       " 'rich representations': 0.21325914561748505,\n",
       " 'synapses': 0.21331167221069336,\n",
       " 'the exploitation': 0.21366524696350098,\n",
       " 'purely data': 0.21379095315933228,\n",
       " 'the confidence': 0.21389584243297577,\n",
       " 'any learning algorithm': 0.21405015885829926,\n",
       " 'pac': 0.21455983817577362,\n",
       " 'the players': 0.2147139608860016,\n",
       " 'melodies': 0.21505700051784515,\n",
       " 'an extensive study': 0.21514326333999634,\n",
       " 'the proposed method': 0.21561919152736664,\n",
       " 'absence': 0.21585595607757568,\n",
       " 'a novel extension': 0.21620547771453857,\n",
       " 'infogan': 0.21666653454303741,\n",
       " 'fine tuning': 0.216818705201149,\n",
       " 'a vocabulary': 0.2169257253408432,\n",
       " 'flaws': 0.21695177257061005,\n",
       " 'the dialogue': 0.21717827022075653,\n",
       " 'a generic framework': 0.21758396923542023,\n",
       " 'unlabeled samples': 0.21784615516662598,\n",
       " 'cmr': 0.2181105762720108,\n",
       " 'the learning rate': 0.21825821697711945,\n",
       " 'an optimal solution': 0.21886849403381348,\n",
       " 'this capability': 0.2190510630607605,\n",
       " 'a knowledge graph': 0.21905724704265594,\n",
       " 'a few examples': 0.219429150223732,\n",
       " 'a given task': 0.21946890652179718,\n",
       " 'basic units': 0.21972569823265076,\n",
       " 'a recurrent neural network': 0.21991190314292908,\n",
       " 'the onset': 0.22021576762199402,\n",
       " 'a convolutional layer': 0.2204814851284027,\n",
       " 'this point': 0.22052402794361115,\n",
       " 'several variants': 0.22076399624347687,\n",
       " 'the classification results': 0.2210901528596878,\n",
       " 'a number of applications': 0.22109118103981018,\n",
       " 'this bias': 0.22119519114494324,\n",
       " 'a distributed system': 0.22150233387947083,\n",
       " 'a gaussian mixture model': 0.2224782407283783,\n",
       " 'a deep generative model': 0.22248445451259613,\n",
       " 'an optimal policy': 0.2225477546453476,\n",
       " 'a wealth': 0.22320377826690674,\n",
       " 'the background': 0.22346214950084686,\n",
       " 'superlinear': 0.22348786890506744,\n",
       " 'necessary and sufficient conditions': 0.22433684766292572,\n",
       " 'prior information': 0.22439134120941162,\n",
       " 'its inputs': 0.22544336318969727,\n",
       " 'a supervised classification': 0.22550669312477112,\n",
       " 'over parametrization': 0.2258647084236145,\n",
       " 'individual layers': 0.22594374418258667,\n",
       " 'uncertainty information': 0.2265281230211258,\n",
       " 'the body': 0.22657692432403564,\n",
       " 'the group': 0.22675330936908722,\n",
       " 'the mean': 0.2270248383283615,\n",
       " 'a prior distribution': 0.22707927227020264,\n",
       " 'predictive accuracy': 0.22731363773345947,\n",
       " 'slate': 0.22743107378482819,\n",
       " 'distributed processing': 0.22745011746883392,\n",
       " 'probabilistic reasoning': 0.22801117599010468,\n",
       " 'objectives': 0.22815421223640442,\n",
       " 'input output pairs': 0.22824588418006897,\n",
       " 'neural representations': 0.22846266627311707,\n",
       " 'missing data': 0.22889220714569092,\n",
       " 'a cost function': 0.22892242670059204,\n",
       " 'costs': 0.22893927991390228,\n",
       " 'a learning task': 0.2289910763502121,\n",
       " 'many ways': 0.22904419898986816,\n",
       " 'the costs': 0.22905990481376648,\n",
       " 'a common approach': 0.2293291687965393,\n",
       " 'super convergence': 0.22950595617294312,\n",
       " 'annotated training data': 0.22965814173221588,\n",
       " 'malaria': 0.22988687455654144,\n",
       " 'statistical tests': 0.23021234571933746,\n",
       " 'klsh': 0.2302274852991104,\n",
       " 'shift': 0.23069243133068085,\n",
       " 'presence': 0.23075167834758759,\n",
       " 'a novel framework': 0.23087236285209656,\n",
       " 'no information': 0.23090793192386627,\n",
       " 'an algorithm': 0.2310125231742859,\n",
       " 'a major limitation': 0.2311597466468811,\n",
       " 'the polarity': 0.23205484449863434,\n",
       " 'different strategies': 0.2320878654718399,\n",
       " 'a new instance': 0.23222196102142334,\n",
       " 'the effectiveness': 0.2323494553565979,\n",
       " 'labeled and unlabeled data': 0.2325350046157837,\n",
       " 'good results': 0.2327534705400467,\n",
       " 'these processes': 0.23286253213882446,\n",
       " 'these items': 0.23289604485034943,\n",
       " 'training time': 0.23296082019805908,\n",
       " 'the video': 0.23420566320419312,\n",
       " 'the convergence rate': 0.23445217311382294,\n",
       " 'multiple modes': 0.23468151688575745,\n",
       " 'a weighted graph': 0.2349189966917038,\n",
       " 'key aspects': 0.2349797487258911,\n",
       " 'share information': 0.2349889725446701,\n",
       " 'stochastic gradient descent sgd': 0.23580807447433472,\n",
       " 'batch algorithms': 0.2358936369419098,\n",
       " 'information flow': 0.23643890023231506,\n",
       " 'the process': 0.2371641993522644,\n",
       " 'the individual classifiers': 0.23756353557109833,\n",
       " 'text corpora': 0.23786766827106476,\n",
       " 'the latent': 0.23794887959957123,\n",
       " 'pipelines': 0.23795080184936523,\n",
       " 'a novel end': 0.23821651935577393,\n",
       " 'active learning algorithm': 0.2382253110408783,\n",
       " 'meetings': 0.23854422569274902,\n",
       " 'classification time': 0.2386636734008789,\n",
       " 'a threat': 0.23873773217201233,\n",
       " 'a deep learning model': 0.23885607719421387,\n",
       " 'the definition': 0.23942874372005463,\n",
       " 'the reliability': 0.2396274358034134,\n",
       " 'shared representations': 0.23966087400913239,\n",
       " 'a variational approximation': 0.2399347424507141,\n",
       " 'abnormalities': 0.23999443650245667,\n",
       " 'the design matrix': 0.24058955907821655,\n",
       " 'the quality': 0.24099469184875488,\n",
       " 'the gap': 0.2412637174129486,\n",
       " 'graph structures': 0.241274893283844,\n",
       " 'content': 0.24129152297973633,\n",
       " 'appropriate choice': 0.24192167818546295,\n",
       " 'a spectrum': 0.24201075732707977,\n",
       " 'a single decision': 0.2421034574508667,\n",
       " 'word': 0.242262601852417,\n",
       " 'a new variant': 0.2426023781299591,\n",
       " 'brain regions': 0.24283020198345184,\n",
       " 'tags': 0.24294665455818176,\n",
       " 'an oracle': 0.24321229755878448,\n",
       " 'the severity': 0.24329131841659546,\n",
       " 'error mse': 0.24333511292934418,\n",
       " 'the data manifold': 0.24339830875396729,\n",
       " 'dimensions': 0.24370265007019043,\n",
       " 'the production': 0.24374984204769135,\n",
       " 'each feature': 0.2440575659275055,\n",
       " 'certain conditions': 0.24467377364635468,\n",
       " 'the problem': 0.244724303483963,\n",
       " 'a word': 0.244870126247406,\n",
       " 'the proofs': 0.24513494968414307,\n",
       " 'the latent spaces': 0.2455066591501236,\n",
       " 'a finite number': 0.24566814303398132,\n",
       " 'the equivalence': 0.24574661254882812,\n",
       " 'few labels': 0.24619442224502563,\n",
       " 'a common space': 0.24629506468772888,\n",
       " 'the quantity': 0.24635550379753113,\n",
       " 'the mechanisms': 0.24641580879688263,\n",
       " 'another': 0.2468966543674469,\n",
       " 'mislabeled': 0.24698947370052338,\n",
       " 'the dissimilarity': 0.24717780947685242,\n",
       " 'the performance of the system': 0.2473449856042862,\n",
       " 'the reconstruction': 0.24774464964866638,\n",
       " 'the maximization': 0.247762069106102,\n",
       " 'the behaviour': 0.24792134761810303,\n",
       " 'a bottleneck': 0.24879609048366547,\n",
       " 'morphologically rich languages': 0.24883219599723816,\n",
       " 'handwritten digit recognition': 0.24912698566913605,\n",
       " 'a sentence': 0.24912887811660767,\n",
       " 'low rank matrices': 0.249223530292511,\n",
       " 'large sets': 0.2496655285358429,\n",
       " 'two limitations': 0.24981696903705597,\n",
       " 'a similarity metric': 0.2505943179130554,\n",
       " 'target domain data': 0.25059521198272705,\n",
       " 'votes': 0.25137001276016235,\n",
       " 'the grammar': 0.2515171468257904,\n",
       " 'changing environments': 0.2519499361515045,\n",
       " 'multiple dimensions': 0.25213849544525146,\n",
       " 'a simple technique': 0.25249654054641724,\n",
       " 'super resolution sr': 0.25253826379776,\n",
       " 'the theoretical analysis': 0.25261738896369934,\n",
       " 'viewpoints': 0.2530013620853424,\n",
       " 'relational structures': 0.25332239270210266,\n",
       " 'the processes': 0.2540826201438904,\n",
       " 'quadrature': 0.25445201992988586,\n",
       " 'graph convolutional networks': 0.25476574897766113,\n",
       " 'an exponential family': 0.2565036714076996,\n",
       " 'pictures': 0.256615549325943,\n",
       " 'a person s': 0.2567709982395172,\n",
       " 'weakly supervised learning': 0.2571907937526703,\n",
       " 'additive models': 0.25731244683265686,\n",
       " 'full use': 0.2573394179344177,\n",
       " 'natural language text': 0.2574387788772583,\n",
       " 'greedy decoding': 0.25839170813560486,\n",
       " 'the structure and parameters': 0.25890323519706726,\n",
       " 'common methods': 0.25898584723472595,\n",
       " 'the inner loop': 0.25924503803253174,\n",
       " 'a new task': 0.26036423444747925,\n",
       " 'the sense': 0.2605159282684326,\n",
       " 'bottom': 0.26065266132354736,\n",
       " 'behaviour': 0.26068273186683655,\n",
       " 'kernel methods': 0.2607450783252716,\n",
       " 'the variety': 0.2607559561729431,\n",
       " 'new environments': 0.2609996497631073,\n",
       " 'archetypal analysis': 0.2611820697784424,\n",
       " 'kernel regression': 0.26208096742630005,\n",
       " 'the storage': 0.2623980939388275,\n",
       " 'a similarity graph': 0.26249241828918457,\n",
       " 'hidden causes': 0.2628534138202667,\n",
       " 'domain generalization': 0.26309734582901,\n",
       " 'the queries': 0.26321399211883545,\n",
       " 'this respect': 0.2636754512786865,\n",
       " 'graph signals': 0.2642333209514618,\n",
       " 'a starting point': 0.2646022439002991,\n",
       " 'probability measures': 0.26472046971321106,\n",
       " 'background knowledge': 0.2650895118713379,\n",
       " 'a gaussian process gp': 0.26509037613868713,\n",
       " 'the series': 0.26530858874320984,\n",
       " 'network structure learning': 0.2654564678668976,\n",
       " 'correspondence': 0.26571574807167053,\n",
       " 'p x': 0.26602017879486084,\n",
       " 'automatic speech recognition asr systems': 0.2661648988723755,\n",
       " 'many types': 0.2664497494697571,\n",
       " 'the overall performance': 0.26660579442977905,\n",
       " 'an adversarial perturbation': 0.26735448837280273,\n",
       " 'high number': 0.2679116427898407,\n",
       " 'space and time': 0.26826024055480957,\n",
       " 'a unifying framework': 0.2690671682357788,\n",
       " 'the relative merits': 0.2692779004573822,\n",
       " 'non trivial': 0.26943832635879517,\n",
       " 'hard problems': 0.27113696932792664,\n",
       " 'distributed algorithms': 0.2717888355255127,\n",
       " 'the intrinsic dimensionality': 0.27298957109451294,\n",
       " 'comparison': 0.2734420895576477,\n",
       " 'image signals': 0.2735632061958313,\n",
       " 'structured learning': 0.27360785007476807,\n",
       " 'circular': 0.27409592270851135,\n",
       " 'linguistics': 0.27482444047927856,\n",
       " 'prevention': 0.27494439482688904,\n",
       " 'information maximization': 0.27523311972618103,\n",
       " 'statistical approaches': 0.2757502496242523,\n",
       " 'incomplete data': 0.276157945394516,\n",
       " 'diagnoses': 0.27640461921691895,\n",
       " 'the synthesis': 0.2769426703453064,\n",
       " 'gcns': 0.27726203203201294,\n",
       " 'regulation': 0.2776532471179962,\n",
       " 'a number of methods': 0.27781087160110474,\n",
       " 'informative features': 0.27783411741256714,\n",
       " 'their weights': 0.2779858410358429,\n",
       " 'the sentiment': 0.27828526496887207,\n",
       " 'deep learning algorithms': 0.2783505916595459,\n",
       " 'various types': 0.2789168655872345,\n",
       " 'deep learning framework': 0.2794402539730072,\n",
       " 'the global optimum': 0.2801859676837921,\n",
       " 'training distribution': 0.2802368700504303,\n",
       " 'the true labels': 0.28026294708251953,\n",
       " 'user preferences': 0.280300110578537,\n",
       " 'hyperplane': 0.2813311219215393,\n",
       " 'this short paper': 0.28244197368621826,\n",
       " 'the neighbors': 0.282465398311615,\n",
       " 'common features': 0.2826607823371887,\n",
       " 'high dimensional state spaces': 0.2826854884624481,\n",
       " 'depth and width': 0.2831362783908844,\n",
       " 'deep nets': 0.28349539637565613,\n",
       " 'a sequence': 0.28349724411964417,\n",
       " 'the bayes optimal classifier': 0.2835395038127899,\n",
       " 'feature interactions': 0.2840704321861267,\n",
       " 'different data sources': 0.284157395362854,\n",
       " 'a human operator': 0.28442418575286865,\n",
       " 'a field': 0.28457847237586975,\n",
       " 'a huge number': 0.2852114140987396,\n",
       " 'lfms': 0.28611305356025696,\n",
       " 'sensor modalities': 0.2865414619445801,\n",
       " 'spectral graph': 0.2873740792274475,\n",
       " 'equality': 0.28804126381874084,\n",
       " 'mixture of experts': 0.2881377637386322,\n",
       " 'the utilization': 0.28826844692230225,\n",
       " 'this idea': 0.28845226764678955,\n",
       " 'different kinds': 0.28867238759994507,\n",
       " 'assessments': 0.2888321876525879,\n",
       " 'social media platforms': 0.2891005277633667,\n",
       " 'expected reward': 0.289182186126709,\n",
       " 'training and testing': 0.28927531838417053,\n",
       " 'a high degree': 0.28945019841194153,\n",
       " 'learner model': 0.28951913118362427,\n",
       " 'graph feedback': 0.29082581400871277,\n",
       " 'the expressiveness': 0.2908768057823181,\n",
       " 'semantic similarity': 0.2909030020236969,\n",
       " 'interpretable machine': 0.29100486636161804,\n",
       " 'labeled training data': 0.2912077009677887,\n",
       " 'the reinforcement': 0.2918863594532013,\n",
       " 'the messages': 0.29198428988456726,\n",
       " 'lexicons': 0.29207202792167664,\n",
       " 'hyperparameter tuning': 0.2921549081802368,\n",
       " 'multivariate time series': 0.2923942804336548,\n",
       " 'tails': 0.29305386543273926,\n",
       " 'statistical properties': 0.293487012386322,\n",
       " 'the player s': 0.29389768838882446,\n",
       " 'clusters': 0.29441946744918823,\n",
       " 'visual tracking': 0.29471710324287415,\n",
       " 'a sequence of tasks': 0.2952137589454651,\n",
       " 'a patient s': 0.29583317041397095,\n",
       " 'a document': 0.29641562700271606,\n",
       " 'neural enquirer': 0.29706981778144836,\n",
       " 'a testbed': 0.29828551411628723,\n",
       " 'metric learning': 0.29878243803977966,\n",
       " 'the premise': 0.2993144690990448,\n",
       " 'descriptors': 0.2994793951511383,\n",
       " 'the optimality': 0.29980576038360596,\n",
       " 'factors of variation': 0.30020850896835327,\n",
       " 'two agents': 0.3003454804420471,\n",
       " 'speech enhancement': 0.30058902502059937,\n",
       " 'short texts': 0.3009854853153229,\n",
       " 'layers': 0.3015318512916565,\n",
       " 'the kinds': 0.3018616735935211,\n",
       " 'performance criteria': 0.30208638310432434,\n",
       " 'markov random fields': 0.30211156606674194,\n",
       " 'kronecker': 0.30241963267326355,\n",
       " 'gaussian process gp models': 0.3031060993671417,\n",
       " 'first order methods': 0.3031924068927765,\n",
       " 'the suitability': 0.3032625913619995,\n",
       " 'two properties': 0.30350273847579956,\n",
       " 'the art machine': 0.3035121262073517,\n",
       " 'two layers': 0.30389291048049927,\n",
       " 'image question': 0.3039521276950836,\n",
       " 'advisors': 0.3048618733882904,\n",
       " 'prognostics': 0.30568090081214905,\n",
       " 'high dimensional regression': 0.3059553802013397,\n",
       " 'eeg signals': 0.30597832798957825,\n",
       " 'perceptions': 0.30601269006729126,\n",
       " 'posterior distributions': 0.30677124857902527,\n",
       " 'learning algorithms': 0.3071027994155884,\n",
       " 'booking': 0.30714184045791626,\n",
       " 'matrix multiplications': 0.30759572982788086,\n",
       " 'memory cells': 0.307740181684494,\n",
       " 'the sake': 0.30869778990745544,\n",
       " 'data sources': 0.3087073862552643,\n",
       " 'cross validation': 0.3091583549976349,\n",
       " 'large images': 0.3103335201740265,\n",
       " 'vice versa': 0.31105902791023254,\n",
       " 'hierarchical bayesian optimization algorithm hboa': 0.31108492612838745,\n",
       " 'small datasets': 0.3116306662559509,\n",
       " '4': 0.31171467900276184,\n",
       " 'the machine learning': 0.31187888979911804,\n",
       " 'flats': 0.31194689869880676,\n",
       " 'local descriptors': 0.31253495812416077,\n",
       " 'random perturbations': 0.31320443749427795,\n",
       " 'key factors': 0.313543438911438,\n",
       " 'engineers': 0.313638299703598,\n",
       " 'a plethora': 0.31393107771873474,\n",
       " 'textual descriptions': 0.31399205327033997,\n",
       " 'variational methods': 0.3141230046749115,\n",
       " 'feature extraction': 0.31489649415016174,\n",
       " 'data selection': 0.3152698576450348,\n",
       " 'a deep autoencoder': 0.3159696161746979,\n",
       " 'some assumptions': 0.31606701016426086,\n",
       " 'its implementation': 0.3165670335292816,\n",
       " 'an important problem': 0.31749626994132996,\n",
       " 'each iteration': 0.31798821687698364,\n",
       " 'trust': 0.31809306144714355,\n",
       " 'pilot': 0.31874480843544006,\n",
       " 'text descriptions': 0.31892600655555725,\n",
       " 'visual turing test': 0.3199605643749237,\n",
       " 'transparency': 0.32001012563705444,\n",
       " 'the locations': 0.3200399577617645,\n",
       " 'complex domains': 0.3201253116130829,\n",
       " 'apis': 0.32021790742874146,\n",
       " 'approaches': 0.32066887617111206,\n",
       " 'phytoplankton': 0.32095468044281006,\n",
       " 'large dataset': 0.3222675621509552,\n",
       " 'weights and activations': 0.32258492708206177,\n",
       " 'structural similarity': 0.3229241967201233,\n",
       " 'harness': 0.3230271339416504,\n",
       " 'daily activities': 0.3238936960697174,\n",
       " 'human concepts': 0.3242606520652771,\n",
       " 'saddle points': 0.32437944412231445,\n",
       " 'a textit': 0.32458898425102234,\n",
       " 'face images': 0.3247237801551819,\n",
       " 'exact inference': 0.3248496651649475,\n",
       " 'a total': 0.3254460096359253,\n",
       " 'an application': 0.32562702894210815,\n",
       " 'the validity': 0.3256571888923645,\n",
       " 'the representation power': 0.3257283568382263,\n",
       " 'the aim': 0.32637935876846313,\n",
       " 'words bow': 0.32655927538871765,\n",
       " 'the generator': 0.32676205039024353,\n",
       " 'a metaconflict function': 0.3274962902069092,\n",
       " 'retrieval tasks': 0.32787439227104187,\n",
       " 'end learning': 0.32827505469322205,\n",
       " 'gaussian graphical models': 0.329961895942688,\n",
       " 't sne': 0.3305690586566925,\n",
       " 'new approaches': 0.33059123158454895,\n",
       " 'adversarial inputs': 0.330594927072525,\n",
       " 'count data': 0.33187609910964966,\n",
       " 'non convex optimization': 0.3328772783279419,\n",
       " 'their properties': 0.33307284116744995,\n",
       " 'multiple algorithms': 0.3343927562236786,\n",
       " 'cognitive processes': 0.33523520827293396,\n",
       " 'a computational model': 0.33579108119010925,\n",
       " 'lexicon based': 0.3358997404575348,\n",
       " 'uncertainty estimates': 0.3363751769065857,\n",
       " 'these problems': 0.3363859951496124,\n",
       " 'the thesis': 0.33668556809425354,\n",
       " 'complex concepts': 0.3368973135948181,\n",
       " 'a pseudo ensemble': 0.3372457027435303,\n",
       " 'the uncertainty': 0.3373219668865204,\n",
       " 'reviews': 0.33738455176353455,\n",
       " 'deep convolutional neural network cnn': 0.3375403881072998,\n",
       " 'rl methods': 0.33785882592201233,\n",
       " 'probabilistic programming': 0.3389193117618561,\n",
       " 'epileptic patients': 0.33960211277008057,\n",
       " 'low dimensional features': 0.34029167890548706,\n",
       " 'a number of tasks': 0.34037652611732483,\n",
       " 'many scenarios': 0.3405560553073883,\n",
       " 'multiple levels': 0.34098973870277405,\n",
       " 'a popular algorithm': 0.34107524156570435,\n",
       " 'neural turing machines': 0.3413647711277008,\n",
       " 'soccer': 0.3414180278778076,\n",
       " 'quantum physics': 0.34204503893852234,\n",
       " 'specification': 0.3423171043395996,\n",
       " 'the regularization term': 0.34314584732055664,\n",
       " 'gm': 0.343511700630188,\n",
       " 'a practical algorithm': 0.3437063694000244,\n",
       " 'w': 0.34404727816581726,\n",
       " 'reason about': 0.34475937485694885,\n",
       " 'terms': 0.3450238108634949,\n",
       " 'communication': 0.3452301025390625,\n",
       " 'students': 0.3456028699874878,\n",
       " 'dlms': 0.3456864356994629,\n",
       " 'a decision': 0.34591343998908997,\n",
       " 'rotational symmetry': 0.3462297022342682,\n",
       " 'exposure': 0.3464471399784088,\n",
       " 'human feedback': 0.3467322289943695,\n",
       " 'a multitude': 0.34767645597457886,\n",
       " 'falls': 0.3478061854839325,\n",
       " 'loss minimization': 0.3480079174041748,\n",
       " 'multiple labels': 0.3497498035430908,\n",
       " 'a signal': 0.34987398982048035,\n",
       " 'data mining and machine learning': 0.3501153290271759,\n",
       " 'cognitive neuroscience': 0.3505323529243469,\n",
       " 'multiple layers': 0.35252615809440613,\n",
       " 'its impact': 0.3526192903518677,\n",
       " 'cnn architectures': 0.35273638367652893,\n",
       " 'hyperparameter': 0.3542676270008087,\n",
       " 'the log likelihood': 0.3544155955314636,\n",
       " 'basketball': 0.35443004965782166,\n",
       " 'related applications': 0.35538190603256226,\n",
       " 'the observed variables': 0.3563064932823181,\n",
       " 'fully connected layer': 0.35635650157928467,\n",
       " 'the approach': 0.3564162850379944,\n",
       " 'fixed points': 0.3576272428035736,\n",
       " 'the value distribution': 0.35848063230514526,\n",
       " 'object appearance': 0.36074206233024597,\n",
       " 'the domains': 0.36097753047943115,\n",
       " 'a regularizer': 0.36188316345214844,\n",
       " 'multitask learning': 0.36200833320617676,\n",
       " 'platforms': 0.36275333166122437,\n",
       " 'data points': 0.36292776465415955,\n",
       " 'large scale machine learning': 0.36343979835510254,\n",
       " 'performance loss': 0.3639337122440338,\n",
       " 'kernel approximation': 0.3641582131385803,\n",
       " 'the kernel trick': 0.3644755184650421,\n",
       " 'the cnn': 0.36526113748550415,\n",
       " 'the network': 0.3653976619243622,\n",
       " 'document images': 0.36646127700805664,\n",
       " 'a popular technique': 0.36682575941085815,\n",
       " 'a relaxation': 0.36703529953956604,\n",
       " 'auctions': 0.36814382672309875,\n",
       " 'nonlinearities': 0.36830270290374756,\n",
       " 'sequential decision making': 0.3683599531650543,\n",
       " 'breakthroughs': 0.3685041069984436,\n",
       " 'the realm': 0.36955541372299194,\n",
       " 'object identity': 0.36966440081596375,\n",
       " 'each pixel': 0.36994698643684387,\n",
       " 'a sparsity constraint': 0.37017911672592163,\n",
       " 'structural features': 0.3702944815158844,\n",
       " 'static': 0.3706027567386627,\n",
       " 'smile': 0.37061169743537903,\n",
       " 'permutation': 0.3715081512928009,\n",
       " 'face verification': 0.3717566132545471,\n",
       " 'boundary detection': 0.3722614049911499,\n",
       " 'machine learning and computer vision': 0.37262430787086487,\n",
       " 'planes': 0.37323424220085144,\n",
       " 'loops': 0.3733389675617218,\n",
       " 'the open source': 0.3735532760620117,\n",
       " 'policy search algorithms': 0.3739036023616791,\n",
       " 'depth estimation': 0.3749346137046814,\n",
       " 'visual recognition': 0.3754018545150757,\n",
       " 'a simple model': 0.37641632556915283,\n",
       " 'perception tasks': 0.3771187961101532,\n",
       " 'computational intelligence': 0.37798577547073364,\n",
       " 'a subclass': 0.3782673180103302,\n",
       " 'the outcomes': 0.3784688115119934,\n",
       " 'stochastic processes': 0.3790736496448517,\n",
       " 'large scale': 0.3794076144695282,\n",
       " 'numeric': 0.3811867833137512,\n",
       " 'gaussian mixture models': 0.38128241896629333,\n",
       " 'lra': 0.38229572772979736,\n",
       " 'the safety': 0.3824281096458435,\n",
       " 'youtube': 0.38260722160339355,\n",
       " 'the large number': 0.3830103278160095,\n",
       " 'hand gestures': 0.38303396105766296,\n",
       " 'sequence modeling': 0.3834587037563324,\n",
       " 'few shot classification': 0.3842095732688904,\n",
       " 'structured outputs': 0.3846551179885864,\n",
       " 'function estimation': 0.3853018283843994,\n",
       " 'different aspects': 0.38583141565322876,\n",
       " 'the general problem': 0.38618817925453186,\n",
       " 'exploitation': 0.38687676191329956,\n",
       " 'patient records': 0.3873487114906311,\n",
       " 'instantiations': 0.3876931369304657,\n",
       " 'diverse domains': 0.38964417576789856,\n",
       " 'knowledge bases': 0.390667587518692,\n",
       " 'the situation': 0.3931363821029663,\n",
       " 'markers': 0.3933901786804199,\n",
       " 'dependency': 0.3950022757053375,\n",
       " 'the challenging problem': 0.39510035514831543,\n",
       " 'the limitations': 0.3952839970588684,\n",
       " 'the possibility': 0.397013783454895,\n",
       " 'empirical success': 0.39716836810112,\n",
       " 'hidden layer': 0.39776304364204407,\n",
       " 'convolutional auto encoder': 0.3978363275527954,\n",
       " 'pedestrian detection': 0.4003039598464966,\n",
       " 'the use of deep neural networks': 0.401385635137558,\n",
       " 'competing': 0.4015493392944336,\n",
       " 'the marginal likelihood': 0.4021053612232208,\n",
       " 'high computational complexity': 0.4034535586833954,\n",
       " 'td lambda': 0.4043947458267212,\n",
       " 'an attention': 0.40574243664741516,\n",
       " 'spiking networks': 0.4075186848640442,\n",
       " 'experience replay': 0.4075445234775543,\n",
       " 'great attention': 0.4090559184551239,\n",
       " 'gaussian markov': 0.40934398770332336,\n",
       " 'engineer': 0.4098750352859497,\n",
       " 'approximate': 0.4101910889148712,\n",
       " 'malware detection': 0.4111802279949188,\n",
       " 'statistics and machine learning': 0.41146162152290344,\n",
       " 'invariant representations': 0.4116102159023285,\n",
       " 'low complexity': 0.41270121932029724,\n",
       " 'society': 0.41344672441482544,\n",
       " 'front': 0.4137607514858246,\n",
       " 'recent literature': 0.41422000527381897,\n",
       " 'spectral methods': 0.4143677055835724,\n",
       " 'robustness': 0.4148772954940796,\n",
       " 'the absence': 0.4169115126132965,\n",
       " 'temporal difference': 0.4175432324409485,\n",
       " 'calculation': 0.4182873070240021,\n",
       " 'concept drift': 0.41957196593284607,\n",
       " 'dags': 0.42397233843803406,\n",
       " 'an order of magnitude': 0.4246539771556854,\n",
       " 'abc logitboost': 0.42538735270500183,\n",
       " 'ensemble classifiers': 0.42552024126052856,\n",
       " 'tilde': 0.4267738163471222,\n",
       " 'the quasi': 0.42678168416023254,\n",
       " 'wearable devices': 0.42770129442214966,\n",
       " 'demands': 0.4292101263999939,\n",
       " 'temporal classification': 0.42968103289604187,\n",
       " 'real world applications': 0.43130794167518616,\n",
       " 'many nlp applications': 0.43185001611709595,\n",
       " 'records': 0.43509459495544434,\n",
       " 'businesses': 0.4351041615009308,\n",
       " 'a variety of applications': 0.43549203872680664,\n",
       " 'one step': 0.43591243028640747,\n",
       " 'descent algorithm': 0.43640825152397156,\n",
       " 'image search': 0.4369131326675415,\n",
       " 'a host': 0.43698376417160034,\n",
       " 'hmdb 51': 0.43895190954208374,\n",
       " 'sub sampling': 0.4428883492946625,\n",
       " 'linear classification': 0.44380077719688416,\n",
       " 'the last decade': 0.4440237283706665,\n",
       " 'humour': 0.4440929591655731,\n",
       " 'bregman divergences': 0.44576495885849,\n",
       " '2d images': 0.44609102606773376,\n",
       " 'embedded platforms': 0.4464036822319031,\n",
       " 'deep rl': 0.44750091433525085,\n",
       " 'wasserstein distance': 0.44767969846725464,\n",
       " 'probabilistic programs': 0.44872841238975525,\n",
       " 'ladder': 0.44930320978164673,\n",
       " 'simplicity': 0.44978052377700806,\n",
       " 'several applications': 0.4499550759792328,\n",
       " 'spectra': 0.45040979981422424,\n",
       " 'the low rank': 0.4511418342590332,\n",
       " 'novel algorithms': 0.4512185752391815,\n",
       " 'each word': 0.4515179693698883,\n",
       " 'phase retrieval': 0.4552672505378723,\n",
       " 'its usefulness': 0.4555935859680176,\n",
       " 'video understanding': 0.45824524760246277,\n",
       " 'brain computer': 0.4585873484611511,\n",
       " 'ct scans': 0.46011021733283997,\n",
       " 'the supervised learning': 0.46102175116539,\n",
       " 'new challenges': 0.46122458577156067,\n",
       " 'several problems': 0.4614463448524475,\n",
       " 'sub tasks': 0.46341076493263245,\n",
       " 'new applications': 0.46354514360427856,\n",
       " 'revision': 0.4636296033859253,\n",
       " 'wide applicability': 0.4658096730709076,\n",
       " 'an important tool': 0.47230929136276245,\n",
       " 'drifts': 0.47256845235824585,\n",
       " 'their applications': 0.4738067388534546,\n",
       " 'humor': 0.4749108552932739,\n",
       " 'computational efficiency': 0.4749893248081207,\n",
       " 'ads': 0.4754108190536499,\n",
       " 'notions': 0.47603705525398254,\n",
       " 'a key role': 0.47683775424957275,\n",
       " 'rank minimization': 0.4768736958503723,\n",
       " 'emotion recognition': 0.4771459698677063,\n",
       " 'impact': 0.47749003767967224,\n",
       " 'sphere': 0.4784466028213501,\n",
       " 'medical diagnostics': 0.48045337200164795,\n",
       " 'clinical practice': 0.4814257025718689,\n",
       " 'an increasing interest': 0.4820389747619629,\n",
       " 'object pose': 0.482808917760849,\n",
       " 'recurrent neural network rnn': 0.48301029205322266,\n",
       " 'real world tasks': 0.48427897691726685,\n",
       " 'companies': 0.4847947359085083,\n",
       " 'an important task': 0.4850029945373535,\n",
       " 'model selection': 0.4857971966266632,\n",
       " 'a range': 0.48635995388031006,\n",
       " 'robust face': 0.4885064959526062,\n",
       " 'an important and challenging problem': 0.488721638917923,\n",
       " 'its capability': 0.4943941533565521,\n",
       " 'deep neural network architectures': 0.49593618512153625,\n",
       " 'convergence guarantees': 0.4967884123325348,\n",
       " 'data streams': 0.49793845415115356,\n",
       " 'gp regression': 0.4984976053237915}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "understanding-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(extracted, open(f'data/pem_extracted_vs_{VECTOR_SIZE}_ws_{WINDOW_SIZE}_nl_{NUM_LAYERS}_mf_{MIN_FREQUENCY}_e_{EPOCHS}_t_{int(time.time())}.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pem",
   "language": "python",
   "name": "pem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
